[{"path":"/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":"/CLAUDE.html","id":"package-overview","dir":"","previous_headings":"","what":"Package Overview","title":"CLAUDE.md","text":"R package called manytestsr implements nesting top-procedures testing detect heterogeneous effects block-randomized experiments. package combines statistical testing adaptive splitting procedures localize causal effects.","code":""},{"path":[]},{"path":"/CLAUDE.html","id":"core-development-workflow","dir":"","previous_headings":"Development Commands","what":"Core Development Workflow","title":"CLAUDE.md","text":"make document - Generate documentation using roxygen2 make test - Run test suite (excludes profiling tests) make check - Run R CMD check package validation make build - Build package make dependencies - Install package dependencies","code":""},{"path":"/CLAUDE.html","id":"interactive-development","dir":"","previous_headings":"Development Commands","what":"Interactive Development","title":"CLAUDE.md","text":"make interactive - Start R session package loaded via load.R R -q ---save R_PROFILE=load.R development session","code":""},{"path":"/CLAUDE.html","id":"testing","dir":"","previous_headings":"Development Commands","what":"Testing","title":"CLAUDE.md","text":"Tests tests/testthat/ directory Main test runner: tests/testthat.R uses test_check(\"manytestsr\", filter = \"profil\", invert = TRUE) Set interactive <- TRUE test files debugging (FALSE production) Tests use setDTthreads(1) ensure reproducible results","code":""},{"path":"/CLAUDE.html","id":"build-system","dir":"","previous_headings":"Development Commands","what":"Build System","title":"CLAUDE.md","text":"Uses renv dependency management (see renv.lock) C++ compilation via Rcpp/RcppArmadillo OpenMP support Makefile provides convenient development targets","code":""},{"path":[]},{"path":"/CLAUDE.html","id":"core-components","dir":"","previous_headings":"Code Architecture","what":"Core Components","title":"CLAUDE.md","text":"P-value computation functions different test statistics Permutation asymptotic testing approaches Fast C++ implementations computing outcome distances fast_dists_by_unit_arma2_par parallel processing (OpenMP) fast_dists_and_trans small n fast_dists_and_trans_by_unit_arma N>20 Includes energy distance calculations rank-based transformations Core find_blocks() function implements adaptive testing procedure Various splitting strategies (equal, LOO, specified, cluster-based) Tree-based approach block identification testing Implements online FDR control procedures Wrappers onlineFDR package functions (ADDIS, Saffron, alpha-investing) Local adjustment procedures (Simes, Hommel, min-p, etc.) Integration global alpha spending procedures","code":""},{"path":"/CLAUDE.html","id":"key-dependencies","dir":"","previous_headings":"Code Architecture","what":"Key Dependencies","title":"CLAUDE.md","text":"Core: data.table, Rcpp, RcppArmadillo performance Statistical: coin exact tests, hommel p-value adjustment Visualization: ggplot2, ggraph, tidygraph result plotting Clustering: ClusterR, Ckmeans.1d.dp adaptive splitting","code":""},{"path":"/CLAUDE.html","id":"data-flow","dir":"","previous_headings":"Code Architecture","what":"Data Flow","title":"CLAUDE.md","text":"Input data unit (idat) block (bdat) levels Recursive splitting using various strategies guided splitby variable Statistical testing node using permutation asymptotic methods Alpha adaptation using online FDR procedures Local p-value adjustment within nodes Tree construction results stored reporting","code":""},{"path":"/CLAUDE.html","id":"performance-considerations","dir":"","previous_headings":"Code Architecture","what":"Performance Considerations","title":"CLAUDE.md","text":"C++ implementations computationally intensive distance calculations OpenMP parallelization available large datasets Memory-efficient approaches large N (avoids holding large matrices) Uses data.table efficient data manipulation Thread control via setDTthreads(1) reproducible results","code":""},{"path":"/CLAUDE.html","id":"testing-strategy","dir":"","previous_headings":"Code Architecture","what":"Testing Strategy","title":"CLAUDE.md","text":"Comprehensive test suite covering major functions Tests use simulated data TreeTestSim package Performance profiling code available tests/ directory Tests exclude profiling default (filter = \"profil\", invert = TRUE)","code":""},{"path":"/CLAUDE_CODING.html","id":"setup","dir":"","previous_headings":"","what":"Setup","title":"NA","text":"working task, read files R/ src/ relevant task, plus NAMESPACE DESCRIPTION. uncertain scope, read broadly rather narrowly — better understand surrounding code make changes isolation. Also read vignettes touch functionality. package uses roxygen2 documentation.","code":""},{"path":"/CLAUDE_CODING.html","id":"code-style","dir":"","previous_headings":"","what":"Code style","title":"NA","text":"First, prefer boring code clever code readability maintainability. mean prefer () loops approaches — since vectorization much faster core piece R language use. fact, often avoiding () loop can make code clear () loop overhead necessary setup objects modify, etc. Second, comment code explaining particular sections lines code .","code":""},{"path":"/CLAUDE_CODING.html","id":"file-organization","dir":"","previous_headings":"","what":"File organization","title":"NA","text":"Group functions conceptual purpose, one file per coherent unit. let files grow past roughly 300 lines. new function conceptually distinct existing contents file (different mechanism, different external dependency, different layer abstraction), create new file rather appending.","code":""},{"path":"/CLAUDE_CODING.html","id":"testing","dir":"","previous_headings":"","what":"Testing","title":"NA","text":"Write well commented unit tests go tests/testthat refactoring. Tests represent code runs importantly represent statistical principles underlying justifying code. writing code square numbers tests squaring numbers — important tests tests provide numeric input get numeric output. Tests pass can judge code correct. appropriate remove failing tests rather fixing source code asking clarification . can skip tests can’t quickly resolve failures, need remember future tasks. Always prioritize readable, maintainable tests comprehensive coverage.","code":""},{"path":"/CLAUDE_CODING.html","id":"build-discipline","dir":"","previous_headings":"","what":"Build discipline","title":"NA","text":"Run devtools::document() adding modifying roxygen2 documentation. Run devtools::check() considering task complete. adding new exported functions, bump patch version DESCRIPTION (e.g., 0.0.3.0 → 0.0.3.1).","code":""},{"path":"/CLAUDE_CODING.html","id":"workflow-with-me","dir":"","previous_headings":"","what":"Workflow with me","title":"NA","text":"Pause review checkpoints: 1. writing tests writing implementation. 2. writing implementation running devtools::check(). 3. Whenever design decision arises plan already resolve. proceed past checkpoint without input.","code":""},{"path":"/HANDOFF.html","id":null,"dir":"","previous_headings":"","what":"Handoff Summary","title":"Handoff Summary","text":"Date: 2026-02-14 Branch: main Last commit: f22bde8 — Fix .Rbuildignore regex excluded compute_error_load.Rd tarball","code":""},{"path":"/HANDOFF.html","id":"id_1-key-decisions-made","dir":"","previous_headings":"","what":"1. Key Decisions Made","title":"Handoff Summary","text":"version bump .Rbuildignore fix. change build-config — code, API, behavior changes. current version remains 0.0.4.1001. Root cause symptoms: R CMD check WARNINGs (undocumented compute_error_load, broken \\link{} cross-references) shared single root cause .Rbuildignore, one targeted fix resolved three check issues (2 WARNINGs + 1 NOTE).","code":""},{"path":[]},{"path":"/HANDOFF.html","id":"rbuildignore-the-only-file-changed","dir":"","previous_headings":"2. Files Changed and Why","what":".Rbuildignore (the only file changed)","title":"Handoff Summary","text":"Two edits: load.R → ^load\\.R$ — original unanchored regex load.R Perl regular expression matched file path containing substring “load” + char + “R”. inadvertently matched man/compute_error_load.Rd, excluding built tarball. Rd file existed source man/ directory roxygen2 generated correctly, R CMD build silently dropped . Anchoring pattern ^load\\.R$ restricts root-level load.R file (interactive dev session loader). Added ^\\.claude$ — .claude/ directory (Claude Code project config) included built package, triggering NOTE hidden files. pattern excludes tarball.","code":""},{"path":"/HANDOFF.html","id":"id_3-current-blockers-or-open-questions","dir":"","previous_headings":"","what":"3. Current Blockers or Open Questions","title":"Handoff Summary","text":"None. make check passes cleanly: 0 errors, 0 warnings, 0 notes. DESCRIPTION says License: MIT + file LICENSE LICENSE.md contains GPL-2 text. flagged R CMD check (separate LICENSE file likely exists), may worth verifying intended license questions arise.","code":""},{"path":"/HANDOFF.html","id":"id_4-important-context-to-preserve","dir":"","previous_headings":"","what":"4. Important Context to Preserve","title":"Handoff Summary","text":".Rbuildignore regex-based. Every line Perl regular expression, glob literal path. Unanchored patterns can silently exclude files deep directory tree. adding entries, always anchor ^ escape dots \\.. compute_error_load function (R/alpha_adaptive.R) fully documented roxygen2 exported NAMESPACE. documentation never missing — just excluded tarball build-ignore regex. Development workflow: make document, make test, make check core commands. project uses renv dependency management. Tests exclude profiling default. See CLAUDE.md CLAUDE_CODING.md full conventions. Checkpoint workflow (CLAUDE_CODING.md): Pause user review (1) writing tests, (2) writing implementation, (3) design decisions.","code":""},{"path":[]},{"path":"/HANDOFF.html","id":"done","dir":"","previous_headings":"5. What’s Done vs. What Remains","what":"Done","title":"Handoff Summary","text":"R CMD check issues resolved (2 WARNINGs + 1 NOTE → 0/0/0) Changes committed main Read understood *.md files repository","code":""},{"path":"/HANDOFF.html","id":"remains","dir":"","previous_headings":"5. What’s Done vs. What Remains","what":"Remains","title":"Handoff Summary","text":"Nothing session outstanding pushed remote (user request push)","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"/articles/advanced-methodologies.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Advanced Statistical Methodologies in manytestsr","text":"manytestsr package incorporates state---art statistical methodologies hierarchical testing multiple comparison corrections. vignette demonstrates advanced features based foundational works : Jelle Goeman (Closed Testing Procedures) Aaditya Ramdas (E-value Methodology) Nicolai Meinshausen (Hierarchical Variable Importance Testing) Paul Rosenbaum (Design Sensitivity Analysis) methodologies address key challenges modern statistical inference: maintaining statistical validity maximizing power complex hierarchical testing scenarios.","code":""},{"path":"/articles/advanced-methodologies.html","id":"setup-and-data-preparation","dir":"Articles","previous_headings":"","what":"Setup and Data Preparation","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"library(manytestsr) library(data.table) library(dplyr) library(ggplot2)  # Load example data data(example_dat, package = \"manytestsr\")  # Prepare individual-level data idat <- as.data.table(example_dat)  # Create block-level summary with enhanced metrics bdat <- idat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),     place = first(place),     year = first(year),     .groups = \"drop\"   ) %>%   as.data.table()  cat(\"Dataset:\", nrow(idat), \"individuals across\", nrow(bdat), \"blocks\\n\") #> Dataset: 1268 individuals across 44 blocks"},{"path":"/articles/advanced-methodologies.html","id":"goemans-closed-testing-procedure","dir":"Articles","previous_headings":"","what":"1. Goeman’s Closed Testing Procedure","title":"Advanced Statistical Methodologies in manytestsr","text":"Goeman’s closed testing procedure (Goeman & Solari, 2011) provides powerful approach hierarchical testing : Controls FWER strongly across hypotheses Exploits hierarchical structure increased power Uses intersection-union tests logical consistency key insight: test possible intersections hypotheses reject individual hypotheses containing intersection hypotheses rejected.","code":"# Run hierarchical testing with Goeman's closed testing result_goeman <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist, # Robust test as Goeman would recommend   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_closed_testing = TRUE,   closed_testing_method = \"simes\", # Goeman & Solari's recommended method   thealpha = 0.05,   maxtest = 15,   trace = FALSE )  # Examine results if (\"closed_testing_reject\" %in% names(result_goeman$node_dat)) {   closed_rejections <- sum(result_goeman$node_dat$closed_testing_reject, na.rm = TRUE)   cat(\"Closed testing rejections:\", closed_rejections, \"\\n\")    # Show rejected nodes   rejected_nodes <- result_goeman$node_dat[closed_testing_reject == TRUE]   if (nrow(rejected_nodes) > 0) {     cat(\"Rejected node details:\\n\")     print(rejected_nodes[, .(nodenum, p, closed_testing_reject, depth)])   } } else {   cat(\"Closed testing procedure was not applied (likely due to early termination)\\n\") } # Compare traditional vs closed testing power traditional_detections <- report_detections(result_goeman$bdat, fwer = TRUE) traditional_hits <- sum(traditional_detections$hit, na.rm = TRUE)  cat(\"Comparison of Methods:\\n\") cat(\"- Traditional FWER control:\", traditional_hits, \"detections\\n\") if (\"closed_testing_reject\" %in% names(result_goeman$node_dat)) {   closed_hits <- sum(result_goeman$node_dat$closed_testing_reject, na.rm = TRUE)   cat(\"- Goeman closed testing:\", closed_hits, \"detections\\n\")   cat(\"- Power improvement:\", closed_hits - traditional_hits, \"\\n\") }"},{"path":"/articles/advanced-methodologies.html","id":"theory","dir":"Articles","previous_headings":"","what":"Theory","title":"Advanced Statistical Methodologies in manytestsr","text":"Goeman’s closed testing procedure (Goeman & Solari, 2011) provides powerful approach hierarchical testing : Controls FWER strongly across hypotheses Exploits hierarchical structure increased power Uses intersection-union tests logical consistency key insight: test possible intersections hypotheses reject individual hypotheses containing intersection hypotheses rejected.","code":""},{"path":"/articles/advanced-methodologies.html","id":"implementation","dir":"Articles","previous_headings":"","what":"Implementation","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Run hierarchical testing with Goeman's closed testing result_goeman <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist, # Robust test as Goeman would recommend   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_closed_testing = TRUE,   closed_testing_method = \"simes\", # Goeman & Solari's recommended method   thealpha = 0.05,   maxtest = 15,   trace = FALSE )  # Examine results if (\"closed_testing_reject\" %in% names(result_goeman$node_dat)) {   closed_rejections <- sum(result_goeman$node_dat$closed_testing_reject, na.rm = TRUE)   cat(\"Closed testing rejections:\", closed_rejections, \"\\n\")    # Show rejected nodes   rejected_nodes <- result_goeman$node_dat[closed_testing_reject == TRUE]   if (nrow(rejected_nodes) > 0) {     cat(\"Rejected node details:\\n\")     print(rejected_nodes[, .(nodenum, p, closed_testing_reject, depth)])   } } else {   cat(\"Closed testing procedure was not applied (likely due to early termination)\\n\") }"},{"path":"/articles/advanced-methodologies.html","id":"key-features-of-closed-testing","dir":"Articles","previous_headings":"","what":"Key Features of Closed Testing","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Compare traditional vs closed testing power traditional_detections <- report_detections(result_goeman$bdat, fwer = TRUE) traditional_hits <- sum(traditional_detections$hit, na.rm = TRUE)  cat(\"Comparison of Methods:\\n\") cat(\"- Traditional FWER control:\", traditional_hits, \"detections\\n\") if (\"closed_testing_reject\" %in% names(result_goeman$node_dat)) {   closed_hits <- sum(result_goeman$node_dat$closed_testing_reject, na.rm = TRUE)   cat(\"- Goeman closed testing:\", closed_hits, \"detections\\n\")   cat(\"- Power improvement:\", closed_hits - traditional_hits, \"\\n\") }"},{"path":"/articles/advanced-methodologies.html","id":"meinshausens-hierarchical-testing-with-sequential-rejection","dir":"Articles","previous_headings":"","what":"2. Meinshausen’s Hierarchical Testing with Sequential Rejection","title":"Advanced Statistical Methodologies in manytestsr","text":"Meinshausen (2008) addresses high-dimensional variable selection : Testing variable groups hierarchically rather individually Maintaining power variables correlated Sequential rejection (enhanced Goeman & Solari 2010) improved efficiency","code":"# Apply Meinshausen's hierarchical testing with sequential rejection result_meinshausen <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_meinshausen = TRUE,   meinshausen_method = \"simes\",   meinshausen_sequential = TRUE, # Use Goeman-Solari enhancement   thealpha = 0.05,   maxtest = 15,   trace = FALSE )  # Examine Meinshausen results if (\"meinshausen_reject\" %in% names(result_meinshausen$node_dat)) {   meinshausen_rejections <- sum(result_meinshausen$node_dat$meinshausen_reject, na.rm = TRUE)   cat(\"Meinshausen hierarchical rejections:\", meinshausen_rejections, \"\\n\")    # Show rejected nodes with adjusted p-values   rejected_meinshausen <- result_meinshausen$node_dat[meinshausen_reject == TRUE]   if (nrow(rejected_meinshausen) > 0) {     cat(\"Meinshausen rejected nodes:\\n\")     print(rejected_meinshausen[, .(nodenum, p, meinshausen_adjusted_p, meinshausen_reject, depth)])   } } else {   cat(\"Meinshausen testing was not applied\\n\") } # Compare sequential vs traditional Meinshausen result_meinshausen_trad <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_meinshausen = TRUE,   meinshausen_method = \"simes\",   meinshausen_sequential = FALSE, # Traditional approach   thealpha = 0.05,   maxtest = 15,   trace = FALSE )  # Compare approaches if (all(c(\"meinshausen_reject\") %in% names(result_meinshausen$node_dat))) {   seq_rejections <- sum(result_meinshausen$node_dat$meinshausen_reject, na.rm = TRUE)    if (\"meinshausen_reject\" %in% names(result_meinshausen_trad$node_dat)) {     trad_rejections <- sum(result_meinshausen_trad$node_dat$meinshausen_reject, na.rm = TRUE)      cat(\"Meinshausen Method Comparison:\\n\")     cat(\"- Sequential rejection approach:\", seq_rejections, \"rejections\\n\")     cat(\"- Traditional approach:\", trad_rejections, \"rejections\\n\")     cat(\"- Improvement from sequential:\", seq_rejections - trad_rejections, \"\\n\")   } }"},{"path":"/articles/advanced-methodologies.html","id":"theory-1","dir":"Articles","previous_headings":"","what":"Theory","title":"Advanced Statistical Methodologies in manytestsr","text":"Meinshausen (2008) addresses high-dimensional variable selection : Testing variable groups hierarchically rather individually Maintaining power variables correlated Sequential rejection (enhanced Goeman & Solari 2010) improved efficiency","code":""},{"path":"/articles/advanced-methodologies.html","id":"implementation-1","dir":"Articles","previous_headings":"","what":"Implementation","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Apply Meinshausen's hierarchical testing with sequential rejection result_meinshausen <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_meinshausen = TRUE,   meinshausen_method = \"simes\",   meinshausen_sequential = TRUE, # Use Goeman-Solari enhancement   thealpha = 0.05,   maxtest = 15,   trace = FALSE )  # Examine Meinshausen results if (\"meinshausen_reject\" %in% names(result_meinshausen$node_dat)) {   meinshausen_rejections <- sum(result_meinshausen$node_dat$meinshausen_reject, na.rm = TRUE)   cat(\"Meinshausen hierarchical rejections:\", meinshausen_rejections, \"\\n\")    # Show rejected nodes with adjusted p-values   rejected_meinshausen <- result_meinshausen$node_dat[meinshausen_reject == TRUE]   if (nrow(rejected_meinshausen) > 0) {     cat(\"Meinshausen rejected nodes:\\n\")     print(rejected_meinshausen[, .(nodenum, p, meinshausen_adjusted_p, meinshausen_reject, depth)])   } } else {   cat(\"Meinshausen testing was not applied\\n\") }"},{"path":"/articles/advanced-methodologies.html","id":"sequential-vs-traditional-meinshausen","dir":"Articles","previous_headings":"","what":"Sequential vs Traditional Meinshausen","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Compare sequential vs traditional Meinshausen result_meinshausen_trad <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_meinshausen = TRUE,   meinshausen_method = \"simes\",   meinshausen_sequential = FALSE, # Traditional approach   thealpha = 0.05,   maxtest = 15,   trace = FALSE )  # Compare approaches if (all(c(\"meinshausen_reject\") %in% names(result_meinshausen$node_dat))) {   seq_rejections <- sum(result_meinshausen$node_dat$meinshausen_reject, na.rm = TRUE)    if (\"meinshausen_reject\" %in% names(result_meinshausen_trad$node_dat)) {     trad_rejections <- sum(result_meinshausen_trad$node_dat$meinshausen_reject, na.rm = TRUE)      cat(\"Meinshausen Method Comparison:\\n\")     cat(\"- Sequential rejection approach:\", seq_rejections, \"rejections\\n\")     cat(\"- Traditional approach:\", trad_rejections, \"rejections\\n\")     cat(\"- Improvement from sequential:\", seq_rejections - trad_rejections, \"\\n\")   } }"},{"path":"/articles/advanced-methodologies.html","id":"ramdas-e-value-methodology","dir":"Articles","previous_headings":"","what":"3. Ramdas E-value Methodology","title":"Advanced Statistical Methodologies in manytestsr","text":"E-values (Ramdas et al.) provide modern framework : Sequential testing without fixed sample sizes Always-valid inference regardless stopping rules Wealth accumulation metaphor evidence gathering","code":"# Apply e-value methodology (currently experimental) result_evalues <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_evalues = TRUE,   evalue_wealth_rule = \"kelly\",   thealpha = 0.05,   maxtest = 10,   trace = FALSE )  cat(\"E-value methodology provides always-valid inference\\n\") cat(\"Particularly useful for:\\n\") cat(\"- Sequential data collection\\n\") cat(\"- Optional stopping\\n\") cat(\"- Online learning scenarios\\n\")"},{"path":"/articles/advanced-methodologies.html","id":"theory-2","dir":"Articles","previous_headings":"","what":"Theory","title":"Advanced Statistical Methodologies in manytestsr","text":"E-values (Ramdas et al.) provide modern framework : Sequential testing without fixed sample sizes Always-valid inference regardless stopping rules Wealth accumulation metaphor evidence gathering","code":""},{"path":"/articles/advanced-methodologies.html","id":"implementation-2","dir":"Articles","previous_headings":"","what":"Implementation","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Apply e-value methodology (currently experimental) result_evalues <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_evalues = TRUE,   evalue_wealth_rule = \"kelly\",   thealpha = 0.05,   maxtest = 10,   trace = FALSE )  cat(\"E-value methodology provides always-valid inference\\n\") cat(\"Particularly useful for:\\n\") cat(\"- Sequential data collection\\n\") cat(\"- Optional stopping\\n\") cat(\"- Online learning scenarios\\n\")"},{"path":"/articles/advanced-methodologies.html","id":"rosenbaums-design-sensitivity-analysis","dir":"Articles","previous_headings":"","what":"4. Rosenbaum’s Design Sensitivity Analysis","title":"Advanced Statistical Methodologies in manytestsr","text":"Rosenbaum’s design sensitivity analysis evaluates: Robustness unobserved confounding Sensitivity parameters (Γ values) causal claims Bounds treatment effects confounding scenarios","code":"# Create simulated data with potential confounding structure set.seed(123) n_blocks <- 20 n_per_block <- 15  # Simulate data with block-level confounders sensitivity_data <- data.table(   block = rep(paste0(\"B\", 1:n_blocks), each = n_per_block),   unit_id = 1:(n_blocks * n_per_block) )  # Add treatment assignment with potential bias sensitivity_data[, block_confounder := rnorm(1), by = block] sensitivity_data[, treatment := rbinom(.N, 1, plogis(0.2 * block_confounder))]  # Add outcomes with treatment effect + confounding sensitivity_data[, outcome := rnorm(.N,   mean = 0.3 * treatment + 0.4 * block_confounder,   sd = 1 )]  cat(\"Simulated sensitivity analysis dataset:\\n\") #> Simulated sensitivity analysis dataset: cat(\"- Blocks with varying confounding levels\\n\") #> - Blocks with varying confounding levels cat(\"- Treatment effects potentially biased by unobserved factors\\n\") #> - Treatment effects potentially biased by unobserved factors cat(\"- Gamma sensitivity parameters can assess robustness\\n\") #> - Gamma sensitivity parameters can assess robustness"},{"path":"/articles/advanced-methodologies.html","id":"theory-3","dir":"Articles","previous_headings":"","what":"Theory","title":"Advanced Statistical Methodologies in manytestsr","text":"Rosenbaum’s design sensitivity analysis evaluates: Robustness unobserved confounding Sensitivity parameters (Γ values) causal claims Bounds treatment effects confounding scenarios","code":""},{"path":"/articles/advanced-methodologies.html","id":"simulated-example","dir":"Articles","previous_headings":"","what":"Simulated Example","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Create simulated data with potential confounding structure set.seed(123) n_blocks <- 20 n_per_block <- 15  # Simulate data with block-level confounders sensitivity_data <- data.table(   block = rep(paste0(\"B\", 1:n_blocks), each = n_per_block),   unit_id = 1:(n_blocks * n_per_block) )  # Add treatment assignment with potential bias sensitivity_data[, block_confounder := rnorm(1), by = block] sensitivity_data[, treatment := rbinom(.N, 1, plogis(0.2 * block_confounder))]  # Add outcomes with treatment effect + confounding sensitivity_data[, outcome := rnorm(.N,   mean = 0.3 * treatment + 0.4 * block_confounder,   sd = 1 )]  cat(\"Simulated sensitivity analysis dataset:\\n\") #> Simulated sensitivity analysis dataset: cat(\"- Blocks with varying confounding levels\\n\") #> - Blocks with varying confounding levels cat(\"- Treatment effects potentially biased by unobserved factors\\n\") #> - Treatment effects potentially biased by unobserved factors cat(\"- Gamma sensitivity parameters can assess robustness\\n\") #> - Gamma sensitivity parameters can assess robustness"},{"path":"/articles/advanced-methodologies.html","id":"comprehensive-method-comparison","dir":"Articles","previous_headings":"","what":"5. Comprehensive Method Comparison","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Compare all methods on same outcome methods_comparison <- data.frame(   Method = character(),   Rejections = numeric(),   Power_Metric = numeric(),   Conservative_Level = character(),   stringsAsFactors = FALSE )  # Traditional approach traditional_hits <- sum(traditional_detections$hit, na.rm = TRUE) methods_comparison <- rbind(methods_comparison, data.frame(   Method = \"Traditional FWER\",   Rejections = traditional_hits,   Power_Metric = traditional_hits,   Conservative_Level = \"High\" ))  # Goeman closed testing if (\"closed_testing_reject\" %in% names(result_goeman$node_dat)) {   closed_hits <- sum(result_goeman$node_dat$closed_testing_reject, na.rm = TRUE)   methods_comparison <- rbind(methods_comparison, data.frame(     Method = \"Goeman Closed Testing\",     Rejections = closed_hits,     Power_Metric = closed_hits,     Conservative_Level = \"Medium\"   )) }  # Meinshausen hierarchical if (\"meinshausen_reject\" %in% names(result_meinshausen$node_dat)) {   meinshausen_hits <- sum(result_meinshausen$node_dat$meinshausen_reject, na.rm = TRUE)   methods_comparison <- rbind(methods_comparison, data.frame(     Method = \"Meinshausen Sequential\",     Rejections = meinshausen_hits,     Power_Metric = meinshausen_hits,     Conservative_Level = \"Low\"   )) }  print(methods_comparison) if (nrow(methods_comparison) > 1) {   # Create comparison plot   ggplot(methods_comparison, aes(x = Method, y = Rejections, fill = Conservative_Level)) +     geom_col(alpha = 0.7) +     geom_text(aes(label = Rejections), vjust = -0.5) +     labs(       title = \"Comparison of Advanced Testing Methodologies\",       subtitle = \"Number of rejections by method\",       x = \"Statistical Method\",       y = \"Number of Rejections\",       fill = \"Conservativeness\"     ) +     theme_minimal() +     theme(axis.text.x = element_text(angle = 45, hjust = 1)) +     scale_fill_manual(values = c(\"High\" = \"#d62728\", \"Medium\" = \"#ff7f0e\", \"Low\" = \"#2ca02c\")) }"},{"path":"/articles/advanced-methodologies.html","id":"power-and-error-control-comparison","dir":"Articles","previous_headings":"","what":"Power and Error Control Comparison","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Compare all methods on same outcome methods_comparison <- data.frame(   Method = character(),   Rejections = numeric(),   Power_Metric = numeric(),   Conservative_Level = character(),   stringsAsFactors = FALSE )  # Traditional approach traditional_hits <- sum(traditional_detections$hit, na.rm = TRUE) methods_comparison <- rbind(methods_comparison, data.frame(   Method = \"Traditional FWER\",   Rejections = traditional_hits,   Power_Metric = traditional_hits,   Conservative_Level = \"High\" ))  # Goeman closed testing if (\"closed_testing_reject\" %in% names(result_goeman$node_dat)) {   closed_hits <- sum(result_goeman$node_dat$closed_testing_reject, na.rm = TRUE)   methods_comparison <- rbind(methods_comparison, data.frame(     Method = \"Goeman Closed Testing\",     Rejections = closed_hits,     Power_Metric = closed_hits,     Conservative_Level = \"Medium\"   )) }  # Meinshausen hierarchical if (\"meinshausen_reject\" %in% names(result_meinshausen$node_dat)) {   meinshausen_hits <- sum(result_meinshausen$node_dat$meinshausen_reject, na.rm = TRUE)   methods_comparison <- rbind(methods_comparison, data.frame(     Method = \"Meinshausen Sequential\",     Rejections = meinshausen_hits,     Power_Metric = meinshausen_hits,     Conservative_Level = \"Low\"   )) }  print(methods_comparison)"},{"path":"/articles/advanced-methodologies.html","id":"visualization-of-method-performance","dir":"Articles","previous_headings":"","what":"Visualization of Method Performance","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"if (nrow(methods_comparison) > 1) {   # Create comparison plot   ggplot(methods_comparison, aes(x = Method, y = Rejections, fill = Conservative_Level)) +     geom_col(alpha = 0.7) +     geom_text(aes(label = Rejections), vjust = -0.5) +     labs(       title = \"Comparison of Advanced Testing Methodologies\",       subtitle = \"Number of rejections by method\",       x = \"Statistical Method\",       y = \"Number of Rejections\",       fill = \"Conservativeness\"     ) +     theme_minimal() +     theme(axis.text.x = element_text(angle = 45, hjust = 1)) +     scale_fill_manual(values = c(\"High\" = \"#d62728\", \"Medium\" = \"#ff7f0e\", \"Low\" = \"#2ca02c\")) }"},{"path":"/articles/advanced-methodologies.html","id":"practical-guidelines","dir":"Articles","previous_headings":"","what":"6. Practical Guidelines","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Step 1: Prepare data with appropriate block-level covariates bdat <- prepare_block_data(idat, block_vars = c(\"size\", \"location\", \"time\"))  # Step 2: Choose methodology based on research question if (hierarchical_structure) {   use_closed_testing <- TRUE }  if (high_dimensional && correlated_blocks) {   use_meinshausen <- TRUE }  if (sequential_data_collection) {   use_evalues <- TRUE }  # Step 3: Apply comprehensive testing results <- find_blocks(   idat = idat, bdat = bdat,   use_closed_testing = use_closed_testing,   use_meinshausen = use_meinshausen,   use_evalues = use_evalues,   # ... other parameters )  # Step 4: Validate results with sensitivity analysis if (causal_inference_context) {   sensitivity_results <- design_sensitivity_analysis(     idat, bdat,     formula = outcome ~ treatment,     gamma_range = seq(1, 2, by = 0.1)   ) }"},{"path":[]},{"path":"/articles/advanced-methodologies.html","id":"implementation-workflow","dir":"Articles","previous_headings":"","what":"Implementation Workflow","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Step 1: Prepare data with appropriate block-level covariates bdat <- prepare_block_data(idat, block_vars = c(\"size\", \"location\", \"time\"))  # Step 2: Choose methodology based on research question if (hierarchical_structure) {   use_closed_testing <- TRUE }  if (high_dimensional && correlated_blocks) {   use_meinshausen <- TRUE }  if (sequential_data_collection) {   use_evalues <- TRUE }  # Step 3: Apply comprehensive testing results <- find_blocks(   idat = idat, bdat = bdat,   use_closed_testing = use_closed_testing,   use_meinshausen = use_meinshausen,   use_evalues = use_evalues,   # ... other parameters )  # Step 4: Validate results with sensitivity analysis if (causal_inference_context) {   sensitivity_results <- design_sensitivity_analysis(     idat, bdat,     formula = outcome ~ treatment,     gamma_range = seq(1, 2, by = 0.1)   ) }"},{"path":"/articles/advanced-methodologies.html","id":"advanced-features-and-extensions","dir":"Articles","previous_headings":"","what":"7. Advanced Features and Extensions","title":"Advanced Statistical Methodologies in manytestsr","text":"Advanced hierarchical testing procedures satisfy consonance property - hypothesis rejected, specific hypotheses testable.","code":"# Check logical consistency of hierarchical rejections consonance_check <- check_consonance_property(   result_goeman$node_dat,   result_goeman$node_tracker,   rejection_column = \"closed_testing_reject\" )  if (!consonance_check$is_consonant) {   warning(\"Consonance violations detected in hierarchical testing\") } # Test multiple outcomes with joint error control outcomes <- c(\"Y1\", \"Y2\") joint_results <- lapply(outcomes, function(outcome) {   formula_str <- paste(outcome, \"~ trtF | blockF\")   find_blocks(     idat = idat, bdat = bdat,     fmla = as.formula(formula_str),     use_closed_testing = TRUE,     use_meinshausen = TRUE   ) })"},{"path":"/articles/advanced-methodologies.html","id":"consonance-property-checking","dir":"Articles","previous_headings":"","what":"Consonance Property Checking","title":"Advanced Statistical Methodologies in manytestsr","text":"Advanced hierarchical testing procedures satisfy consonance property - hypothesis rejected, specific hypotheses testable.","code":"# Check logical consistency of hierarchical rejections consonance_check <- check_consonance_property(   result_goeman$node_dat,   result_goeman$node_tracker,   rejection_column = \"closed_testing_reject\" )  if (!consonance_check$is_consonant) {   warning(\"Consonance violations detected in hierarchical testing\") }"},{"path":"/articles/advanced-methodologies.html","id":"multiple-outcome-testing","dir":"Articles","previous_headings":"","what":"Multiple Outcome Testing","title":"Advanced Statistical Methodologies in manytestsr","text":"","code":"# Test multiple outcomes with joint error control outcomes <- c(\"Y1\", \"Y2\") joint_results <- lapply(outcomes, function(outcome) {   formula_str <- paste(outcome, \"~ trtF | blockF\")   find_blocks(     idat = idat, bdat = bdat,     fmla = as.formula(formula_str),     use_closed_testing = TRUE,     use_meinshausen = TRUE   ) })"},{"path":"/articles/advanced-methodologies.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Advanced Statistical Methodologies in manytestsr","text":"manytestsr package integrates cutting-edge statistical methodologies hierarchical testing: Goeman’s closed testing provides rigorous FWER control power improvements Meinshausen’s hierarchical approach excels high-dimensional, correlated settings Ramdas e-values enable always-valid sequential inference Rosenbaum’s sensitivity analysis assesses robustness confounding methods collectively address modern challenges multiple testing complex experimental designs maintaining statistical rigor maximizing power detect true effects. Goeman, J. J., & Solari, . (2011). Multiple testing exploratory research. Statistical Science, 26(4), 584-597. Meinshausen, N. (2008). Hierarchical testing variable importance. Biometrika, 95(2), 265-278. Ramdas, ., Ruf, J., Larsson, M., & Koolen, W. M. (2020). unified treatment multiple testing prior knowledge using e-value. Annals Statistics, 48(5), 2790-2807. Rosenbaum, P. R. (2017). Observation Experiment: Introduction Causal Inference. Harvard University Press.","code":""},{"path":"/articles/advanced-methodologies.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Advanced Statistical Methodologies in manytestsr","text":"Goeman, J. J., & Solari, . (2011). Multiple testing exploratory research. Statistical Science, 26(4), 584-597. Meinshausen, N. (2008). Hierarchical testing variable importance. Biometrika, 95(2), 265-278. Ramdas, ., Ruf, J., Larsson, M., & Koolen, W. M. (2020). unified treatment multiple testing prior knowledge using e-value. Annals Statistics, 48(5), 2790-2807. Rosenbaum, P. R. (2017). Observation Experiment: Introduction Causal Inference. Harvard University Press.","code":""},{"path":"/articles/getting-started.html","id":"what-is-hierarchical-testing","dir":"Articles","previous_headings":"","what":"What is Hierarchical Testing?","title":"Getting Started with manytestsr","text":"manytestsr package implements hierarchical testing procedures detecting treatment effects across multiple experimental blocks. Instead testing block independently (inflates Type error) testing everything together (reduces power), hierarchical testing: Splits blocks groups based similarity pre-specified structure Tests groups level hierarchy Controls error rates maintaining power detect heterogeneous effects especially useful : - Multi-site experiments varying treatment effects - /B tests across different user segments - Clinical trials multiple centers - setting clustered/blocked experimental units","code":""},{"path":"/articles/getting-started.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick Start","title":"Getting Started with manytestsr","text":"","code":"library(manytestsr) library(data.table) library(dplyr)  # Load built-in example data data(example_dat, package = \"manytestsr\")  # Prepare individual-level data idat <- as.data.table(example_dat)  # Create block-level summary bdat <- idat %>%   group_by(blockF) %>%   summarize(     nb = n(),                    # Block size     pb = mean(trt),             # Proportion treated     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),  # Harmonic mean weight     .groups = \"drop\"   ) %>%   as.data.table()  print(paste(\"Data:\", nrow(idat), \"individuals in\", nrow(bdat), \"blocks\")) #> [1] \"Data: 1268 individuals in 44 blocks\" # Basic hierarchical testing results <- find_blocks(   idat = idat,                    # Individual-level data   bdat = bdat,                    # Block-level data   blockid = \"blockF\",             # Block identifier column   splitfn = splitCluster,         # How to split blocks (k-means clustering)   pfn = pOneway,                  # Statistical test to use (t-test)   fmla = Y1 ~ trtF | blockF,     # Formula: outcome ~ treatment | block   splitby = \"hwt\",               # Variable to guide splitting   parallel = \"no\",               # Disable parallel processing for demo   thealpha = 0.05                # Overall error rate )  print(paste(\"Testing created\", nrow(results$node_dat), \"nodes in the tree\")) #> [1] \"Testing created 1 nodes in the tree\" # Identify blocks with significant treatment effects detections <- report_detections(results$bdat, fwer = TRUE, alpha = 0.05)  # Summary cat(\"Results Summary:\\n\") #> Results Summary: cat(\"- Total blocks tested:\", nrow(detections), \"\\n\") #> - Total blocks tested: 44 cat(\"- Blocks with significant effects:\", sum(detections$hit, na.rm = TRUE), \"\\n\")   #> - Blocks with significant effects: 0 cat(\"- Detection rate:\", round(mean(detections$hit, na.rm = TRUE) * 100, 1), \"%\\n\") #> - Detection rate: 0 %  # Show significant blocks if any found if(sum(detections$hit, na.rm = TRUE) > 0) {   sig_blocks <- detections[hit == TRUE, .(blockF, pfinalb)]   cat(\"\\nSignificant blocks:\\n\")   print(sig_blocks) }"},{"path":"/articles/getting-started.html","id":"load-package-and-data","dir":"Articles","previous_headings":"","what":"Load Package and Data","title":"Getting Started with manytestsr","text":"","code":"library(manytestsr) library(data.table) library(dplyr)  # Load built-in example data data(example_dat, package = \"manytestsr\")  # Prepare individual-level data idat <- as.data.table(example_dat)  # Create block-level summary bdat <- idat %>%   group_by(blockF) %>%   summarize(     nb = n(),                    # Block size     pb = mean(trt),             # Proportion treated     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),  # Harmonic mean weight     .groups = \"drop\"   ) %>%   as.data.table()  print(paste(\"Data:\", nrow(idat), \"individuals in\", nrow(bdat), \"blocks\")) #> [1] \"Data: 1268 individuals in 44 blocks\""},{"path":"/articles/getting-started.html","id":"run-hierarchical-testing","dir":"Articles","previous_headings":"","what":"Run Hierarchical Testing","title":"Getting Started with manytestsr","text":"","code":"# Basic hierarchical testing results <- find_blocks(   idat = idat,                    # Individual-level data   bdat = bdat,                    # Block-level data   blockid = \"blockF\",             # Block identifier column   splitfn = splitCluster,         # How to split blocks (k-means clustering)   pfn = pOneway,                  # Statistical test to use (t-test)   fmla = Y1 ~ trtF | blockF,     # Formula: outcome ~ treatment | block   splitby = \"hwt\",               # Variable to guide splitting   parallel = \"no\",               # Disable parallel processing for demo   thealpha = 0.05                # Overall error rate )  print(paste(\"Testing created\", nrow(results$node_dat), \"nodes in the tree\")) #> [1] \"Testing created 1 nodes in the tree\""},{"path":"/articles/getting-started.html","id":"find-significant-effects","dir":"Articles","previous_headings":"","what":"Find Significant Effects","title":"Getting Started with manytestsr","text":"","code":"# Identify blocks with significant treatment effects detections <- report_detections(results$bdat, fwer = TRUE, alpha = 0.05)  # Summary cat(\"Results Summary:\\n\") #> Results Summary: cat(\"- Total blocks tested:\", nrow(detections), \"\\n\") #> - Total blocks tested: 44 cat(\"- Blocks with significant effects:\", sum(detections$hit, na.rm = TRUE), \"\\n\")   #> - Blocks with significant effects: 0 cat(\"- Detection rate:\", round(mean(detections$hit, na.rm = TRUE) * 100, 1), \"%\\n\") #> - Detection rate: 0 %  # Show significant blocks if any found if(sum(detections$hit, na.rm = TRUE) > 0) {   sig_blocks <- detections[hit == TRUE, .(blockF, pfinalb)]   cat(\"\\nSignificant blocks:\\n\")   print(sig_blocks) }"},{"path":"/articles/getting-started.html","id":"key-components","dir":"Articles","previous_headings":"","what":"Key Components","title":"Getting Started with manytestsr","text":"Choose divide blocks step: Choose statistical test: Control Type error rates:","code":"# Cluster-based splitting (most common) splitCluster      # Groups similar blocks using k-means  # Pre-specified hierarchical splitting   splitSpecifiedFactor  # Follows predefined hierarchy (e.g., state > district > school)  # Leave-one-out splitting splitLOO          # Focuses on largest/most powerful blocks first  # Equal-sum splitting splitEqualApprox  # Balances total size/weight across groups pOneway       # T-tests (assumes normality) pIndepDist    # Distance-based tests (robust, recommended) pWilcox       # Wilcoxon rank-sum tests (ordinal outcomes) # Fixed alpha (FWER control) alphafn = NULL, thealpha = 0.05  # Sequential FDR control (more powerful)   alphafn = alpha_investing, thealpha = 0.05, thew0 = 0.049"},{"path":"/articles/getting-started.html","id":"splitting-functions","dir":"Articles","previous_headings":"","what":"Splitting Functions","title":"Getting Started with manytestsr","text":"Choose divide blocks step:","code":"# Cluster-based splitting (most common) splitCluster      # Groups similar blocks using k-means  # Pre-specified hierarchical splitting   splitSpecifiedFactor  # Follows predefined hierarchy (e.g., state > district > school)  # Leave-one-out splitting splitLOO          # Focuses on largest/most powerful blocks first  # Equal-sum splitting splitEqualApprox  # Balances total size/weight across groups"},{"path":"/articles/getting-started.html","id":"test-functions","dir":"Articles","previous_headings":"","what":"Test Functions","title":"Getting Started with manytestsr","text":"Choose statistical test:","code":"pOneway       # T-tests (assumes normality) pIndepDist    # Distance-based tests (robust, recommended) pWilcox       # Wilcoxon rank-sum tests (ordinal outcomes)"},{"path":"/articles/getting-started.html","id":"error-control","dir":"Articles","previous_headings":"","what":"Error Control","title":"Getting Started with manytestsr","text":"Control Type error rates:","code":"# Fixed alpha (FWER control) alphafn = NULL, thealpha = 0.05  # Sequential FDR control (more powerful)   alphafn = alpha_investing, thealpha = 0.05, thew0 = 0.049"},{"path":"/articles/getting-started.html","id":"example-different-approaches","dir":"Articles","previous_headings":"","what":"Example: Different Approaches","title":"Getting Started with manytestsr","text":"","code":"results_robust <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,           # Distance-based test (robust)   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\" )  robust_detections <- report_detections(results_robust$bdat) cat(\"Robust approach detections:\", sum(robust_detections$hit, na.rm = TRUE), \"\\n\") #> Robust approach detections: 8 results_fdr <- find_blocks(   idat = idat,   bdat = bdat,    blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   alphafn = alpha_investing,   # Sequential FDR control   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05,   thew0 = 0.049              # Starting \"wealth\" )  fdr_detections <- report_detections(results_fdr$bdat, fwer = FALSE) cat(\"FDR approach detections:\", sum(fdr_detections$hit, na.rm = TRUE), \"\\n\") #> FDR approach detections: 44"},{"path":"/articles/getting-started.html","id":"approach-1-robust-distance-based-testing","dir":"Articles","previous_headings":"","what":"Approach 1: Robust Distance-Based Testing","title":"Getting Started with manytestsr","text":"","code":"results_robust <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,           # Distance-based test (robust)   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\" )  robust_detections <- report_detections(results_robust$bdat) cat(\"Robust approach detections:\", sum(robust_detections$hit, na.rm = TRUE), \"\\n\") #> Robust approach detections: 8"},{"path":"/articles/getting-started.html","id":"approach-2-sequential-fdr-control","dir":"Articles","previous_headings":"","what":"Approach 2: Sequential FDR Control","title":"Getting Started with manytestsr","text":"","code":"results_fdr <- find_blocks(   idat = idat,   bdat = bdat,    blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   alphafn = alpha_investing,   # Sequential FDR control   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05,   thew0 = 0.049              # Starting \"wealth\" )  fdr_detections <- report_detections(results_fdr$bdat, fwer = FALSE) cat(\"FDR approach detections:\", sum(fdr_detections$hit, na.rm = TRUE), \"\\n\") #> FDR approach detections: 44"},{"path":"/articles/getting-started.html","id":"visualizing-results","dir":"Articles","previous_headings":"","what":"Visualizing Results","title":"Getting Started with manytestsr","text":"","code":"library(ggraph) library(ggplot2)  # Create tree visualization tree_data <- make_results_tree(results, block_id = \"blockF\") tree_plot <- make_results_ggraph(tree_data$graph)  # Display the tree tree_plot +    labs(title = \"Hierarchical Testing Tree\") +   theme_void() # Create summary table summary_table <- tree_data$test_summary if(!is.null(summary_table) && is.data.frame(summary_table) && nrow(summary_table) > 0) {   cat(\"Test Summary:\\n\")   print(summary_table) } else {   cat(\"Tree structure (nodes by depth):\\n\")   if(!is.null(tree_data$nodes) && is.data.frame(tree_data$nodes)) {     print(tree_data$nodes[, .N, by = depth])   } else {     cat(\"Tree data structure available but not displayed in simple format.\\n\")   } } #> Tree structure (nodes by depth): #>    depth     N #>    <int> <int> #> 1:     1     1"},{"path":"/articles/getting-started.html","id":"tree-structure","dir":"Articles","previous_headings":"","what":"Tree Structure","title":"Getting Started with manytestsr","text":"","code":"library(ggraph) library(ggplot2)  # Create tree visualization tree_data <- make_results_tree(results, block_id = \"blockF\") tree_plot <- make_results_ggraph(tree_data$graph)  # Display the tree tree_plot +    labs(title = \"Hierarchical Testing Tree\") +   theme_void()"},{"path":"/articles/getting-started.html","id":"results-summary","dir":"Articles","previous_headings":"","what":"Results Summary","title":"Getting Started with manytestsr","text":"","code":"# Create summary table summary_table <- tree_data$test_summary if(!is.null(summary_table) && is.data.frame(summary_table) && nrow(summary_table) > 0) {   cat(\"Test Summary:\\n\")   print(summary_table) } else {   cat(\"Tree structure (nodes by depth):\\n\")   if(!is.null(tree_data$nodes) && is.data.frame(tree_data$nodes)) {     print(tree_data$nodes[, .N, by = depth])   } else {     cat(\"Tree data structure available but not displayed in simple format.\\n\")   } } #> Tree structure (nodes by depth): #>    depth     N #>    <int> <int> #> 1:     1     1"},{"path":"/articles/getting-started.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"Getting Started with manytestsr","text":"Ensure treatment assignment balanced within blocks Include relevant block-level covariates splitting Calculate appropriate power weights (harmonic mean weights work well) Start : splitCluster + pIndepDist + alpha_investing hierarchical data: Use splitSpecifiedFactor robustness: Always consider pIndepDist power: Use sequential procedures (alpha_investing, alpha_saffron) Focus blocks identified significant Consider effect sizes, just p-values Validate findings additional data possible","code":""},{"path":"/articles/getting-started.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"1. Data Preparation","title":"Getting Started with manytestsr","text":"Ensure treatment assignment balanced within blocks Include relevant block-level covariates splitting Calculate appropriate power weights (harmonic mean weights work well)","code":""},{"path":"/articles/getting-started.html","id":"method-selection","dir":"Articles","previous_headings":"","what":"2. Method Selection","title":"Getting Started with manytestsr","text":"Start : splitCluster + pIndepDist + alpha_investing hierarchical data: Use splitSpecifiedFactor robustness: Always consider pIndepDist power: Use sequential procedures (alpha_investing, alpha_saffron)","code":""},{"path":"/articles/getting-started.html","id":"interpretation","dir":"Articles","previous_headings":"","what":"3. Interpretation","title":"Getting Started with manytestsr","text":"Focus blocks identified significant Consider effect sizes, just p-values Validate findings additional data possible","code":""},{"path":"/articles/getting-started.html","id":"advanced-methodologies","dir":"Articles","previous_headings":"","what":"Advanced Methodologies","title":"Getting Started with manytestsr","text":"manytestsr package also includes state---art methodologies leading researchers: comprehensive coverage advanced methodologies, see specialized vignettes: - Advanced Methodologies: Goeman, Meinshausen, Ramdas, Rosenbaum approaches - Hierarchical Testing Workflow: Complete analysis pipelines - Design Sensitivity Analysis: Robustness unobserved confounding hierarchical testing framework provides principled way navigate multiple testing problem maintaining power detect heterogeneous treatment effects across experimental blocks.","code":"# Enhanced FWER control with improved power results_goeman <- find_blocks(   idat = idat, bdat = bdat,   blockid = \"blockF\", splitfn = splitCluster, pfn = pIndepDist,   fmla = Y1 ~ trtF | blockF, splitby = \"hwt\", parallel = \"no\",   use_closed_testing = TRUE,                    # Enable Goeman's method   closed_testing_method = \"simes\",              # Recommended approach   thealpha = 0.05 ) # Hierarchical variable importance testing with sequential rejection results_meinshausen <- find_blocks(   idat = idat, bdat = bdat,   blockid = \"blockF\", splitfn = splitCluster, pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF, splitby = \"hwt\", parallel = \"no\",   use_meinshausen = TRUE,                       # Enable Meinshausen's method   meinshausen_method = \"simes\",                 # P-value combination   meinshausen_sequential = TRUE,                # Sequential rejection enhancement   thealpha = 0.05 ) # Always-valid inference for sequential data collection results_evalues <- find_blocks(   idat = idat, bdat = bdat,   blockid = \"blockF\", splitfn = splitCluster, pfn = pOneway,   fmla = Y1 ~ trtF | blockF, splitby = \"hwt\", parallel = \"no\",   use_evalues = TRUE,                           # Enable e-value methodology   evalue_wealth_rule = \"kelly\",                 # Wealth accumulation rule   thealpha = 0.05 )"},{"path":"/articles/getting-started.html","id":"goemans-closed-testing","dir":"Articles","previous_headings":"","what":"Goeman’s Closed Testing","title":"Getting Started with manytestsr","text":"","code":"# Enhanced FWER control with improved power results_goeman <- find_blocks(   idat = idat, bdat = bdat,   blockid = \"blockF\", splitfn = splitCluster, pfn = pIndepDist,   fmla = Y1 ~ trtF | blockF, splitby = \"hwt\", parallel = \"no\",   use_closed_testing = TRUE,                    # Enable Goeman's method   closed_testing_method = \"simes\",              # Recommended approach   thealpha = 0.05 )"},{"path":"/articles/getting-started.html","id":"meinshausens-hierarchical-testing","dir":"Articles","previous_headings":"","what":"Meinshausen’s Hierarchical Testing","title":"Getting Started with manytestsr","text":"","code":"# Hierarchical variable importance testing with sequential rejection results_meinshausen <- find_blocks(   idat = idat, bdat = bdat,   blockid = \"blockF\", splitfn = splitCluster, pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF, splitby = \"hwt\", parallel = \"no\",   use_meinshausen = TRUE,                       # Enable Meinshausen's method   meinshausen_method = \"simes\",                 # P-value combination   meinshausen_sequential = TRUE,                # Sequential rejection enhancement   thealpha = 0.05 )"},{"path":"/articles/getting-started.html","id":"e-values-for-sequential-testing","dir":"Articles","previous_headings":"","what":"E-values for Sequential Testing","title":"Getting Started with manytestsr","text":"","code":"# Always-valid inference for sequential data collection results_evalues <- find_blocks(   idat = idat, bdat = bdat,   blockid = \"blockF\", splitfn = splitCluster, pfn = pOneway,   fmla = Y1 ~ trtF | blockF, splitby = \"hwt\", parallel = \"no\",   use_evalues = TRUE,                           # Enable e-value methodology   evalue_wealth_rule = \"kelly\",                 # Wealth accumulation rule   thealpha = 0.05 )"},{"path":"/articles/getting-started.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Getting Started with manytestsr","text":"comprehensive coverage advanced methodologies, see specialized vignettes: - Advanced Methodologies: Goeman, Meinshausen, Ramdas, Rosenbaum approaches - Hierarchical Testing Workflow: Complete analysis pipelines - Design Sensitivity Analysis: Robustness unobserved confounding hierarchical testing framework provides principled way navigate multiple testing problem maintaining power detect heterogeneous treatment effects across experimental blocks.","code":""},{"path":"/articles/hierarchical-testing-workflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Hierarchical Testing with manytestsr","text":"manytestsr package implements hierarchical testing procedures detecting treatment effects across multiple experimental blocks controlling error rates. approach particularly useful : Multiple experimental units organized blocks Heterogeneous treatment effects across blocks Need identify specific blocks show effects Want control family-wise error rate (FWER) false discovery rate (FDR) vignette walks complete workflow data preparation results interpretation.","code":""},{"path":"/articles/hierarchical-testing-workflow.html","id":"loading-the-package-and-data","dir":"Articles","previous_headings":"","what":"Loading the Package and Data","title":"Hierarchical Testing with manytestsr","text":"example dataset contains: id: Individual unit identifier blockF: Block (cluster) factor trtF: Treatment assignment factor (0 = control, 1 = treatment) Y1, Y2: Outcome variables place: Location identifier year: Time identifier place_year_block: Hierarchical grouping variable","code":"library(manytestsr) library(data.table) library(dplyr) library(ggplot2) library(ggraph)  # Load example data data(example_dat, package = \"manytestsr\") head(example_dat) #>       id  year   trt    Y1    Y2   trtF place_year_block  place blockF #>    <int> <int> <int> <num> <num> <fctr>           <char> <char> <fctr> #> 1:     1     1     0     0     0      0         A.1.B082      A   B082 #> 2:     2     3     0     0    12      0         B.3.B094      B   B094 #> 3:     3     1     0     0     0      0         C.1.B097      C   B097 #> 4:     4     1     0     6     0      0         C.1.B097      C   B097 #> 5:     5     1     0     7    11      0         B.1.B089      B   B089 #> 6:     6     1     1     0     0      1         A.1.B080      A   B080"},{"path":"/articles/hierarchical-testing-workflow.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"Hierarchical Testing with manytestsr","text":"hierarchical testing approach requires individual-level data (idat) block-level summaries (bdat): Key block-level variables: hwt (harmonic mean weight): Measures testing power block nb (block size): Number units block pb (proportion treated): Treatment assignment rate place_year_block: Hierarchical factor pre-specified splits","code":"# Individual-level data is already in the right format idat <- as.data.table(example_dat) print(paste(\"Number of individuals:\", nrow(idat))) #> [1] \"Number of individuals: 1268\" print(paste(\"Number of blocks:\", length(unique(idat$blockF)))) #> [1] \"Number of blocks: 44\"  # Create block-level dataset with key variables bdat <- idat %>%   group_by(blockF) %>%   summarize(     # Sample size     nb = n(),     # Proportion treated     pb = mean(trt),     # Harmonic mean weight (for testing power)     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),     # Block characteristics     place = unique(place),     year = unique(year),     place_year_block = factor(unique(place_year_block)),     .groups = \"drop\"   ) %>%   as.data.table()  head(bdat) #>    blockF    nb        pb         hwt  place  year place_year_block #>    <fctr> <int>     <num>       <num> <char> <int>           <fctr> #> 1:   B080   129 0.6666667 0.022607781      A     1         A.1.B080 #> 2:   B081    68 0.6617647 0.012003618      A     1         A.1.B081 #> 3:   B082    56 0.6607143 0.009900293      A     1         A.1.B082 #> 4:   B083     8 0.6250000 0.001478707      A     1         A.1.B083 #> 5:   B084     9 0.7777778 0.001226779      A     2         A.2.B084 #> 6:   B085    53 0.6226415 0.009820844      A     3         A.3.B085"},{"path":"/articles/hierarchical-testing-workflow.html","id":"basic-hierarchical-testing","dir":"Articles","previous_headings":"","what":"Basic Hierarchical Testing","title":"Hierarchical Testing with manytestsr","text":"common approach uses clustering split blocks based continuous variable:","code":"# Run hierarchical testing with cluster-based splitting results_cluster <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster, # Split using k-means clustering   pfn = pOneway, # Use t-tests   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\", # Split based on harmonic mean weights   parallel = \"no\", # Disable parallel processing for demo   trace = TRUE, # Show split progression   thealpha = 0.05 )  # Examine the structure str(results_cluster, max.level = 1) #> List of 2 #>  $ bdat    :Classes 'data.table' and 'data.frame':   44 obs. of  17 variables: #>   ..- attr(*, \".internal.selfref\")=<externalptr>  #>   ..- attr(*, \"sorted\")= chr \"testable\" #>  $ node_dat:Classes 'data.table' and 'data.frame':   1 obs. of  10 variables: #>   ..- attr(*, \".internal.selfref\")=<externalptr> # Block-level results cat(\"Block-level results structure:\\n\") #> Block-level results structure: cat(\"Number of blocks:\", nrow(results_cluster$bdat), \"\\n\") #> Number of blocks: 44 cat(\"Variables:\", names(results_cluster$bdat), \"\\n\\n\") #> Variables: blockF nb pb hwt place year place_year_block p1 pfinalb group_id node_id g1 alpha1 testable nodenum_current nodenum_prev blocksbygroup  # Node-level results cat(\"Node-level results structure:\\n\") #> Node-level results structure: cat(\"Number of nodes:\", nrow(results_cluster$node_dat), \"\\n\") #> Number of nodes: 1 cat(\"Depth levels:\", sort(unique(results_cluster$node_dat$depth)), \"\\n\") #> Depth levels: 1"},{"path":"/articles/hierarchical-testing-workflow.html","id":"using-cluster-based-splitting","dir":"Articles","previous_headings":"","what":"Using Cluster-Based Splitting","title":"Hierarchical Testing with manytestsr","text":"common approach uses clustering split blocks based continuous variable:","code":"# Run hierarchical testing with cluster-based splitting results_cluster <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster, # Split using k-means clustering   pfn = pOneway, # Use t-tests   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\", # Split based on harmonic mean weights   parallel = \"no\", # Disable parallel processing for demo   trace = TRUE, # Show split progression   thealpha = 0.05 )  # Examine the structure str(results_cluster, max.level = 1) #> List of 2 #>  $ bdat    :Classes 'data.table' and 'data.frame':   44 obs. of  17 variables: #>   ..- attr(*, \".internal.selfref\")=<externalptr>  #>   ..- attr(*, \"sorted\")= chr \"testable\" #>  $ node_dat:Classes 'data.table' and 'data.frame':   1 obs. of  10 variables: #>   ..- attr(*, \".internal.selfref\")=<externalptr>"},{"path":"/articles/hierarchical-testing-workflow.html","id":"results-overview","dir":"Articles","previous_headings":"","what":"Results Overview","title":"Hierarchical Testing with manytestsr","text":"","code":"# Block-level results cat(\"Block-level results structure:\\n\") #> Block-level results structure: cat(\"Number of blocks:\", nrow(results_cluster$bdat), \"\\n\") #> Number of blocks: 44 cat(\"Variables:\", names(results_cluster$bdat), \"\\n\\n\") #> Variables: blockF nb pb hwt place year place_year_block p1 pfinalb group_id node_id g1 alpha1 testable nodenum_current nodenum_prev blocksbygroup  # Node-level results cat(\"Node-level results structure:\\n\") #> Node-level results structure: cat(\"Number of nodes:\", nrow(results_cluster$node_dat), \"\\n\") #> Number of nodes: 1 cat(\"Depth levels:\", sort(unique(results_cluster$node_dat$depth)), \"\\n\") #> Depth levels: 1"},{"path":"/articles/hierarchical-testing-workflow.html","id":"alternative-splitting-strategies","dir":"Articles","previous_headings":"","what":"Alternative Splitting Strategies","title":"Hierarchical Testing with manytestsr","text":"natural hierarchical structure, use factor-based splitting: Focus testing largest blocks first:","code":"# Use pre-specified hierarchical splits results_hierarchical <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitSpecifiedFactor,   pfn = pIndepDist, # Use distance-based test   fmla = Y2 ~ trtF | blockF,   splitby = \"place_year_block\",   parallel = \"no\",   trace = TRUE,   thealpha = 0.05 )  print(paste(   \"Hierarchical approach found\",   nrow(results_hierarchical$node_dat), \"nodes\" )) #> [1] \"Hierarchical approach found 5 nodes\" # Leave-one-out approach results_loo <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitLOO,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\", # Focus on blocks with highest power   parallel = \"no\",   thealpha = 0.05 )  print(paste(   \"LOO approach found\",   nrow(results_loo$node_dat), \"nodes\" )) #> [1] \"LOO approach found 1 nodes\""},{"path":"/articles/hierarchical-testing-workflow.html","id":"pre-specified-hierarchical-splitting","dir":"Articles","previous_headings":"","what":"Pre-specified Hierarchical Splitting","title":"Hierarchical Testing with manytestsr","text":"natural hierarchical structure, use factor-based splitting:","code":"# Use pre-specified hierarchical splits results_hierarchical <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitSpecifiedFactor,   pfn = pIndepDist, # Use distance-based test   fmla = Y2 ~ trtF | blockF,   splitby = \"place_year_block\",   parallel = \"no\",   trace = TRUE,   thealpha = 0.05 )  print(paste(   \"Hierarchical approach found\",   nrow(results_hierarchical$node_dat), \"nodes\" )) #> [1] \"Hierarchical approach found 5 nodes\""},{"path":"/articles/hierarchical-testing-workflow.html","id":"leave-one-out-splitting","dir":"Articles","previous_headings":"","what":"Leave-One-Out Splitting","title":"Hierarchical Testing with manytestsr","text":"Focus testing largest blocks first:","code":"# Leave-one-out approach results_loo <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitLOO,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\", # Focus on blocks with highest power   parallel = \"no\",   thealpha = 0.05 )  print(paste(   \"LOO approach found\",   nrow(results_loo$node_dat), \"nodes\" )) #> [1] \"LOO approach found 1 nodes\""},{"path":"/articles/hierarchical-testing-workflow.html","id":"sequential-error-rate-control","dir":"Articles","previous_headings":"","what":"Sequential Error Rate Control","title":"Hierarchical Testing with manytestsr","text":"powerful testing FDR control:","code":"# Use alpha investing for sequential FDR control results_fdr <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   alphafn = alpha_investing, # Sequential FDR control   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05,   thew0 = 0.049 # Starting wealth )  # Compare alpha levels across approaches alpha_comparison <- data.frame(   Node = 1:min(nrow(results_cluster$node_dat), nrow(results_fdr$node_dat)),   Fixed_Alpha = results_cluster$node_dat$a[1:min(     nrow(results_cluster$node_dat),     nrow(results_fdr$node_dat)   )],   Adaptive_Alpha = results_fdr$node_dat$a[1:min(     nrow(results_cluster$node_dat),     nrow(results_fdr$node_dat)   )] )  print(\"Alpha level comparison:\") #> [1] \"Alpha level comparison:\" head(alpha_comparison) #>   Node Fixed_Alpha Adaptive_Alpha #> 1    1        0.05           0.05"},{"path":"/articles/hierarchical-testing-workflow.html","id":"alpha-investing-fdr-control","dir":"Articles","previous_headings":"","what":"Alpha Investing (FDR Control)","title":"Hierarchical Testing with manytestsr","text":"powerful testing FDR control:","code":"# Use alpha investing for sequential FDR control results_fdr <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   alphafn = alpha_investing, # Sequential FDR control   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05,   thew0 = 0.049 # Starting wealth )  # Compare alpha levels across approaches alpha_comparison <- data.frame(   Node = 1:min(nrow(results_cluster$node_dat), nrow(results_fdr$node_dat)),   Fixed_Alpha = results_cluster$node_dat$a[1:min(     nrow(results_cluster$node_dat),     nrow(results_fdr$node_dat)   )],   Adaptive_Alpha = results_fdr$node_dat$a[1:min(     nrow(results_cluster$node_dat),     nrow(results_fdr$node_dat)   )] )  print(\"Alpha level comparison:\") #> [1] \"Alpha level comparison:\" head(alpha_comparison) #>   Node Fixed_Alpha Adaptive_Alpha #> 1    1        0.05           0.05"},{"path":"/articles/hierarchical-testing-workflow.html","id":"detecting-significant-effects","dir":"Articles","previous_headings":"","what":"Detecting Significant Effects","title":"Hierarchical Testing with manytestsr","text":"","code":"# Detect significant blocks using FWER control detections_fwer <- report_detections(   results_cluster$bdat,   fwer = TRUE,   alpha = 0.05,   blockid = \"blockF\" )  # Summary of detections cat(\"FWER Results:\\n\") #> FWER Results: cat(\"Total blocks:\", nrow(detections_fwer), \"\\n\") #> Total blocks: 44 cat(\"Significant blocks:\", sum(detections_fwer$hit, na.rm = TRUE), \"\\n\") #> Significant blocks: 0 cat(   \"Detection rate:\",   round(mean(detections_fwer$hit, na.rm = TRUE) * 100, 1), \"%\\n\\n\" ) #> Detection rate: 0 %  # Show significant blocks if (sum(detections_fwer$hit, na.rm = TRUE) > 0) {   significant_blocks <- detections_fwer[     hit == TRUE,     .(blockF, pfinalb, fin_nodenum)   ]   print(\"Significant blocks:\")   print(significant_blocks) } # Detect using FDR control detections_fdr <- report_detections(   results_fdr$bdat,   fwer = FALSE, # Use FDR instead   alpha = 0.05 )  cat(\"FDR Results:\\n\") #> FDR Results: cat(\"Significant blocks:\", sum(detections_fdr$hit, na.rm = TRUE), \"\\n\") #> Significant blocks: 44 cat(   \"Detection rate:\",   round(mean(detections_fdr$hit, na.rm = TRUE) * 100, 1), \"%\\n\" ) #> Detection rate: 100 %"},{"path":"/articles/hierarchical-testing-workflow.html","id":"using-fwer-control","dir":"Articles","previous_headings":"","what":"Using FWER Control","title":"Hierarchical Testing with manytestsr","text":"","code":"# Detect significant blocks using FWER control detections_fwer <- report_detections(   results_cluster$bdat,   fwer = TRUE,   alpha = 0.05,   blockid = \"blockF\" )  # Summary of detections cat(\"FWER Results:\\n\") #> FWER Results: cat(\"Total blocks:\", nrow(detections_fwer), \"\\n\") #> Total blocks: 44 cat(\"Significant blocks:\", sum(detections_fwer$hit, na.rm = TRUE), \"\\n\") #> Significant blocks: 0 cat(   \"Detection rate:\",   round(mean(detections_fwer$hit, na.rm = TRUE) * 100, 1), \"%\\n\\n\" ) #> Detection rate: 0 %  # Show significant blocks if (sum(detections_fwer$hit, na.rm = TRUE) > 0) {   significant_blocks <- detections_fwer[     hit == TRUE,     .(blockF, pfinalb, fin_nodenum)   ]   print(\"Significant blocks:\")   print(significant_blocks) }"},{"path":"/articles/hierarchical-testing-workflow.html","id":"using-fdr-control","dir":"Articles","previous_headings":"","what":"Using FDR Control","title":"Hierarchical Testing with manytestsr","text":"","code":"# Detect using FDR control detections_fdr <- report_detections(   results_fdr$bdat,   fwer = FALSE, # Use FDR instead   alpha = 0.05 )  cat(\"FDR Results:\\n\") #> FDR Results: cat(\"Significant blocks:\", sum(detections_fdr$hit, na.rm = TRUE), \"\\n\") #> Significant blocks: 44 cat(   \"Detection rate:\",   round(mean(detections_fdr$hit, na.rm = TRUE) * 100, 1), \"%\\n\" ) #> Detection rate: 100 %"},{"path":"/articles/hierarchical-testing-workflow.html","id":"visualizing-results","dir":"Articles","previous_headings":"","what":"Visualizing Results","title":"Hierarchical Testing with manytestsr","text":"","code":"# Create tree structure for visualization tree_results <- make_results_tree(   results_cluster,   block_id = \"blockF\",   node_label = \"hwt\" )  # Create the graph visualization tree_plot <- make_results_ggraph(tree_results$graph, remove_na_p = TRUE)  # Customize the plot tree_plot_styled <- tree_plot +   labs(     title = \"Hierarchical Testing Results Tree\",     subtitle = \"Nodes colored by p-values, size by block weight\",     caption = \"Red nodes indicate significant results\"   ) +   theme_void() +   theme(     plot.title = element_text(size = 14, hjust = 0.5),     plot.subtitle = element_text(size = 12, hjust = 0.5),     plot.caption = element_text(size = 10, hjust = 0.5)   )  print(tree_plot_styled) # Compare detection rates across methods detection_summary <- data.frame(   Method = c(\"FWER (Cluster)\", \"FDR (Alpha Investing)\", \"FWER (Hierarchical)\", \"FWER (LOO)\"),   Detections = c(     sum(detections_fwer$hit, na.rm = TRUE),     sum(detections_fdr$hit, na.rm = TRUE),     sum(report_detections(results_hierarchical$bdat, fwer = TRUE)$hit, na.rm = TRUE),     sum(report_detections(results_loo$bdat, fwer = TRUE)$hit, na.rm = TRUE)   ),   Total_Blocks = c(     nrow(detections_fwer),     nrow(detections_fdr),     nrow(results_hierarchical$bdat),     nrow(results_loo$bdat)   ) )  detection_summary$Detection_Rate <- detection_summary$Detections / detection_summary$Total_Blocks  # Create comparison plot ggplot(detection_summary, aes(x = Method, y = Detection_Rate, fill = Method)) +   geom_col(alpha = 0.8) +   geom_text(aes(label = paste0(Detections, \"/\", Total_Blocks)),     vjust = -0.5, size = 3   ) +   labs(     title = \"Detection Rates Across Testing Methods\",     y = \"Proportion of Blocks with Detected Effects\",     x = \"Testing Method\"   ) +   scale_y_continuous(labels = scales::percent_format()) +   theme_minimal() +   theme(     axis.text.x = element_text(angle = 45, hjust = 1),     legend.position = \"none\"   )"},{"path":"/articles/hierarchical-testing-workflow.html","id":"tree-structure-visualization","dir":"Articles","previous_headings":"","what":"Tree Structure Visualization","title":"Hierarchical Testing with manytestsr","text":"","code":"# Create tree structure for visualization tree_results <- make_results_tree(   results_cluster,   block_id = \"blockF\",   node_label = \"hwt\" )  # Create the graph visualization tree_plot <- make_results_ggraph(tree_results$graph, remove_na_p = TRUE)  # Customize the plot tree_plot_styled <- tree_plot +   labs(     title = \"Hierarchical Testing Results Tree\",     subtitle = \"Nodes colored by p-values, size by block weight\",     caption = \"Red nodes indicate significant results\"   ) +   theme_void() +   theme(     plot.title = element_text(size = 14, hjust = 0.5),     plot.subtitle = element_text(size = 12, hjust = 0.5),     plot.caption = element_text(size = 10, hjust = 0.5)   )  print(tree_plot_styled)"},{"path":"/articles/hierarchical-testing-workflow.html","id":"detection-summary-plot","dir":"Articles","previous_headings":"","what":"Detection Summary Plot","title":"Hierarchical Testing with manytestsr","text":"","code":"# Compare detection rates across methods detection_summary <- data.frame(   Method = c(\"FWER (Cluster)\", \"FDR (Alpha Investing)\", \"FWER (Hierarchical)\", \"FWER (LOO)\"),   Detections = c(     sum(detections_fwer$hit, na.rm = TRUE),     sum(detections_fdr$hit, na.rm = TRUE),     sum(report_detections(results_hierarchical$bdat, fwer = TRUE)$hit, na.rm = TRUE),     sum(report_detections(results_loo$bdat, fwer = TRUE)$hit, na.rm = TRUE)   ),   Total_Blocks = c(     nrow(detections_fwer),     nrow(detections_fdr),     nrow(results_hierarchical$bdat),     nrow(results_loo$bdat)   ) )  detection_summary$Detection_Rate <- detection_summary$Detections / detection_summary$Total_Blocks  # Create comparison plot ggplot(detection_summary, aes(x = Method, y = Detection_Rate, fill = Method)) +   geom_col(alpha = 0.8) +   geom_text(aes(label = paste0(Detections, \"/\", Total_Blocks)),     vjust = -0.5, size = 3   ) +   labs(     title = \"Detection Rates Across Testing Methods\",     y = \"Proportion of Blocks with Detected Effects\",     x = \"Testing Method\"   ) +   scale_y_continuous(labels = scales::percent_format()) +   theme_minimal() +   theme(     axis.text.x = element_text(angle = 45, hjust = 1),     legend.position = \"none\"   )"},{"path":"/articles/hierarchical-testing-workflow.html","id":"interpreting-p-values-and-alpha-levels","dir":"Articles","previous_headings":"","what":"Interpreting P-values and Alpha Levels","title":"Hierarchical Testing with manytestsr","text":"","code":"# Examine p-value patterns pvalue_data <- results_cluster$node_dat[, .(   nodenum,   depth,   p,   a,   testable,   nodesize )]  # Show how p-values change with depth pvalue_summary <- pvalue_data[!is.na(p), .(   Mean_P = mean(p),   Median_P = median(p),   Mean_Alpha = mean(a),   N_Nodes = .N ), by = depth]  print(\"P-value progression by tree depth:\") #> [1] \"P-value progression by tree depth:\" print(pvalue_summary) #>    depth     Mean_P   Median_P Mean_Alpha N_Nodes #>    <int>      <num>      <num>      <num>   <int> #> 1:     1 0.05971615 0.05971615       0.05       1 # Analyze relationship between block characteristics and detection power_data <- merge(   detections_fwer[, .(blockF, hit, pfinalb)],   bdat[, .(blockF, hwt, nb, pb)],   by = \"blockF\" )  # Power vs. block size p1 <- ggplot(power_data, aes(x = nb, y = -log10(pfinalb), color = hit)) +   geom_point(alpha = 0.7, size = 2) +   scale_color_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +   labs(     title = \"Detection by Block Size\",     x = \"Block Size (number of units)\",     y = \"-log10(p-value)\",     color = \"Detected\"   ) +   theme_minimal()  # Power vs. harmonic mean weight p2 <- ggplot(power_data, aes(x = hwt, y = -log10(pfinalb), color = hit)) +   geom_point(alpha = 0.7, size = 2) +   scale_color_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +   labs(     title = \"Detection by Harmonic Mean Weight\",     x = \"Harmonic Mean Weight\",     y = \"-log10(p-value)\",     color = \"Detected\"   ) +   theme_minimal()  # Combine plots library(patchwork) p1 + p2 + plot_layout(ncol = 2)"},{"path":"/articles/hierarchical-testing-workflow.html","id":"p-value-progression-through-tree","dir":"Articles","previous_headings":"","what":"P-value Progression Through Tree","title":"Hierarchical Testing with manytestsr","text":"","code":"# Examine p-value patterns pvalue_data <- results_cluster$node_dat[, .(   nodenum,   depth,   p,   a,   testable,   nodesize )]  # Show how p-values change with depth pvalue_summary <- pvalue_data[!is.na(p), .(   Mean_P = mean(p),   Median_P = median(p),   Mean_Alpha = mean(a),   N_Nodes = .N ), by = depth]  print(\"P-value progression by tree depth:\") #> [1] \"P-value progression by tree depth:\" print(pvalue_summary) #>    depth     Mean_P   Median_P Mean_Alpha N_Nodes #>    <int>      <num>      <num>      <num>   <int> #> 1:     1 0.05971615 0.05971615       0.05       1"},{"path":"/articles/hierarchical-testing-workflow.html","id":"statistical-power-analysis","dir":"Articles","previous_headings":"","what":"Statistical Power Analysis","title":"Hierarchical Testing with manytestsr","text":"","code":"# Analyze relationship between block characteristics and detection power_data <- merge(   detections_fwer[, .(blockF, hit, pfinalb)],   bdat[, .(blockF, hwt, nb, pb)],   by = \"blockF\" )  # Power vs. block size p1 <- ggplot(power_data, aes(x = nb, y = -log10(pfinalb), color = hit)) +   geom_point(alpha = 0.7, size = 2) +   scale_color_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +   labs(     title = \"Detection by Block Size\",     x = \"Block Size (number of units)\",     y = \"-log10(p-value)\",     color = \"Detected\"   ) +   theme_minimal()  # Power vs. harmonic mean weight p2 <- ggplot(power_data, aes(x = hwt, y = -log10(pfinalb), color = hit)) +   geom_point(alpha = 0.7, size = 2) +   scale_color_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +   labs(     title = \"Detection by Harmonic Mean Weight\",     x = \"Harmonic Mean Weight\",     y = \"-log10(p-value)\",     color = \"Detected\"   ) +   theme_minimal()  # Combine plots library(patchwork) p1 + p2 + plot_layout(ncol = 2)"},{"path":"/articles/hierarchical-testing-workflow.html","id":"advanced-usage-multiple-outcomes","dir":"Articles","previous_headings":"","what":"Advanced Usage: Multiple Outcomes","title":"Hierarchical Testing with manytestsr","text":"","code":"# Test both outcomes with local p-value adjustment results_multi_Y1 <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   local_adj_p_fn = local_simes, # Simes adjustment within nodes   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05 )  results_multi_Y2 <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   local_adj_p_fn = local_simes,   fmla = Y2 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05 )  # Compare results multi_comparison <- data.frame(   Outcome = c(\"Y1\", \"Y2\"),   Nodes = c(nrow(results_multi_Y1$node_dat), nrow(results_multi_Y2$node_dat)),   Detections = c(     sum(report_detections(results_multi_Y1$bdat)$hit, na.rm = TRUE),     sum(report_detections(results_multi_Y2$bdat)$hit, na.rm = TRUE)   ) )  print(\"Multiple outcome comparison:\") #> [1] \"Multiple outcome comparison:\" print(multi_comparison) #>   Outcome Nodes Detections #> 1      Y1    11         42 #> 2      Y2    11         42"},{"path":"/articles/hierarchical-testing-workflow.html","id":"testing-multiple-outcomes","dir":"Articles","previous_headings":"","what":"Testing Multiple Outcomes","title":"Hierarchical Testing with manytestsr","text":"","code":"# Test both outcomes with local p-value adjustment results_multi_Y1 <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   local_adj_p_fn = local_simes, # Simes adjustment within nodes   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05 )  results_multi_Y2 <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   local_adj_p_fn = local_simes,   fmla = Y2 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   thealpha = 0.05 )  # Compare results multi_comparison <- data.frame(   Outcome = c(\"Y1\", \"Y2\"),   Nodes = c(nrow(results_multi_Y1$node_dat), nrow(results_multi_Y2$node_dat)),   Detections = c(     sum(report_detections(results_multi_Y1$bdat)$hit, na.rm = TRUE),     sum(report_detections(results_multi_Y2$bdat)$hit, na.rm = TRUE)   ) )  print(\"Multiple outcome comparison:\") #> [1] \"Multiple outcome comparison:\" print(multi_comparison) #>   Outcome Nodes Detections #> 1      Y1    11         42 #> 2      Y2    11         42"},{"path":"/articles/hierarchical-testing-workflow.html","id":"summary-and-best-practices","dir":"Articles","previous_headings":"","what":"Summary and Best Practices","title":"Hierarchical Testing with manytestsr","text":"Cluster-based (splitCluster): Good continuous block characteristics Hierarchical (splitSpecifiedFactor): Use natural hierarchical structure Leave-one-(splitLOO): Focus highest-power blocks first Fixed alpha: Simple FWER control Alpha investing: powerful FDR control SAFFRON/ADDIS: Alternative sequential procedures pOneway: Standard t-tests, good normal outcomes pIndepDist: Distance-based tests, robust distributions pWilcox: Rank-based tests, good ordinal outcomes Use parallel = \"multicore\" faster computation multi-core systems Set appropriate simthresh balance accuracy vs. speed Consider maxtest limit tree depth large datasets Use trace = TRUE development monitor progress hierarchical testing framework provides principled approach multiple testing maintaining interpretability controlling error rates. flexible design allows adaptation various experimental contexts research questions.","code":"# 1. Prepare data idat <- your_individual_data bdat <- create_block_summary(idat)  # 2. Choose approach based on data structure if (have_hierarchical_structure) {   splitfn <- splitSpecifiedFactor   splitby <- \"hierarchical_variable\" } else {   splitfn <- splitCluster   splitby <- \"power_variable\" # e.g., block size or weights }  # 3. Run hierarchical testing results <- find_blocks(   idat = idat,   bdat = bdat,   splitfn = splitfn,   pfn = pIndepDist, # Robust choice   alphafn = alpha_investing, # For more power   splitby = splitby,   thealpha = 0.05 )  # 4. Detect significant effects detections <- report_detections(results$bdat, fwer = FALSE) # FDR control  # 5. Visualize results tree <- make_results_tree(results, block_id = \"block_variable\") plot <- make_results_ggraph(tree$graph)"},{"path":"/articles/hierarchical-testing-workflow.html","id":"key-takeaways","dir":"Articles","previous_headings":"","what":"Key Takeaways","title":"Hierarchical Testing with manytestsr","text":"Cluster-based (splitCluster): Good continuous block characteristics Hierarchical (splitSpecifiedFactor): Use natural hierarchical structure Leave-one-(splitLOO): Focus highest-power blocks first Fixed alpha: Simple FWER control Alpha investing: powerful FDR control SAFFRON/ADDIS: Alternative sequential procedures pOneway: Standard t-tests, good normal outcomes pIndepDist: Distance-based tests, robust distributions pWilcox: Rank-based tests, good ordinal outcomes","code":""},{"path":"/articles/hierarchical-testing-workflow.html","id":"recommended-workflow","dir":"Articles","previous_headings":"","what":"Recommended Workflow","title":"Hierarchical Testing with manytestsr","text":"","code":"# 1. Prepare data idat <- your_individual_data bdat <- create_block_summary(idat)  # 2. Choose approach based on data structure if (have_hierarchical_structure) {   splitfn <- splitSpecifiedFactor   splitby <- \"hierarchical_variable\" } else {   splitfn <- splitCluster   splitby <- \"power_variable\" # e.g., block size or weights }  # 3. Run hierarchical testing results <- find_blocks(   idat = idat,   bdat = bdat,   splitfn = splitfn,   pfn = pIndepDist, # Robust choice   alphafn = alpha_investing, # For more power   splitby = splitby,   thealpha = 0.05 )  # 4. Detect significant effects detections <- report_detections(results$bdat, fwer = FALSE) # FDR control  # 5. Visualize results tree <- make_results_tree(results, block_id = \"block_variable\") plot <- make_results_ggraph(tree$graph)"},{"path":"/articles/hierarchical-testing-workflow.html","id":"performance-considerations","dir":"Articles","previous_headings":"","what":"Performance Considerations","title":"Hierarchical Testing with manytestsr","text":"Use parallel = \"multicore\" faster computation multi-core systems Set appropriate simthresh balance accuracy vs. speed Consider maxtest limit tree depth large datasets Use trace = TRUE development monitor progress hierarchical testing framework provides principled approach multiple testing maintaining interpretability controlling error rates. flexible design allows adaptation various experimental contexts research questions.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jake Bowers. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bowers J (2026). manytestsr: Testing detect heterogeneous effects block-randomized experiments. R package version 0.0.4.1001.","code":"@Manual{,   title = {manytestsr: Testing to detect heterogeneous effects in block-randomized experiments},   author = {Jake Bowers},   year = {2026},   note = {R package version 0.0.4.1001}, }"},{"path":[]},{"path":"/index.html","id":"development-info","dir":"","previous_headings":"","what":"Development Info","title":"Testing to detect heterogeneous effects in block-randomized experiments","text":"","code":"devtools::document() devtools::check()"},{"path":"/index.html","id":"notes","dir":"","previous_headings":"","what":"Notes:","title":"Testing to detect heterogeneous effects in block-randomized experiments","text":"using three main C functions: fast_dists_by_unit_arma2_par using openmp (parallel=\"yes\"), fast_dists_and_trans small n, fast_dists_and_trans_by_unit_arma N>20. (20 chosen without lot profiling. Basic idea hold large matrices memory. Matrices faster N large, bog large number observations.","code":""},{"path":"/reference/adaptive_evalue.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive betting strategy for e-values — adaptive_evalue","title":"Adaptive betting strategy for e-values — adaptive_evalue","text":"Adaptive betting strategy e-values","code":""},{"path":"/reference/adaptive_evalue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive betting strategy for e-values — adaptive_evalue","text":"","code":"adaptive_evalue(t_stat, n1, n0)"},{"path":"/reference/adaptive_evalue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive betting strategy for e-values — adaptive_evalue","text":"t_stat T-statistic n1 Treatment group size n0 Control group size","code":""},{"path":"/reference/adaptive_evalue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive betting strategy for e-values — adaptive_evalue","text":"E-value using adaptive betting","code":""},{"path":"/reference/add_nodes_to_tracker.html","id":null,"dir":"Reference","previous_headings":"","what":"Add nodes to tracker — add_nodes_to_tracker","title":"Add nodes to tracker — add_nodes_to_tracker","text":"Add nodes tracker","code":""},{"path":"/reference/add_nodes_to_tracker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add nodes to tracker — add_nodes_to_tracker","text":"","code":"add_nodes_to_tracker(tracker, parent_ids, depth, n_children)"},{"path":"/reference/add_nodes_to_tracker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add nodes to tracker — add_nodes_to_tracker","text":"tracker Node tracking object parent_ids Vector parent node IDs depth Current depth n_children Number children create","code":""},{"path":"/reference/add_nodes_to_tracker.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add nodes to tracker — add_nodes_to_tracker","text":"Updated tracker new node IDs","code":""},{"path":"/reference/adjust_pvalues_level.html","id":null,"dir":"Reference","previous_headings":"","what":"Adjust p-values at a given level using sequential rejection — adjust_pvalues_level","title":"Adjust p-values at a given level using sequential rejection — adjust_pvalues_level","text":"Adjust p-values given level using sequential rejection","code":""},{"path":"/reference/adjust_pvalues_level.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adjust p-values at a given level using sequential rejection — adjust_pvalues_level","text":"","code":"adjust_pvalues_level(p_values, method, alpha)"},{"path":"/reference/adjust_pvalues_level.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adjust p-values at a given level using sequential rejection — adjust_pvalues_level","text":"p_values Vector p-values current level method Adjustment method alpha Error rate","code":""},{"path":"/reference/adjust_pvalues_level.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adjust p-values at a given level using sequential rejection — adjust_pvalues_level","text":"Vector adjusted p-values","code":""},{"path":"/reference/alpha_adaptive.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive Alpha Adjustment Based on Power Decay with find_blocks. The returned function adjusts significance levels at each tree depth based on estimated power decay (Algorithm 1 from Appendix B of the supplement). — alpha_adaptive","title":"Adaptive Alpha Adjustment Based on Power Decay with find_blocks. The returned function adjusts significance levels at each tree depth based on estimated power decay (Algorithm 1 from Appendix B of the supplement). — alpha_adaptive","text":"Adaptive Alpha Adjustment Based Power Decay find_blocks. returned function adjusts significance levels tree depth based estimated power decay (Algorithm 1 Appendix B supplement).","code":""},{"path":"/reference/alpha_adaptive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive Alpha Adjustment Based on Power Decay with find_blocks. The returned function adjusts significance levels at each tree depth based on estimated power decay (Algorithm 1 from Appendix B of the supplement). — alpha_adaptive","text":"","code":"alpha_adaptive(k, delta_hat, N_total, max_depth = 20L)"},{"path":"/reference/alpha_adaptive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive Alpha Adjustment Based on Power Decay with find_blocks. The returned function adjusts significance levels at each tree depth based on estimated power decay (Algorithm 1 from Appendix B of the supplement). — alpha_adaptive","text":"k Branching factor. Either scalar (constant k levels) integer vector length max_depth k[ell] branching factor level ell. delta_hat Estimated standardized effect size (e.g., Cohen's d). Conservative (larger) values produce stringent adjustment, preserves FWER guarantee. Use upper bound true effect size. N_total Total sample size root level. max_depth Maximum depth compute (default 20).","code":""},{"path":"/reference/alpha_adaptive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive Alpha Adjustment Based on Power Decay with find_blocks. The returned function adjusts significance levels at each tree depth based on estimated power decay (Algorithm 1 from Appendix B of the supplement). — alpha_adaptive","text":"function signature function(pval, batch, nodesize, thealpha, thew0, depth) conforming alphafn interface used find_blocks.","code":""},{"path":"/reference/alpha_adaptive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive Alpha Adjustment Based on Power Decay with find_blocks. The returned function adjusts significance levels at each tree depth based on estimated power decay (Algorithm 1 from Appendix B of the supplement). — alpha_adaptive","text":"returned function uses depth parameter (passed find_blocks) look pre-computed alpha node's tree depth. pval, batch, nodesize, thew0 parameters accepted interface compatibility used — unlike online FDR methods, adaptive alpha depends tree structure, observed p-values. Results cached internally: vector adjusted alphas computed per unique value thealpha reused subsequent calls. error load 1 (natural gating suffices), nominal alpha returned every level.","code":""},{"path":"/reference/alpha_adaptive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive Alpha Adjustment Based on Power Decay with find_blocks. The returned function adjusts significance levels at each tree depth based on estimated power decay (Algorithm 1 from Appendix B of the supplement). — alpha_adaptive","text":"","code":"# Create an adaptive alpha function for a 4-ary tree my_alpha <- alpha_adaptive(k = 4, delta_hat = 0.5, N_total = 1000)  # Use with find_blocks # find_blocks(idat, bdat, ..., alphafn = my_alpha)  # Inspect the alpha schedule it will use compute_adaptive_alphas(k = 4, delta_hat = 0.5, N_total = 1000) #>            1            2            3            4            5            6  #> 0.0500000000 0.0125000000 0.0031250000 0.0007997540 0.0003946938 0.0005959012  #>            7            8            9           10           11           12  #> 0.0020881421 0.0120383281 0.0500000000 0.0500000000 0.0500000000 0.0500000000  #>           13           14           15           16           17           18  #> 0.0500000000 0.0500000000 0.0500000000 0.0500000000 0.0500000000 0.0500000000  #>           19           20  #> 0.0500000000 0.0500000000  #> attr(,\"error_load\") #> attr(,\"error_load\")$G #>            1            2            3            4            5            6  #> 1.000000e+00 4.000000e+00 1.562981e+01 3.167012e+01 2.097663e+01 5.986183e+00  #>            7            8            9           10           11           12  #> 1.038350e+00 1.376706e-01 1.587883e-02 1.706042e-03 1.768565e-04 1.800728e-05  #>           13           14           15           16           17           18  #> 1.817040e-06 1.825254e-07 1.829376e-08 1.831441e-09 1.832474e-10 1.832991e-11  #>           19           20  #> 1.833249e-12 1.833378e-13  #>  #> attr(,\"error_load\")$sum_G #> [1] 80.45654 #>  #> attr(,\"error_load\")$needs_adjustment #> [1] TRUE #>  #> attr(,\"error_load\")$thetas #>          1          2          3          4          5          6          7  #> 1.00000000 1.00000000 0.97686287 0.50656612 0.16558692 0.07134347 0.04336445  #>          8          9         10         11         12         13         14  #> 0.03314649 0.02883482 0.02686032 0.02591620 0.02545465 0.02522646 0.02511302  #>         15         16         17         18         19         20  #> 0.02505646 0.02502821 0.02501410 0.02500705 0.02500353 0.02500176  #>  #> attr(,\"error_load\")$critical_level #> [1] 5 #>  #> attr(,\"error_load\")$n_by_level #>            1            2            3            4            5            6  #> 1.000000e+03 2.500000e+02 6.250000e+01 1.562500e+01 3.906250e+00 9.765625e-01  #>            7            8            9           10           11           12  #> 2.441406e-01 6.103516e-02 1.525879e-02 3.814697e-03 9.536743e-04 2.384186e-04  #>           13           14           15           16           17           18  #> 5.960464e-05 1.490116e-05 3.725290e-06 9.313226e-07 2.328306e-07 5.820766e-08  #>           19           20  #> 1.455192e-08 3.637979e-09  #>"},{"path":"/reference/alpha_adaptive_tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive Alpha from an Actual Tree Structure — alpha_adaptive_tree","title":"Adaptive Alpha from an Actual Tree Structure — alpha_adaptive_tree","text":"Factory function counterpart alpha_adaptive irregular trees. Creates alpha adjustment function use find_blocks, using actual per-node sample sizes compute alpha schedule instead assuming regular k-ary tree.","code":""},{"path":"/reference/alpha_adaptive_tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive Alpha from an Actual Tree Structure — alpha_adaptive_tree","text":"","code":"alpha_adaptive_tree(node_dat, delta_hat, max_depth = NULL)"},{"path":"/reference/alpha_adaptive_tree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive Alpha from an Actual Tree Structure — alpha_adaptive_tree","text":"node_dat data.frame data.table columns nodenum, parent, depth, nodesize. Typically extracted find_blocks result. root node must parent = 0 depth = 1. delta_hat Estimated standardized effect size (e.g., Cohen's d). Conservative (larger) values produce stringent adjustment, preserves FWER guarantee. max_depth Maximum depth compute. Defaults maximum depth present node_dat.","code":""},{"path":"/reference/alpha_adaptive_tree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive Alpha from an Actual Tree Structure — alpha_adaptive_tree","text":"function signature function(pval, batch, nodesize, thealpha, thew0, depth) conforming alphafn interface used find_blocks.","code":""},{"path":"/reference/alpha_adaptive_tree.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adaptive Alpha from an Actual Tree Structure — alpha_adaptive_tree","text":"returned function behaves identically closure alpha_adaptive: uses depth parameter look pre-computed alphas, ignoring pval, batch, arguments. difference alpha schedule comes compute_adaptive_alphas_tree rather parametric formula, handles irregular branching correctly. Results cached internally: alpha schedule computed per unique value thealpha reused subsequent calls.","code":""},{"path":"/reference/alpha_adaptive_tree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive Alpha from an Actual Tree Structure — alpha_adaptive_tree","text":"","code":"# Build a tree template from the DPP design nd <- data.frame(   nodenum  = 1:7,   parent   = c(0, 1, 1, 2, 2, 3, 3),   depth    = c(1, 2, 2, 3, 3, 3, 3),   nodesize = c(500, 250, 250, 125, 125, 100, 150) ) my_alpha <- alpha_adaptive_tree(node_dat = nd, delta_hat = 0.5)  # Use with find_blocks # find_blocks(idat, bdat, ..., alphafn = my_alpha)"},{"path":"/reference/alpha_addis.html","id":null,"dir":"Reference","previous_headings":"","what":"Alpha adjustment function: ADDIS — alpha_addis","title":"Alpha adjustment function: ADDIS — alpha_addis","text":"function accept vector p-values vector indicating batch p-values produces vector alpha values.","code":""},{"path":"/reference/alpha_addis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alpha adjustment function: ADDIS — alpha_addis","text":"","code":"alpha_addis(   pval,   batch,   nodesize,   thealpha = 0.05,   thew0 = 0.05 - 0.001,   depth = NULL )"},{"path":"/reference/alpha_addis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alpha adjustment function: ADDIS — alpha_addis","text":"pval numeric vector p-values batch character factor even numeric vector indicating grouping among p-values order p-values. first element pval first element batch first p-value produced tree subsequent p-values produced children root (children children root, etc.) nodesize vector indicating information used create p-value. example, number observations, weighted version. thealpha overall error rate given test thew0 starting \"wealth\" alpha investing procedure depth Integer vector tree depths p-value (1 = root). Accepted interface compatibility tree-structured alpha adjusters used function.","code":""},{"path":"/reference/alpha_addis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Alpha adjustment function: ADDIS — alpha_addis","text":"vector alpha values","code":""},{"path":"/reference/alpha_addis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Alpha adjustment function: ADDIS — alpha_addis","text":"","code":"# Example with ADDIS procedure for hierarchical testing pvals <- c(0.01, 0.04, 0.12, 0.08, 0.15, 0.02) batches <- c(1, 2, 2, 3, 3, 3) # Tree depth levels node_sizes <- c(100, 50, 50, 25, 25, 25) # Sample sizes at each node  # Apply ADDIS procedure alpha_vals <- alpha_addis(pvals, batches, node_sizes, thealpha = 0.05)  # Compare p-values to adjusted alpha levels data.frame(   pval = pvals,   batch = batches,   alpha = alpha_vals,   significant = pvals <= alpha_vals ) #>   pval batch       alpha significant #> 1 0.01     1 0.006431105       FALSE #> 2 0.04     2 0.006431105       FALSE #> 3 0.12     2 0.006431105       FALSE #> 4 0.08     3 0.006431105       FALSE #> 5 0.15     3 0.006431105       FALSE #> 6 0.02     3 0.006431105       FALSE"},{"path":"/reference/alpha_investing.html","id":null,"dir":"Reference","previous_headings":"","what":"Alpha adjustment function: Alpha Investing — alpha_investing","title":"Alpha adjustment function: Alpha Investing — alpha_investing","text":"function accept vector p-values vector indicating batch p-values produces vector alpha values.","code":""},{"path":"/reference/alpha_investing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alpha adjustment function: Alpha Investing — alpha_investing","text":"","code":"alpha_investing(   pval,   batch,   nodesize,   thealpha = 0.05,   thew0 = 0.05 - 0.001,   depth = NULL )"},{"path":"/reference/alpha_investing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alpha adjustment function: Alpha Investing — alpha_investing","text":"pval numeric vector p-values batch character factor even numeric vector indicating grouping among p-values order p-values. first element pval first element batch first p-value produced tree subsequent p-values produced children root (children children root, etc.) nodesize vector indicating information used create p-value. example, number observations, weighted version. thealpha overall error rate given test thew0 starting \"wealth\" alpha investing procedure depth Integer vector tree depths p-value (1 = root). Accepted interface compatibility tree-structured alpha adjusters used function.","code":""},{"path":"/reference/alpha_investing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Alpha adjustment function: Alpha Investing — alpha_investing","text":"vector alpha values","code":""},{"path":"/reference/alpha_investing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Alpha adjustment function: Alpha Investing — alpha_investing","text":"","code":"# Example with hierarchical testing scenario # Simulate p-values from different levels of a tree pvals <- c(0.01, 0.04, 0.12, 0.08, 0.15, 0.02) batches <- c(1, 2, 2, 3, 3, 3) # Tree depth levels node_sizes <- c(100, 50, 50, 25, 25, 25) # Sample sizes at each node  # Apply alpha investing procedure alpha_vals <- alpha_investing(pvals, batches, node_sizes, thealpha = 0.05)  # Compare p-values to adjusted alpha levels data.frame(   pval = pvals,   batch = batches,   alpha = alpha_vals,   significant = pvals <= alpha_vals ) #>   pval batch       alpha significant #> 1 0.01     1 0.020987117        TRUE #> 2 0.04     2 0.021406257       FALSE #> 3 0.12     2 0.007164201       FALSE #> 4 0.08     3 0.003757589       FALSE #> 5 0.15     3 0.002374706       FALSE #> 6 0.02     3 0.001662890       FALSE"},{"path":"/reference/alpha_saffron.html","id":null,"dir":"Reference","previous_headings":"","what":"Alpha adjustment function: SAFFRON — alpha_saffron","title":"Alpha adjustment function: SAFFRON — alpha_saffron","text":"function accept vector p-values vector indicating batch p-values produces vector alpha values.","code":""},{"path":"/reference/alpha_saffron.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alpha adjustment function: SAFFRON — alpha_saffron","text":"","code":"alpha_saffron(   pval,   batch,   nodesize,   thealpha = 0.05,   thew0 = 0.05 - 0.001,   depth = NULL )"},{"path":"/reference/alpha_saffron.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alpha adjustment function: SAFFRON — alpha_saffron","text":"pval numeric vector p-values batch character factor even numeric vector indicating grouping among p-values order p-values. first element pval first element batch first p-value produced tree subsequent p-values produced children root (children children root, etc.) nodesize vector indicating information used create p-value. example, number observations, weighted version. thealpha overall error rate given test thew0 starting \"wealth\" alpha investing procedure depth Integer vector tree depths p-value (1 = root). Accepted interface compatibility tree-structured alpha adjusters used function.","code":""},{"path":"/reference/alpha_saffron.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Alpha adjustment function: SAFFRON — alpha_saffron","text":"vector alpha values","code":""},{"path":"/reference/alpha_saffron.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Alpha adjustment function: SAFFRON — alpha_saffron","text":"","code":"# Example with SAFFRON procedure for hierarchical testing pvals <- c(0.01, 0.04, 0.12, 0.08, 0.15, 0.02) batches <- c(1, 2, 2, 3, 3, 3) # Tree depth levels node_sizes <- c(100, 50, 50, 25, 25, 25) # Sample sizes at each node  # Apply SAFFRON procedure alpha_vals <- alpha_saffron(pvals, batches, node_sizes, thealpha = 0.05)  # Compare p-values to adjusted alpha levels data.frame(   pval = pvals,   batch = batches,   alpha = alpha_vals,   significant = pvals <= alpha_vals ) #>   pval batch      alpha significant #> 1 0.01     1 0.01714961        TRUE #> 2 0.04     2 0.01749961       FALSE #> 3 0.12     2 0.01749961       FALSE #> 4 0.08     3 0.01749961       FALSE #> 5 0.15     3 0.01749961       FALSE #> 6 0.02     3 0.01749961       FALSE"},{"path":"/reference/analyze_violation_patterns.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze violation patterns — analyze_violation_patterns","title":"Analyze violation patterns — analyze_violation_patterns","text":"Analyze violation patterns","code":""},{"path":"/reference/analyze_violation_patterns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze violation patterns — analyze_violation_patterns","text":"","code":"analyze_violation_patterns(violations, hierarchy)"},{"path":"/reference/analyze_violation_patterns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze violation patterns — analyze_violation_patterns","text":"violations List consonance violations hierarchy Hierarchy structure","code":""},{"path":"/reference/analyze_violation_patterns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze violation patterns — analyze_violation_patterns","text":"Analysis violation patterns","code":""},{"path":"/reference/apply_closed_testing_principle.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply closed testing principle to determine valid rejections — apply_closed_testing_principle","title":"Apply closed testing principle to determine valid rejections — apply_closed_testing_principle","text":"Apply closed testing principle determine valid rejections","code":""},{"path":"/reference/apply_closed_testing_principle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply closed testing principle to determine valid rejections — apply_closed_testing_principle","text":"","code":"apply_closed_testing_principle(intersection_results, intersections, alpha)"},{"path":"/reference/apply_closed_testing_principle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply closed testing principle to determine valid rejections — apply_closed_testing_principle","text":"intersection_results Results intersection testing intersections Original intersection definitions alpha Type error rate","code":""},{"path":"/reference/apply_closed_testing_principle.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply closed testing principle to determine valid rejections — apply_closed_testing_principle","text":"Named list rejection decisions node","code":""},{"path":"/reference/apply_consonance_correction.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply consonance correction to test results — apply_consonance_correction","title":"Apply consonance correction to test results — apply_consonance_correction","text":"Apply consonance correction test results","code":""},{"path":"/reference/apply_consonance_correction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply consonance correction to test results — apply_consonance_correction","text":"","code":"apply_consonance_correction(   node_dat,   tracker,   rejection_column = \"testable\",   correction_method = \"propagate_up\" )"},{"path":"/reference/apply_consonance_correction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply consonance correction to test results — apply_consonance_correction","text":"node_dat Node data table test results tracker Node tracker object rejection_column Column rejection decisions correction_method Method correction (\"propagate_up\", \"propagate_down\", \"closed_testing\")","code":""},{"path":"/reference/apply_consonance_correction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply consonance correction to test results — apply_consonance_correction","text":"Corrected node data table","code":""},{"path":"/reference/apply_sequential_rejection_meinshausen.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Sequential Rejection Principle (Goeman-Solari enhancement) — apply_sequential_rejection_meinshausen","title":"Apply Sequential Rejection Principle (Goeman-Solari enhancement) — apply_sequential_rejection_meinshausen","text":"Apply Sequential Rejection Principle (Goeman-Solari enhancement)","code":""},{"path":"/reference/apply_sequential_rejection_meinshausen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Sequential Rejection Principle (Goeman-Solari enhancement) — apply_sequential_rejection_meinshausen","text":"","code":"apply_sequential_rejection_meinshausen(node_dat, node_tracker, alpha, method)"},{"path":"/reference/apply_sequential_rejection_meinshausen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Sequential Rejection Principle (Goeman-Solari enhancement) — apply_sequential_rejection_meinshausen","text":"node_dat Node data node_tracker Node tracking structure alpha Error rate method P-value combination method","code":""},{"path":"/reference/apply_sequential_rejection_meinshausen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Sequential Rejection Principle (Goeman-Solari enhancement) — apply_sequential_rejection_meinshausen","text":"Updated node data sequential rejections","code":""},{"path":"/reference/apply_traditional_meinshausen.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Traditional Meinshausen Procedure — apply_traditional_meinshausen","title":"Apply Traditional Meinshausen Procedure — apply_traditional_meinshausen","text":"Apply Traditional Meinshausen Procedure","code":""},{"path":"/reference/apply_traditional_meinshausen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Traditional Meinshausen Procedure — apply_traditional_meinshausen","text":"","code":"apply_traditional_meinshausen(node_dat, node_tracker, alpha, method)"},{"path":"/reference/apply_traditional_meinshausen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Traditional Meinshausen Procedure — apply_traditional_meinshausen","text":"node_dat Node data node_tracker Node tracking structure alpha Error rate method P-value combination method","code":""},{"path":"/reference/apply_traditional_meinshausen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Traditional Meinshausen Procedure — apply_traditional_meinshausen","text":"Updated node data traditional Meinshausen results","code":""},{"path":"/reference/build_hierarchy_structure.html","id":null,"dir":"Reference","previous_headings":"","what":"Build hierarchy structure from node data and tracker — build_hierarchy_structure","title":"Build hierarchy structure from node data and tracker — build_hierarchy_structure","text":"Build hierarchy structure node data tracker","code":""},{"path":"/reference/build_hierarchy_structure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build hierarchy structure from node data and tracker — build_hierarchy_structure","text":"","code":"build_hierarchy_structure(node_dat, tracker)"},{"path":"/reference/build_hierarchy_structure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build hierarchy structure from node data and tracker — build_hierarchy_structure","text":"node_dat Node data table tracker Node tracker object","code":""},{"path":"/reference/build_hierarchy_structure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build hierarchy structure from node data and tracker — build_hierarchy_structure","text":"List hierarchy information","code":""},{"path":"/reference/build_numeric_ancestry.html","id":null,"dir":"Reference","previous_headings":"","what":"Build ancestry path for numeric node ID — build_numeric_ancestry","title":"Build ancestry path for numeric node ID — build_numeric_ancestry","text":"Build ancestry path numeric node ID","code":""},{"path":"/reference/build_numeric_ancestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build ancestry path for numeric node ID — build_numeric_ancestry","text":"","code":"build_numeric_ancestry(tracker, node_id)"},{"path":"/reference/build_numeric_ancestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build ancestry path for numeric node ID — build_numeric_ancestry","text":"tracker Node tracking object node_id Node ID build path ","code":""},{"path":"/reference/build_numeric_ancestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build ancestry path for numeric node ID — build_numeric_ancestry","text":"Integer vector ancestry path root node","code":""},{"path":"/reference/check_consonance_property.html","id":null,"dir":"Reference","previous_headings":"","what":"Consonance Property Checking for Hierarchical Testing — check_consonance_property","title":"Consonance Property Checking for Hierarchical Testing — check_consonance_property","text":"Implementation consonance property validation hierarchical multiple testing procedures. consonance property ensures logical consistency: hypothesis rejected, hypotheses logically imply also rejectable. Essential validating find_blocks() results ensuring logical coherence.","code":""},{"path":"/reference/check_consonance_property.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Consonance Property Checking for Hierarchical Testing — check_consonance_property","text":"","code":"check_consonance_property(   node_dat,   tracker,   rejection_column = \"testable\",   alpha = 0.05 )"},{"path":"/reference/check_consonance_property.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Consonance Property Checking for Hierarchical Testing — check_consonance_property","text":"node_dat Node data test results tracker Node tracker hierarchy structure rejection_column Column name containing rejection decisions alpha Type error rate used testing","code":""},{"path":"/reference/check_consonance_property.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Consonance Property Checking for Hierarchical Testing — check_consonance_property","text":"List consonance check results recommendations","code":""},{"path":"/reference/check_consonance_property.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Consonance Property Checking for Hierarchical Testing — check_consonance_property","text":"Goeman, J. J., & Solari, . (2011). Multiple testing exploratory research. Statistical science, 26(4), 584-597.","code":""},{"path":"/reference/check_consonance_property.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Consonance Property Checking for Hierarchical Testing — check_consonance_property","text":"","code":"if (FALSE) { # \\dontrun{ # Check consonance of find_blocks results # Requires dplyr package data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Prepare data idat <- as.data.table(example_dat) bdat <- idat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),     .groups = \"drop\"   ) %>%   as.data.table()  # Run find_blocks results <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   maxtest = 20,   trace = FALSE )  # Check consonance of the testable decisions consonance_check <- check_consonance_property(   node_dat = results$node_dat,   tracker = results$node_tracker, # Assuming tracker is available   rejection_column = \"testable\",   alpha = 0.05 )  # Examine consonance results if (consonance_check$is_consonant) {   cat(\"The hierarchical testing procedure is consonant.\\n\") } else {   cat(\"Found consonance violations:\\n\")   for (i in seq_along(consonance_check$violations)) {     violation <- consonance_check$violations[[i]]     cat(\"-\", violation$violation_description, \"\\n\")   }    # Print violation analysis   analysis <- consonance_check$violation_analysis   cat(\"\\nViolation Analysis:\\n\")   cat(\"Total violations:\", analysis$total_violations, \"\\n\")   cat(\"Severity level:\", analysis$severity, \"\\n\")   cat(\"Affected nodes:\", paste(analysis$affected_nodes, collapse = \", \"), \"\\n\") }  # Apply consonance correction if needed if (!consonance_check$is_consonant) {   cat(\"\\nApplying consonance correction...\\n\")    corrected_results <- apply_consonance_correction(     node_dat = results$node_dat,     tracker = results$node_tracker,     rejection_column = \"testable\",     correction_method = \"propagate_up\"   )    # Check if correction worked   corrected_check <- check_consonance_property(     corrected_results,     results$node_tracker,     \"testable_consonant\"   )    if (corrected_check$is_consonant) {     cat(\"Consonance correction successful.\\n\")   } else {     cat(\"Additional corrections may be needed.\\n\")   } }  # Compare original and corrected rejection counts original_rejections <- sum(results$node_dat$testable, na.rm = TRUE) if (exists(\"corrected_results\")) {   corrected_rejections <- sum(corrected_results$testable_consonant, na.rm = TRUE)   cat(\"Original rejections:\", original_rejections, \"\\n\")   cat(\"Corrected rejections:\", corrected_rejections, \"\\n\") } } # }"},{"path":"/reference/check_iu_consistency.html","id":null,"dir":"Reference","previous_headings":"","what":"Check consistency of intersection-union test results — check_iu_consistency","title":"Check consistency of intersection-union test results — check_iu_consistency","text":"Check consistency intersection-union test results","code":""},{"path":"/reference/check_iu_consistency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check consistency of intersection-union test results — check_iu_consistency","text":"","code":"check_iu_consistency(iu_results)"},{"path":"/reference/check_iu_consistency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check consistency of intersection-union test results — check_iu_consistency","text":"iu_results Results intersection_union_tests","code":""},{"path":"/reference/check_iu_consistency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check consistency of intersection-union test results — check_iu_consistency","text":"Logical vector indicating consistency","code":""},{"path":"/reference/check_stopping_rule.html","id":null,"dir":"Reference","previous_headings":"","what":"Check stopping rule — check_stopping_rule","title":"Check stopping rule — check_stopping_rule","text":"Check stopping rule","code":""},{"path":"/reference/check_stopping_rule.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check stopping rule — check_stopping_rule","text":"","code":"check_stopping_rule(wealth, e_value, alpha, stopping_rule)"},{"path":"/reference/check_stopping_rule.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check stopping rule — check_stopping_rule","text":"wealth Current wealth e_value Current e-value alpha Type error rate stopping_rule Stopping rule apply","code":""},{"path":"/reference/check_stopping_rule.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check stopping rule — check_stopping_rule","text":"Logical indicating whether stop","code":""},{"path":"/reference/closed_testing_procedure.html","id":null,"dir":"Reference","previous_headings":"","what":"Proper Closed Testing Procedure for Hierarchical Hypotheses — closed_testing_procedure","title":"Proper Closed Testing Procedure for Hierarchical Hypotheses — closed_testing_procedure","text":"implements valid closed testing procedure following Goeman's methodology ensures proper FWER control testing intersection hypotheses.","code":""},{"path":"/reference/closed_testing_procedure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Proper Closed Testing Procedure for Hierarchical Hypotheses — closed_testing_procedure","text":"","code":"closed_testing_procedure(node_dat, tracker, alpha = 0.05, method = \"simes\")"},{"path":"/reference/closed_testing_procedure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Proper Closed Testing Procedure for Hierarchical Hypotheses — closed_testing_procedure","text":"node_dat Data table node information including p-values hierarchy tracker Node tracker object tree structure alpha Overall Type error rate method Method combining p-values intersections (\"simes\", \"fisher\", \"min\")","code":""},{"path":"/reference/closed_testing_procedure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Proper Closed Testing Procedure for Hierarchical Hypotheses — closed_testing_procedure","text":"Updated node_dat valid rejection decisions","code":""},{"path":"/reference/closed_testing_procedure.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Proper Closed Testing Procedure for Hierarchical Hypotheses — closed_testing_procedure","text":"Goeman, J. J., & Solari, . (2011). Multiple testing exploratory research. Statistical science, 26(4), 584-597. Goeman, J. J., & Finos, L. (2012). inheritance procedure: multiple testing tree-structured hypotheses. Statistical Applications Genetics Molecular Biology, 11(1).","code":""},{"path":"/reference/closed_testing_procedure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Proper Closed Testing Procedure for Hierarchical Hypotheses — closed_testing_procedure","text":"","code":"if (FALSE) { # \\dontrun{ # Load example data and run find_blocks with closed testing data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Prepare data idat <- as.data.table(example_dat) bdat <- idat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),     .groups = \"drop\"   ) %>%   as.data.table()  # Run find_blocks with closed testing procedure results_closed <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_closed_testing = TRUE,   closed_testing_method = \"simes\", # Goeman & Solari recommend Simes   thealpha = 0.05,   maxtest = 20 )  # Compare with traditional approach results_traditional <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_closed_testing = FALSE,   thealpha = 0.05,   maxtest = 20 )  # Examine closed testing results if (\"closed_testing_reject\" %in% names(results_closed$node_dat)) {   closed_rejections <- results_closed$node_dat[     closed_testing_reject == TRUE,     .(nodenum, p, closed_testing_reject)   ]   cat(\"Closed testing rejections:\\n\")   print(closed_rejections) }  # Traditional rejections for comparison traditional_detections <- report_detections(results_traditional$bdat, fwer = TRUE) cat(\"Traditional FWER rejections:\", sum(traditional_detections$hit, na.rm = TRUE), \"\\n\") } # }"},{"path":"/reference/combine_pvalues.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine p-values using specified method — combine_pvalues","title":"Combine p-values using specified method — combine_pvalues","text":"Combine p-values using specified method","code":""},{"path":"/reference/combine_pvalues.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine p-values using specified method — combine_pvalues","text":"","code":"combine_pvalues(p_values, method)"},{"path":"/reference/combine_pvalues.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine p-values using specified method — combine_pvalues","text":"p_values Vector p-values method Combination method","code":""},{"path":"/reference/combine_pvalues.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine p-values using specified method — combine_pvalues","text":"Combined p-value","code":""},{"path":"/reference/combine_pvalues_traditional.html","id":null,"dir":"Reference","previous_headings":"","what":"Traditional p-value combination (used in original Meinshausen) — combine_pvalues_traditional","title":"Traditional p-value combination (used in original Meinshausen) — combine_pvalues_traditional","text":"Traditional p-value combination (used original Meinshausen)","code":""},{"path":"/reference/combine_pvalues_traditional.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Traditional p-value combination (used in original Meinshausen) — combine_pvalues_traditional","text":"","code":"combine_pvalues_traditional(p_values, method)"},{"path":"/reference/combine_pvalues_traditional.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Traditional p-value combination (used in original Meinshausen) — combine_pvalues_traditional","text":"p_values Vector p-values combine method Combination method","code":""},{"path":"/reference/combine_pvalues_traditional.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Traditional p-value combination (used in original Meinshausen) — combine_pvalues_traditional","text":"Combined p-value","code":""},{"path":"/reference/combined_distances_tstat.html","id":null,"dir":"Reference","previous_headings":"","what":"The combined distances test statistic — combined_distances_tstat","title":"The combined distances test statistic — combined_distances_tstat","text":"combined distances test statistic","code":""},{"path":"/reference/combined_distances_tstat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The combined distances test statistic — combined_distances_tstat","text":"","code":"combined_distances_tstat(   fmla = Y ~ trtF | blockF,   distfn = fast_dists_and_trans_hybrid,   dat )"},{"path":"/reference/combined_distances_tstat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The combined distances test statistic — combined_distances_tstat","text":"fmla formula specifying response, treatment, optionally block variables (e.g., Y ~ trtF | blockF) distfn distance function compute distances (default: fast_dists_and_trans_hybrid) dat data.table containing variables specified formula","code":""},{"path":"/reference/combined_distances_tstat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The combined distances test statistic — combined_distances_tstat","text":"","code":"if (FALSE) { # \\dontrun{ # Requires dat object to be defined in environment # tstat_dt <- combined_distances_tstat()  splitCluster(\"blockF\", x) ## clus <- tryCatch(KMeans_rcpp(as.matrix(x), clusters = 2, num_init = 2, ##        initializer = \"optimal_init\")$clusters, error = function(e) { ##    kmeans(x, centers = 2)$cluster})  ## Approach 2: # clus <- Ckmeans.1d.dp(x, k = 2)$cluster # group <- factor(as.numeric(clus == 1)) # } # }"},{"path":"/reference/compute_adaptive_alphas.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Adaptive Alpha Levels by Tree Depth — compute_adaptive_alphas","title":"Compute Adaptive Alpha Levels by Tree Depth — compute_adaptive_alphas","text":"Implements Algorithm 1 Appendix B supplement. First checks whether natural gating suffices (total error load \\(\\le 1\\)); , returns nominal alpha every level. Otherwise, computes adjusted significance levels compensate error load depth.","code":""},{"path":"/reference/compute_adaptive_alphas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Adaptive Alpha Levels by Tree Depth — compute_adaptive_alphas","text":"","code":"compute_adaptive_alphas(   k,   delta_hat,   N_total,   max_depth = 20L,   thealpha = 0.05 )"},{"path":"/reference/compute_adaptive_alphas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Adaptive Alpha Levels by Tree Depth — compute_adaptive_alphas","text":"k Branching factor. Either scalar (constant k levels) integer vector length max_depth k[ell] branching factor level ell. delta_hat Estimated standardized effect size (e.g., Cohen's d). Conservative (larger) values produce stringent adjustment, preserves FWER guarantee. Use upper bound true effect size. N_total Total sample size root level. max_depth Maximum depth compute (default 20). thealpha Nominal significance level (default 0.05).","code":""},{"path":"/reference/compute_adaptive_alphas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Adaptive Alpha Levels by Tree Depth — compute_adaptive_alphas","text":"Named numeric vector adjusted alpha levels, one per depth (1 max_depth). Names depth levels characters. attribute \"error_load\" containing compute_error_load result, caller can inspect whether adjustment needed.","code":""},{"path":"/reference/compute_adaptive_alphas.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Adaptive Alpha Levels by Tree Depth — compute_adaptive_alphas","text":"function first calls compute_error_load assess whether natural gating suffices. \\(\\sum G_\\ell \\le 1\\), adjustment needed nominal thealpha returned every level. adjustment needed, formula level \\(\\ell\\) : $$\\alpha_\\ell^{adj} = \\min\\left\\{\\alpha,\\;   \\frac{\\alpha}{k^{(\\ell-1)} \\cdot \\prod_{j=1}^{\\ell-1} \\hat\\theta_j}   \\right\\}$$ FWER guarantee (Theorem supplement) requires power underestimated (.e., \\(\\hat\\theta_j \\geq \\theta_j\\)). practice means using conservatively large delta_hat.","code":""},{"path":"/reference/compute_adaptive_alphas.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Adaptive Alpha Levels by Tree Depth — compute_adaptive_alphas","text":"","code":"# Natural gating sufficient: all alphas = 0.05 compute_adaptive_alphas(k = 3, delta_hat = 0.2, N_total = 100,                         max_depth = 4) #>    1    2    3    4  #> 0.05 0.05 0.05 0.05  #> attr(,\"error_load\") #> attr(,\"error_load\")$G #>          1          2          3          4  #> 0.51596779 0.32557645 0.09567467 0.01653857  #>  #> attr(,\"error_load\")$sum_G #> [1] 0.9537575 #>  #> attr(,\"error_load\")$needs_adjustment #> [1] FALSE #>  #> attr(,\"error_load\")$thetas #>          1          2          3          4  #> 0.51596779 0.21033384 0.09795412 0.05762086  #>  #> attr(,\"error_load\")$critical_level #> [1] 2 #>  #> attr(,\"error_load\")$n_by_level #>          1          2          3          4  #> 100.000000  33.333333  11.111111   3.703704  #>   # Needs adjustment: alphas shrink at deeper levels compute_adaptive_alphas(k = 4, delta_hat = 0.5, N_total = 1000,                         max_depth = 5) #>            1            2            3            4            5  #> 0.0500000000 0.0125000000 0.0031250000 0.0007997540 0.0003946938  #> attr(,\"error_load\") #> attr(,\"error_load\")$G #>        1        2        3        4        5  #>  1.00000  4.00000 15.62981 31.67012 20.97663  #>  #> attr(,\"error_load\")$sum_G #> [1] 73.27656 #>  #> attr(,\"error_load\")$needs_adjustment #> [1] TRUE #>  #> attr(,\"error_load\")$thetas #>         1         2         3         4         5  #> 1.0000000 1.0000000 0.9768629 0.5065661 0.1655869  #>  #> attr(,\"error_load\")$critical_level #> [1] 5 #>  #> attr(,\"error_load\")$n_by_level #>          1          2          3          4          5  #> 1000.00000  250.00000   62.50000   15.62500    3.90625  #>"},{"path":"/reference/compute_adaptive_alphas_tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Adaptive Alpha Levels from an Actual Tree — compute_adaptive_alphas_tree","title":"Compute Adaptive Alpha Levels from an Actual Tree — compute_adaptive_alphas_tree","text":"Tree-mode counterpart compute_adaptive_alphas. Instead assuming regular k-ary tree equal splits, function takes actual tree structure (per-node sample sizes) computes per-depth adjusted significance levels. handles irregular trees branching factor sample sizes vary across nodes.","code":""},{"path":"/reference/compute_adaptive_alphas_tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Adaptive Alpha Levels from an Actual Tree — compute_adaptive_alphas_tree","text":"","code":"compute_adaptive_alphas_tree(   node_dat,   delta_hat,   max_depth = NULL,   thealpha = 0.05 )"},{"path":"/reference/compute_adaptive_alphas_tree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Adaptive Alpha Levels from an Actual Tree — compute_adaptive_alphas_tree","text":"node_dat data.frame data.table columns nodenum, parent, depth, nodesize. Typically extracted find_blocks result. root node must parent = 0 depth = 1. delta_hat Estimated standardized effect size (e.g., Cohen's d). Conservative (larger) values produce stringent adjustment, preserves FWER guarantee. max_depth Maximum depth compute. Defaults maximum depth present node_dat. thealpha Nominal significance level (default 0.05).","code":""},{"path":"/reference/compute_adaptive_alphas_tree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Adaptive Alpha Levels from an Actual Tree — compute_adaptive_alphas_tree","text":"Named numeric vector adjusted alpha levels, one per depth (1 max_depth). Names depth levels characters. attribute \"error_load\" containing compute_error_load result tree.","code":""},{"path":"/reference/compute_adaptive_alphas_tree.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Adaptive Alpha Levels from an Actual Tree — compute_adaptive_alphas_tree","text":"algorithm mirrors compute_adaptive_alphas uses actual per-node power instead parametric assumption: Compute per-node power \\(\\theta\\) path power (product ancestor thetas) via compute_error_load. total error load \\(\\sum G_\\ell \\le 1\\): natural gating suffices, return nominal thealpha every depth. Otherwise, depth \\(d\\): $$\\alpha_d = \\min\\left\\{\\alpha,\\;       \\frac{\\alpha}{\\sum_{\\text{nodes depth } d}         \\text{path\\_power}(\\text{node})} \\right\\}$$ denominator depth \\(d\\) sum path powers nodes depth — .e., expected number tests procedure conducts depth \\(d\\). regular k-ary tree equal sample sizes, reduces \\(k^{d-1} \\prod_{j=1}^{d-1} \\theta_j\\), matching parametric formula compute_adaptive_alphas.","code":""},{"path":"/reference/compute_adaptive_alphas_tree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Adaptive Alpha Levels from an Actual Tree — compute_adaptive_alphas_tree","text":"","code":"# Build a small irregular tree nd <- data.frame(   nodenum  = 1:7,   parent   = c(0, 1, 1, 2, 2, 3, 3),   depth    = c(1, 2, 2, 3, 3, 3, 3),   nodesize = c(500, 250, 250, 125, 125, 100, 150) ) compute_adaptive_alphas_tree(node_dat = nd, delta_hat = 0.5) #>      1      2      3  #> 0.0500 0.0250 0.0125  #> attr(,\"error_load\") #> attr(,\"error_load\")$G #>        1        2        3  #> 1.000000 2.000000 3.998518  #>  #> attr(,\"error_load\")$sum_G #> [1] 6.998518 #>  #> attr(,\"error_load\")$needs_adjustment #> [1] TRUE #>  #> attr(,\"error_load\")$thetas #>         1         2         3  #> 1.0000000 1.0000000 0.9996296  #>  #> attr(,\"error_load\")$critical_level #> [1] NA #>  #> attr(,\"error_load\")$n_by_level #>   1   2   3  #> 500 250 125  #>  #> attr(,\"error_load\")$node_detail #>   nodenum parent depth nodesize     theta path_power #> 1       1      0     1      500 1.0000000          1 #> 2       2      1     2      250 1.0000000          1 #> 3       3      1     2      250 1.0000000          1 #> 4       4      2     3      125 0.9998584          1 #> 5       5      2     3      125 0.9998584          1 #> 6       6      3     3      100 0.9988173          1 #> 7       7      3     3      150 0.9999843          1 #>"},{"path":"/reference/compute_error_load.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Error Load for Natural Gating Assessment — compute_error_load","title":"Compute Error Load for Natural Gating Assessment — compute_error_load","text":"Computes error load tree level, measures expected number -null sibling groups tested. total error load 1, unadjusted procedure (fixed alpha every node) controls FWER level alpha — adjustment needed. exceeds 1, adaptive alpha adjustment required.","code":""},{"path":"/reference/compute_error_load.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Error Load for Natural Gating Assessment — compute_error_load","text":"","code":"compute_error_load(   k = NULL,   delta_hat,   N_total = NULL,   node_dat = NULL,   max_depth = 20L,   thealpha = 0.05 )"},{"path":"/reference/compute_error_load.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Error Load for Natural Gating Assessment — compute_error_load","text":"k Branching factor. Either scalar (constant k levels) integer vector length max_depth k[ell] branching factor level ell. Used parametric mode; ignored node_dat provided. delta_hat Estimated standardized effect size (e.g., Cohen's d). Used compute power level via normal approximation. N_total Total sample size root level. Used parametric mode; ignored node_dat provided. node_dat Optional data.frame data.table columns nodenum, parent, depth, nodesize. provided, function computes per-node power nodesize aggregates error load depth. supports irregular trees (e.g., DPP design unequal splits). max_depth Maximum tree depth compute. parametric mode, defaults 20. tree mode, inferred node_dat. thealpha Nominal significance level (default 0.05).","code":""},{"path":"/reference/compute_error_load.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Error Load for Natural Gating Assessment — compute_error_load","text":"named list components: G Numeric vector error load level (length max_depth). sum_G Total error load: sum(G). needs_adjustment Logical: TRUE sum_G > 1. thetas Estimated power (conditional rejection probability) level. critical_level First level theta < 1/k (natural gating begins dominate), NA theta never drops 1/k. n_by_level Sample size level.","code":""},{"path":"/reference/compute_error_load.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Error Load for Natural Gating Assessment — compute_error_load","text":"Two interfaces provided. parametric interface (k, delta_hat, N_total) assumes regular k-ary tree equal splitting: sample size level ell \\(N / k^{ell-1}\\). tree interface (node_dat) accepts per-node sample sizes actual (possibly irregular) tree, returned find_blocks. parametric mode, error load level ell : $$G_\\ell = k^{\\ell-1} \\prod_{j=0}^{\\ell-1} \\theta_j$$ \\(\\theta_j = \\Phi(\\delta \\sqrt{n_j} - z_{1-\\alpha/2})\\) \\(n_j = N / k^{j}\\). tree mode, function computes per-node power actual sample sizes. node depth d, computes \\(\\theta = \\Phi(\\delta \\sqrt{n_{\\text{node}}} - z_{1-\\alpha/2})\\). error load depth d sum nodes depth d product theta values along path root node's parent, generalizes regular-tree formula irregular branching.","code":""},{"path":"/reference/compute_error_load.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Error Load for Natural Gating Assessment — compute_error_load","text":"","code":"# Parametric mode: regular 3-ary tree, moderate effect compute_error_load(k = 3, delta_hat = 0.3, N_total = 500) #> $G #>            1            2            3            4            5            6  #> 9.999990e-01 2.916379e+00 5.326176e+00 4.022713e+00 1.354743e+00 2.562653e-01  #>            7            8            9           10           11           12  #> 3.343749e-02 3.475357e-03 3.153782e-04 2.642414e-05 2.113216e-06 1.644896e-07  #>           13           14           15           16           17           18  #> 1.260452e-08 9.571413e-10 7.230190e-11 5.445132e-12 4.093620e-13 3.074454e-14  #>           19           20  #> 2.307679e-15 1.731555e-16  #>  #> $sum_G #> [1] 14.91353 #>  #> $needs_adjustment #> [1] TRUE #>  #> $thetas #>          1          2          3          4          5          6          7  #> 0.99999897 0.97212721 0.60876590 0.25175746 0.11225782 0.06305386 0.04349332  #>          8          9         10         11         12         13         14  #> 0.03464531 0.03024900 0.02792852 0.02665765 0.02594617 0.02554268 0.02531212  #>         15         16         17         18         19         20  #> 0.02517981 0.02510368 0.02505982 0.02503452 0.02501993 0.02501150  #>  #> $critical_level #> [1] 4 #>  #> $n_by_level #>            1            2            3            4            5            6  #> 5.000000e+02 1.666667e+02 5.555556e+01 1.851852e+01 6.172840e+00 2.057613e+00  #>            7            8            9           10           11           12  #> 6.858711e-01 2.286237e-01 7.620790e-02 2.540263e-02 8.467544e-03 2.822515e-03  #>           13           14           15           16           17           18  #> 9.408382e-04 3.136127e-04 1.045376e-04 3.484586e-05 1.161529e-05 3.871762e-06  #>           19           20  #> 1.290587e-06 4.301958e-07  #>   # High power, wide tree: needs adjustment res <- compute_error_load(k = 10, delta_hat = 0.5, N_total = 5000,                           max_depth = 3) res$sum_G     # likely > 1 #> [1] 105.2438 res$needs_adjustment #> [1] TRUE"},{"path":"/reference/compute_evalue.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute e-value for a single test — compute_evalue","title":"Compute e-value for a single test — compute_evalue","text":"Compute e-value single test","code":""},{"path":"/reference/compute_evalue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute e-value for a single test — compute_evalue","text":"","code":"compute_evalue(data, outcome, treatment, wealth_rule)"},{"path":"/reference/compute_evalue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute e-value for a single test — compute_evalue","text":"data Block data outcome Outcome variable name treatment Treatment variable name wealth_rule Wealth rule betting","code":""},{"path":"/reference/compute_evalue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute e-value for a single test — compute_evalue","text":"E-value","code":""},{"path":"/reference/compute_exact_wilcoxon_bound.html","id":null,"dir":"Reference","previous_headings":"","what":"Exact Wilcoxon bound computation (simplified) — compute_exact_wilcoxon_bound","title":"Exact Wilcoxon bound computation (simplified) — compute_exact_wilcoxon_bound","text":"Exact Wilcoxon bound computation (simplified)","code":""},{"path":"/reference/compute_exact_wilcoxon_bound.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exact Wilcoxon bound computation (simplified) — compute_exact_wilcoxon_bound","text":"","code":"compute_exact_wilcoxon_bound(w_plus, total_pairs, prob)"},{"path":"/reference/compute_exact_wilcoxon_bound.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exact Wilcoxon bound computation (simplified) — compute_exact_wilcoxon_bound","text":"w_plus Number positive pairwise differences total_pairs Total number pairs prob Probability parameter","code":""},{"path":"/reference/compute_exact_wilcoxon_bound.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exact Wilcoxon bound computation (simplified) — compute_exact_wilcoxon_bound","text":"P-value bound","code":""},{"path":"/reference/compute_sensitivity_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute sensitivity bounds for a single block — compute_sensitivity_bounds","title":"Compute sensitivity bounds for a single block — compute_sensitivity_bounds","text":"Compute sensitivity bounds single block","code":""},{"path":"/reference/compute_sensitivity_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute sensitivity bounds for a single block — compute_sensitivity_bounds","text":"","code":"compute_sensitivity_bounds(data, outcome, treatment, gamma, test_statistic)"},{"path":"/reference/compute_sensitivity_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute sensitivity bounds for a single block — compute_sensitivity_bounds","text":"data Block data outcome Outcome variable name treatment Treatment variable name gamma Sensitivity parameter test_statistic Type test statistic","code":""},{"path":"/reference/compute_sensitivity_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute sensitivity bounds for a single block — compute_sensitivity_bounds","text":"List upper lower p-value bounds","code":""},{"path":"/reference/compute_sensitivity_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute summary statistics for sensitivity analysis — compute_sensitivity_summary","title":"Compute summary statistics for sensitivity analysis — compute_sensitivity_summary","text":"Compute summary statistics sensitivity analysis","code":""},{"path":"/reference/compute_sensitivity_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute summary statistics for sensitivity analysis — compute_sensitivity_summary","text":"","code":"compute_sensitivity_summary(sensitivity_results, alpha)"},{"path":"/reference/compute_sensitivity_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute summary statistics for sensitivity analysis — compute_sensitivity_summary","text":"sensitivity_results Data frame sensitivity results alpha Type error rate","code":""},{"path":"/reference/compute_sensitivity_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute summary statistics for sensitivity analysis — compute_sensitivity_summary","text":"Summary statistics","code":""},{"path":"/reference/compute_t_statistic.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute t-statistic for two-sample test — compute_t_statistic","title":"Compute t-statistic for two-sample test — compute_t_statistic","text":"Compute t-statistic two-sample test","code":""},{"path":"/reference/compute_t_statistic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute t-statistic for two-sample test — compute_t_statistic","text":"","code":"compute_t_statistic(y, trt)"},{"path":"/reference/compute_t_statistic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute t-statistic for two-sample test — compute_t_statistic","text":"y Outcomes trt Treatment indicators (0/1)","code":""},{"path":"/reference/compute_t_statistic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute t-statistic for two-sample test — compute_t_statistic","text":"T-statistic","code":""},{"path":"/reference/create_node_tracker.html","id":null,"dir":"Reference","previous_headings":"","what":"Create node tracker object — create_node_tracker","title":"Create node tracker object — create_node_tracker","text":"Create node tracker object","code":""},{"path":"/reference/create_node_tracker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create node tracker object — create_node_tracker","text":"","code":"create_node_tracker()"},{"path":"/reference/create_node_tracker.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create node tracker object — create_node_tracker","text":"List tracker data.table next_id counter","code":""},{"path":"/reference/design_sensitivity_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Design Sensitivity Analysis for Hierarchical Testing — design_sensitivity_analysis","title":"Design Sensitivity Analysis for Hierarchical Testing — design_sensitivity_analysis","text":"Implementation Rosenbaum's design sensitivity analysis assess robustness treatment effect findings unobserved confounding. extends standard single-study approach hierarchical testing.","code":""},{"path":"/reference/design_sensitivity_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Design Sensitivity Analysis for Hierarchical Testing — design_sensitivity_analysis","text":"","code":"design_sensitivity_analysis(   idat,   bdat,   blockid,   formula,   gamma_range = seq(1, 3, by = 0.1),   test_statistic = \"wilcoxon\",   alpha = 0.05 )"},{"path":"/reference/design_sensitivity_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Design Sensitivity Analysis for Hierarchical Testing — design_sensitivity_analysis","text":"idat Individual-level data bdat Block-level data blockid Block identifier formula Test formula gamma_range Vector sensitivity parameters test test_statistic Type test statistic (\"wilcoxon\", \"sign\", \"hodges_lehmann\") alpha Type error rate","code":""},{"path":"/reference/design_sensitivity_analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Design Sensitivity Analysis for Hierarchical Testing — design_sensitivity_analysis","text":"Design sensitivity analysis results","code":""},{"path":"/reference/design_sensitivity_analysis.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Design Sensitivity Analysis for Hierarchical Testing — design_sensitivity_analysis","text":"Rosenbaum, P. R. (2017). Observation experiment: introduction causal inference. Harvard University Press.","code":""},{"path":"/reference/dists_and_trans.html","id":null,"dir":"Reference","previous_headings":"","what":"Outcome distances and transformations — dists_and_trans","title":"Outcome distances and transformations — dists_and_trans","text":"Outcome distances transformations","code":""},{"path":"/reference/dists_and_trans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome distances and transformations — dists_and_trans","text":"","code":"dists_and_trans(x)"},{"path":"/reference/dists_and_trans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome distances and transformations — dists_and_trans","text":"x numeric vector ( outcome variable)","code":""},{"path":"/reference/dists_and_trans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outcome distances and transformations — dists_and_trans","text":"list  inclusion data.table  distances unit units well transformations distances","code":""},{"path":"/reference/dists_and_trans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Outcome distances and transformations — dists_and_trans","text":"","code":"# Example with continuous outcome data outcome <- c(2.1, 5.3, 1.8, 7.2, 3.4, 4.6)  # Compute distance-based transformations dist_results <- dists_and_trans(outcome)  # View components str(dist_results) #> List of 5 #>  $ mean_dist     : num [1:6] 2.48 2.24 2.72 3.76 1.96 1.96 #>  $ mean_rank_dist: num [1:6] 2.2 2.2 3 3 1.8 1.8 #>  $ max_dist      : num [1:6] 5.1 3.5 5.4 5.4 3.8 2.8 #>  $ rankY         : num [1:6] 2 5 1 6 3 4 #>  $ tanhY         : num [1:6] 0.97 1 0.947 1 0.998 ... print(dist_results$mean_dist) # Mean distance to all other units #> [1] 2.48 2.24 2.72 3.76 1.96 1.96 print(dist_results$rankY) # Ranks of original values #> [1] 2 5 1 6 3 4 print(dist_results$tanhY) # Hyperbolic tangent transformation #> [1] 0.9704519 0.9999502 0.9468060 0.9999989 0.9977749 0.9997979"},{"path":"/reference/dot-error_load_from_tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute error load from an actual tree structure (internal) — .error_load_from_tree","title":"Compute error load from an actual tree structure (internal) — .error_load_from_tree","text":"Compute error load actual tree structure (internal)","code":""},{"path":"/reference/dot-error_load_from_tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute error load from an actual tree structure (internal) — .error_load_from_tree","text":"","code":".error_load_from_tree(node_dat, delta_hat, z_crit, thealpha)"},{"path":"/reference/dot-error_load_from_tree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute error load from an actual tree structure (internal) — .error_load_from_tree","text":"node_dat data.frame nodenum, parent, depth, nodesize delta_hat Standardized effect size z_crit Critical z-value thealpha Nominal alpha","code":""},{"path":"/reference/dot-error_load_from_tree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute error load from an actual tree structure (internal) — .error_load_from_tree","text":"structure compute_error_load","code":""},{"path":"/reference/edisti.html","id":null,"dir":"Reference","previous_headings":"","what":"Outcome e-distances between treatment arms — edisti","title":"Outcome e-distances between treatment arms — edisti","text":"Outcome e-distances treatment arms","code":""},{"path":"/reference/edisti.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome e-distances between treatment arms — edisti","text":"","code":"edisti(x, Z)"},{"path":"/reference/edisti.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome e-distances between treatment arms — edisti","text":"x numeric vector ( outcome variable) Z binary numeric vector  factor vector   two values","code":""},{"path":"/reference/edisti.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outcome e-distances between treatment arms — edisti","text":"Vector  individual level components energy distances unit units  control condition","code":""},{"path":"/reference/edisti.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Outcome e-distances between treatment arms — edisti","text":"","code":"# Example with treatment and control groups outcome <- c(2.1, 5.3, 1.8, 7.2, 3.4, 4.6, 6.1, 2.8) treatment <- c(0, 1, 0, 1, 0, 1, 1, 0) # Binary treatment indicator  # Compute energy distances e_dists <- edisti(outcome, treatment) print(e_dists) #>      1      2      3      4      5      6      7      8  #> 1.5625 0.9625 1.6375 1.6375 0.7625 0.4375 1.3625 1.2125   if (FALSE) { # \\dontrun{ # With factor treatment variable treatment_factor <- factor(treatment, labels = c(\"Control\", \"Treatment\")) e_dists2 <- edisti(outcome, treatment_factor) print(e_dists2) } # }"},{"path":"/reference/evalue_confidence_sequence.html","id":null,"dir":"Reference","previous_headings":"","what":"E-value based confidence sequences — evalue_confidence_sequence","title":"E-value based confidence sequences — evalue_confidence_sequence","text":"E-value based confidence sequences","code":""},{"path":"/reference/evalue_confidence_sequence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"E-value based confidence sequences — evalue_confidence_sequence","text":"","code":"evalue_confidence_sequence(e_values, alpha = 0.05)"},{"path":"/reference/evalue_confidence_sequence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"E-value based confidence sequences — evalue_confidence_sequence","text":"e_values Vector e-values sequential testing alpha Type error rate","code":""},{"path":"/reference/evalue_confidence_sequence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"E-value based confidence sequences — evalue_confidence_sequence","text":"Confidence sequence boundaries","code":""},{"path":"/reference/evalue_sequential_test.html","id":null,"dir":"Reference","previous_headings":"","what":"E-value Based Sequential Testing — evalue_sequential_test","title":"E-value Based Sequential Testing — evalue_sequential_test","text":"Implementation e-value methodology sequential hypothesis testing following framework Ramdas et al. provides anytime-valid inference optional stopping flexible error control.","code":""},{"path":"/reference/evalue_sequential_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"E-value Based Sequential Testing — evalue_sequential_test","text":"","code":"evalue_sequential_test(   data,   formula,   blocks,   alpha = 0.05,   wealth_rule = \"kelly\",   stopping_rule = \"wealth_threshold\" )"},{"path":"/reference/evalue_sequential_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"E-value Based Sequential Testing — evalue_sequential_test","text":"data Individual-level data formula Test formula blocks Block identifiers alpha Type error rate wealth_rule Wealth accumulation rule (\"kelly\", \"fixed\", \"adaptive\") stopping_rule Early stopping rule (\"never\", \"wealth_threshold\", \"e_threshold\")","code":""},{"path":"/reference/evalue_sequential_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"E-value Based Sequential Testing — evalue_sequential_test","text":"E-value based test results","code":""},{"path":"/reference/evalue_sequential_test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"E-value Based Sequential Testing — evalue_sequential_test","text":"Ramdas, ., Grünwald, P., Vovk, V., & Shafer, G. (2023). Game-theoretic statistics safe anytime-valid inference. Statistical Science, 38(4), 576-601.","code":""},{"path":"/reference/evalue_sequential_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"E-value Based Sequential Testing — evalue_sequential_test","text":"","code":"if (FALSE) { # \\dontrun{ # Load example data for e-value testing with find_blocks data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Prepare data idat <- as.data.table(example_dat) bdat <- idat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),     .groups = \"drop\"   ) %>%   as.data.table()  # Run find_blocks with e-value methodology (experimental) # Note: This provides anytime-valid inference results_evalues <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_evalues = TRUE,   evalue_wealth_rule = \"kelly\", # Ramdas recommends Kelly betting   thealpha = 0.05,   maxtest = 15 )  # Direct use of e-value sequential testing evalue_results <- evalue_sequential_test(   data = idat,   formula = Y1 ~ trtF,   blocks = idat$blockF,   alpha = 0.05,   wealth_rule = \"kelly\",   stopping_rule = \"wealth_threshold\" )  # Examine e-value results cat(\"E-value sequential testing results:\\n\") cat(\"Final wealth:\", evalue_results$final_wealth, \"\\n\") cat(\"Total rejections:\", evalue_results$total_rejections, \"\\n\") cat(\"Stopped early:\", evalue_results$stopped_early, \"\\n\")  # Plot wealth accumulation over time if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)   wealth_plot <- ggplot(     evalue_results$results,     aes(x = seq_along(block_id), y = wealth)   ) +     geom_line() +     geom_hline(yintercept = 1 / 0.05, linetype = \"dashed\", color = \"red\") +     labs(       x = \"Sequential Test Number\", y = \"Wealth\",       title = \"E-value Wealth Accumulation\",       subtitle = \"Red line shows rejection threshold (1/alpha)\"     )   print(wealth_plot) }  # Compare e-values with traditional p-values comparison_data <- data.frame(   test = seq_along(evalue_results$results$e_value),   e_value = evalue_results$results$e_value,   p_value = evalue_results$results$p_value,   wealth = evalue_results$results$wealth )  cat(\"E-value vs P-value comparison (first 5 tests):\\n\") print(head(comparison_data, 5)) } # }"},{"path":"/reference/evalue_to_pvalue.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert e-value to p-value — evalue_to_pvalue","title":"Convert e-value to p-value — evalue_to_pvalue","text":"Convert e-value p-value","code":""},{"path":"/reference/evalue_to_pvalue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert e-value to p-value — evalue_to_pvalue","text":"","code":"evalue_to_pvalue(e_value)"},{"path":"/reference/evalue_to_pvalue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert e-value to p-value — evalue_to_pvalue","text":"e_value E-value","code":""},{"path":"/reference/evalue_to_pvalue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert e-value to p-value — evalue_to_pvalue","text":"Corresponding p-value","code":""},{"path":"/reference/example_dat.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Data for Block-Randomized Experiment — example_dat","title":"Example Data for Block-Randomized Experiment — example_dat","text":"simulated dataset demonstrating block-randomized experiment two outcome variables. dataset used throughout package examples illustrate use various testing splitting procedures.","code":""},{"path":"/reference/example_dat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example Data for Block-Randomized Experiment — example_dat","text":"","code":"example_dat"},{"path":"/reference/example_dat.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Data for Block-Randomized Experiment — example_dat","text":"data.table 1268 rows 9 variables: id Integer. Unique identifier unit. year Integer. Year indicator (1 3). trt Integer. Binary treatment indicator (0 = control, 1 = treated). Y1 Numeric. First outcome variable. Y2 Numeric. Second outcome variable. trtF Factor. Treatment indicator factor levels \"0\" \"1\". place_year_block Character. Combined identifier place, year, block. place Character. Place/location identifier (, B, C, etc.). blockF Factor. Block identifier 44 levels (B080 B123).","code":""},{"path":"/reference/example_dat.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example Data for Block-Randomized Experiment — example_dat","text":"Simulated data demonstration purposes","code":""},{"path":"/reference/example_dat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Data for Block-Randomized Experiment — example_dat","text":"","code":"data(example_dat, package = \"manytestsr\") head(example_dat) #>       id  year   trt    Y1    Y2   trtF place_year_block  place blockF #>    <int> <int> <int> <num> <num> <fctr>           <char> <char> <fctr> #> 1:     1     1     0     0     0      0         A.1.B082      A   B082 #> 2:     2     3     0     0    12      0         B.3.B094      B   B094 #> 3:     3     1     0     0     0      0         C.1.B097      C   B097 #> 4:     4     1     0     6     0      0         C.1.B097      C   B097 #> 5:     5     1     0     7    11      0         B.1.B089      B   B089 #> 6:     6     1     1     0     0      1         A.1.B080      A   B080 table(example_dat$trt) #>  #>   0   1  #> 439 829  table(example_dat$blockF) #>  #> B080 B081 B082 B083 B084 B085 B086 B087 B088 B089 B090 B091 B092 B093 B094 B095  #>  129   68   56    8    9   53  167   60   23   62    4    6    2    5   42   14  #> B096 B097 B098 B099 B100 B101 B102 B103 B104 B105 B106 B107 B108 B109 B110 B111  #>   14   52   20   12    9    4   41   23   21    3   34    3    3    2    4   38  #> B112 B113 B114 B115 B116 B117 B118 B119 B120 B121 B122 B123  #>   10    9    2   54   24   27   14   13   34   21   50   19"},{"path":"/reference/extract_hierarchy_relationships.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract hierarchy relationships — extract_hierarchy_relationships","title":"Extract hierarchy relationships — extract_hierarchy_relationships","text":"Extract hierarchy relationships","code":""},{"path":"/reference/extract_hierarchy_relationships.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract hierarchy relationships — extract_hierarchy_relationships","text":"","code":"extract_hierarchy_relationships(node_dat, tracker)"},{"path":"/reference/extract_hierarchy_relationships.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract hierarchy relationships — extract_hierarchy_relationships","text":"node_dat Node data table tracker Node tracker object","code":""},{"path":"/reference/extract_hierarchy_relationships.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract hierarchy relationships — extract_hierarchy_relationships","text":"List hierarchy relationships","code":""},{"path":"/reference/fast_dists_and_trans_hybrid.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast per-unit distance summaries (scalar outcome) — fast_dists_and_trans_hybrid","title":"Fast per-unit distance summaries (scalar outcome) — fast_dists_and_trans_hybrid","text":"Computes, observation \\(= 1,\\dots,n\\) numeric vector \\(x\\), scalar summaries plus simple transforms, without ever forming full \\(n\\times n\\) distance matrix.","code":""},{"path":"/reference/fast_dists_and_trans_hybrid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast per-unit distance summaries (scalar outcome) — fast_dists_and_trans_hybrid","text":"","code":"fast_dists_and_trans_hybrid(x)"},{"path":"/reference/fast_dists_and_trans_hybrid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast per-unit distance summaries (scalar outcome) — fast_dists_and_trans_hybrid","text":"x Numeric vector – scalar outcome units.","code":""},{"path":"/reference/fast_dists_and_trans_hybrid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast per-unit distance summaries (scalar outcome) — fast_dists_and_trans_hybrid","text":"named list components mean_dist numeric vector, length n. mean_rank_dist numeric vector, length n. max_dist numeric vector, length n. rankY average ranks (mid-ranks). tanhY \\(\\tanh(x_i)\\) values.","code":""},{"path":"/reference/fast_dists_and_trans_hybrid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast per-unit distance summaries (scalar outcome) — fast_dists_and_trans_hybrid","text":"mean_dist – mean absolute distance $$\\frac{1}{n-1}\\sum_{j\\neq }|x_i-x_j|$$ mean_rank_dist – mean mid-rank scale; closed-form, second loop. max_dist – maximum absolute distance \\(\\max\\{\\,x_i-\\min(x),\\;\\max(x)-x_i\\,\\}\\) rankY – average (mid-) rank x (ties=\"average\"). tanhY – element-wise \\(\\tanh(x_i)\\) shrink transform. Complexity \\(O(n \\log n)\\) time (sorting + prefix sums) \\(O(n)\\)  space (vectors length n) n = 10\\,000 typically runs ≈2 ms Apple M-series core < 0.5 MB peak RAM, much faster far lighter allocating full distance matrix.","code":""},{"path":"/reference/fast_dists_and_trans_hybrid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast per-unit distance summaries (scalar outcome) — fast_dists_and_trans_hybrid","text":"","code":"set.seed(1) x <- rnorm(8) fast_dists_and_trans_hybrid(x) #> $mean_dist #> [1] 0.9813777 0.7499214 1.1052377 1.6729445 0.7499214 1.0922432 0.7950418 #> [8] 0.9384107 #>  #> $mean_rank_dist #> [1] 2.571429 2.285714 4.000000 4.000000 2.285714 3.142857 2.571429 3.142857 #>  #> $max_dist #> [1] 2.221735 1.411637 2.430909 2.430909 1.265773 2.415749 1.323058 1.573953 #>  #> $rankY #> [1] 3 4 1 8 5 2 6 7 #>  #> $tanhY #> [1] -0.5556056  0.1816063 -0.6834867  0.9209551  0.3180784 -0.6753247  0.4521735 #> [8]  0.6281319 #>   ## compare to explicit distance matrix (slow / big): dx <- abs(outer(x, x, \"-\")) mean_dist_ref <- colSums(dx) / (length(x) - 1) stopifnot(all.equal(fast_dists_and_trans_hybrid(x)$mean_dist,                     mean_dist_ref))"},{"path":"/reference/fast_dists_and_trans_new_parallel.html","id":null,"dir":"Reference","previous_headings":"","what":"Outcome distances and transformations: C++ OpenMP Parallel version — fast_dists_and_trans_new_parallel","title":"Outcome distances and transformations: C++ OpenMP Parallel version — fast_dists_and_trans_new_parallel","text":"Outcome distances transformations: C++ OpenMP Parallel version","code":""},{"path":"/reference/fast_dists_and_trans_new_parallel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome distances and transformations: C++ OpenMP Parallel version — fast_dists_and_trans_new_parallel","text":"","code":"fast_dists_and_trans_new_parallel(threads)"},{"path":"/reference/fast_dists_and_trans_new_parallel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome distances and transformations: C++ OpenMP Parallel version — fast_dists_and_trans_new_parallel","text":"threads integer number cores use.","code":""},{"path":"/reference/fast_dists_and_trans_new_parallel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outcome distances and transformations: C++ OpenMP Parallel version — fast_dists_and_trans_new_parallel","text":"distance creation function taking x (numeric vector, usually outcome) variable Z used.","code":""},{"path":"/reference/find_all_descendants.html","id":null,"dir":"Reference","previous_headings":"","what":"Find all descendants of a node — find_all_descendants","title":"Find all descendants of a node — find_all_descendants","text":"Find descendants node","code":""},{"path":"/reference/find_all_descendants.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find all descendants of a node — find_all_descendants","text":"","code":"find_all_descendants(node_id, tracker, all_nodes)"},{"path":"/reference/find_all_descendants.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find all descendants of a node — find_all_descendants","text":"node_id Node find descendants tracker Node tracker object all_nodes Vector node IDs","code":""},{"path":"/reference/find_all_descendants.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find all descendants of a node — find_all_descendants","text":"Vector descendant node IDs","code":""},{"path":"/reference/find_blocks.html","id":null,"dir":"Reference","previous_headings":"","what":"Test, Split, Repeat — find_blocks","title":"Test, Split, Repeat — find_blocks","text":"Split test.","code":""},{"path":"/reference/find_blocks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test, Split, Repeat — find_blocks","text":"","code":"find_blocks(   idat,   bdat,   blockid = \"block\",   splitfn,   pfn,   alphafn = NULL,   local_adj_p_fn = NULL,   simthresh = 20,   sims = 1000,   maxtest = 2000,   thealpha = 0.05,   thew0 = 0.05 - 0.001,   fmla = YContNorm ~ trtF | blockF,   parallel = \"multicore\",   ncores = 4,   copydts = FALSE,   splitby = \"hwt\",   stop_splitby_constant = TRUE,   blocksize = \"hwt\",   return_what = c(\"blocks\", \"nodes\"),   trace = FALSE,   use_closed_testing = FALSE,   use_evalues = FALSE,   use_meinshausen = FALSE,   closed_testing_method = \"simes\",   evalue_wealth_rule = \"kelly\",   meinshausen_method = \"simes\",   meinshausen_sequential = TRUE )"},{"path":"/reference/find_blocks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test, Split, Repeat — find_blocks","text":"idat Data unit level. bdat Data block level. blockid character name column idat bdat indicating block. splitfn function split data two pieces — using bdat pfn function produce pvalues — using idat. alphafn function adjust alpha step. Takes one p-values plus stratum batch indicator. Currently alpha_investing, alpha_saffron, alpha_addis accepted. wrap corresponding functions onlineFDR package. local_adj_p_fn Function. function adjusts p-values node (e.g. local_simes). simthresh number total observations p-value functions use permutations rather asymptotic approximations sims Number permutations permutation-based testing maxtest Maximum splits tests . probably smaller number experimental blocks. thealpha error rate given test (cases alphafn NULL, starting alpha alphafn null) thew0 starting \"wealth\" alpha investing procedure (relevant alphafn null). fmla formula outcome~treatment assignment  | block treatment assignment block must factors. parallel pfn use multicore processing permutation based testing. Default . \"snow\" \"multicore\" following approximate coin package. ncores number cores used parallel processing copydts TRUE FALSE. TRUE using find_blocks standalone. FALSE copied objects sent find_blocks functions. splitby string indicating column bdat contains variable guide splitting (example, column block sizes block harmonic mean weights column covariate (function covariates) column factor levels separated \".\" indicates pre-specified series splits (see splitSpecifiedFactor)) stop_splitby_constant TRUE splitting stop splitby constant within given branch tree. FALSE splitting continue even splitby constant. Default TRUE. Different combinations splitby, splitfn, stop_splitby_constant make less sense described . blocksize string name column bdat contains information size block (determinant power tests within block, harmonic mean weight block variance outcome within block.) return_what Character. Return data.table blocks \"blocks\", data.table nodes \"nodes\", default c(\"blocks\",\"nodes\"). trace Logical, FALSE (default) print split number. TRUE prints split number. use_closed_testing Logical, whether apply proper closed testing procedure (Goeman methodology) use_evalues Logical, whether use e-value methodology sequential testing (Ramdas approach) use_meinshausen Logical, whether apply Meinshausen's hierarchical testing (2008) sequential rejection closed_testing_method Method combining p-values intersection tests (\"simes\", \"fisher\", \"min\") evalue_wealth_rule Wealth rule e-value betting (\"kelly\", \"fixed\", \"adaptive\") meinshausen_method Method Meinshausen hierarchical testing (\"simes\", \"fisher\", \"bonferroni\") meinshausen_sequential Logical, whether use Goeman-Solari sequential rejection enhancement","code":""},{"path":"/reference/find_blocks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test, Split, Repeat — find_blocks","text":"data.table containing information sequence splitting testing","code":""},{"path":"/reference/find_blocks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test, Split, Repeat — find_blocks","text":"notes splitting functions relate splitting criteria (splitby) stopping criteria (stop_splitby_constant). splitCluster() splits blocks groups similar possible splitby using kmeans clustering algorithm (using combination kmeans() ClusterR::KMeans_rcpp()). work factor variables. splitting criteria constant, return random splits roughly two equal sized groups blocks stop_splitby_constant=FALSE. stop_splitby_constant=TRUE find_blocks() stop return groups blocks detected . splitSpecifiedFactor() split blocks two groups following prespecified pattern encoded labels levels factor. example, imagine three nested levels splitting (like states, districts, neighborhoods), factor labels like category1_level1.category2_level1.category3_level1 splits occur left right depending whether existing variation level. factor constant stop_splitby_constant=TRUE splitting stops. reason recommend right-label factor individual blocks —ensure testing descends block level can. stop_splitby_constant=FALSE, uses random splits. splitSpecifiedFactorMulti() split blocks two groups following prespecified pattern encoded labels levels factor. example, imagine three nested levels splitting (like states, districts, neighborhoods), factor labels like category1_level1.category2_level1.category3_level1 splits occur left right depending whether existing variation level. reason recommend right-label factor individual blocks —ensure testing descends block level can. factor constant stop_splitby_constant=TRUE splitting stops. stop_splitby_constant=FALSE, uses random splits. splitEqualApprox() splits sets blocks two groups sum splitby vector approximately split. example, splitby number units block, splitting function makes two groups blocks, group total number units. splitting function work discrete factors : rank_splitby <- rank(splitby) divide blocks groups based taking every rank. , factors variables categories ordered, allocate every category one another group. splitLOO() chooses blocks largest splitby vector one time two tests, one focusing highest ranked block one rest blocks (example, block units versus rest blocks). splitby vector ties, chooses one block random among tied first largest rank. split vector values, example, two values, still split assuming vector numeric (, 1 ranked higher 0) randomly among ties. stop_splitby_constant=TRUE, algorithm stop exhausting blocks higher ranked category (thinking binary splitby case). reason advise using splitLOO factor splitby vector categories. splitLOO() best used splitby vector like block-size — constant thus just create random choice single block vary thus focus testing largest/highest ranked blocks.","code":""},{"path":"/reference/find_blocks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test, Split, Repeat — find_blocks","text":"","code":"if (FALSE) { # \\dontrun{ # Load example data data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Create block-level dataset example_bdat <- example_dat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(example_dat)) * (pb * (1 - pb)),     place = unique(place),     year = unique(year),     place_year_block = factor(unique(place_year_block)),     .groups = \"drop\"   ) %>%   as.data.table()  # Basic usage with cluster-based splitting result1 <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   trace = TRUE )  # Access block-level results head(result1$bdat)  # Access node-level results head(result1$node_dat)  # Example with pre-specified factor splitting result2 <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitSpecifiedFactor,   pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF,   splitby = \"place_year_block\",   parallel = \"no\" )  # Example using Goeman's proper closed testing procedure result3 <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_closed_testing = TRUE,   closed_testing_method = \"simes\",   thealpha = 0.05 )  # Check closed testing results if (\"closed_testing_reject\" %in% names(result3$node_dat)) {   cat(\"Nodes rejected by closed testing procedure:\\n\")   rejected_nodes <- result3$node_dat[closed_testing_reject == TRUE, nodenum]   print(rejected_nodes) }  # Example using e-value methodology for sequential testing result4 <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_evalues = TRUE,   evalue_wealth_rule = \"kelly\",   thealpha = 0.05 )  # Example using Meinshausen's hierarchical testing with sequential rejection result5 <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y2 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   use_meinshausen = TRUE,   meinshausen_method = \"simes\",   meinshausen_sequential = TRUE,   thealpha = 0.05 )  # Check Meinshausen testing results if (\"meinshausen_reject\" %in% names(result5$node_dat)) {   cat(\"Nodes rejected by Meinshausen hierarchical procedure:\\\\n\")   rejected_meinshausen <- result5$node_dat[meinshausen_reject == TRUE, nodenum]   print(rejected_meinshausen) }  # Compare traditional FWER control vs closed testing traditional_detections <- report_detections(result1$bdat, fwer = TRUE) if (exists(\"result3\") && \"node_dat\" %in% names(result3)) {   cat(\"Traditional FWER detections:\", sum(traditional_detections$hit, na.rm = TRUE), \"\\n\")   if (\"closed_testing_reject\" %in% names(result3$node_dat)) {     closed_detections <- sum(result3$node_dat$closed_testing_reject, na.rm = TRUE)     cat(\"Closed testing detections:\", closed_detections, \"\\n\")   } } } # }"},{"path":"/reference/find_consonance_violations.html","id":null,"dir":"Reference","previous_headings":"","what":"Find consonance violations — find_consonance_violations","title":"Find consonance violations — find_consonance_violations","text":"Find consonance violations","code":""},{"path":"/reference/find_consonance_violations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find consonance violations — find_consonance_violations","text":"","code":"find_consonance_violations(node_dat, hierarchy, rejection_column)"},{"path":"/reference/find_consonance_violations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find consonance violations — find_consonance_violations","text":"node_dat Node data table hierarchy Hierarchy relationships rejection_column Column rejection decisions","code":""},{"path":"/reference/find_consonance_violations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find consonance violations — find_consonance_violations","text":"List violations","code":""},{"path":"/reference/find_containing_intersections.html","id":null,"dir":"Reference","previous_headings":"","what":"Find all intersection results containing a specific node — find_containing_intersections","title":"Find all intersection results containing a specific node — find_containing_intersections","text":"Find intersection results containing specific node","code":""},{"path":"/reference/find_containing_intersections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find all intersection results containing a specific node — find_containing_intersections","text":"","code":"find_containing_intersections(node_id, intersection_results)"},{"path":"/reference/find_containing_intersections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find all intersection results containing a specific node — find_containing_intersections","text":"node_id Node ID find intersection_results intersection test results","code":""},{"path":"/reference/find_containing_intersections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find all intersection results containing a specific node — find_containing_intersections","text":"List intersection results containing node","code":""},{"path":"/reference/find_leaf_descendants.html","id":null,"dir":"Reference","previous_headings":"","what":"Find leaf descendants of a node — find_leaf_descendants","title":"Find leaf descendants of a node — find_leaf_descendants","text":"Find leaf descendants node","code":""},{"path":"/reference/find_leaf_descendants.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find leaf descendants of a node — find_leaf_descendants","text":"","code":"find_leaf_descendants(node_id, leaf_nodes, tracker)"},{"path":"/reference/find_leaf_descendants.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find leaf descendants of a node — find_leaf_descendants","text":"node_id Node ID leaf_nodes Vector leaf node IDs tracker Node tracker object","code":""},{"path":"/reference/find_leaf_descendants.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find leaf descendants of a node — find_leaf_descendants","text":"Vector leaf node IDs descendants","code":""},{"path":"/reference/find_leaf_nodes.html","id":null,"dir":"Reference","previous_headings":"","what":"Find leaf nodes in the hierarchy — find_leaf_nodes","title":"Find leaf nodes in the hierarchy — find_leaf_nodes","text":"Find leaf nodes hierarchy","code":""},{"path":"/reference/find_leaf_nodes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find leaf nodes in the hierarchy — find_leaf_nodes","text":"","code":"find_leaf_nodes(node_dat, tracker)"},{"path":"/reference/find_leaf_nodes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find leaf nodes in the hierarchy — find_leaf_nodes","text":"node_dat Node data table tracker Node tracker object","code":""},{"path":"/reference/find_leaf_nodes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find leaf nodes in the hierarchy — find_leaf_nodes","text":"Vector leaf node IDs","code":""},{"path":"/reference/fixed_evalue.html","id":null,"dir":"Reference","previous_headings":"","what":"Fixed betting strategy for e-values — fixed_evalue","title":"Fixed betting strategy for e-values — fixed_evalue","text":"Fixed betting strategy e-values","code":""},{"path":"/reference/fixed_evalue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fixed betting strategy for e-values — fixed_evalue","text":"","code":"fixed_evalue(t_stat, n1, n0)"},{"path":"/reference/fixed_evalue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fixed betting strategy for e-values — fixed_evalue","text":"t_stat T-statistic n1 Treatment group size n0 Control group size","code":""},{"path":"/reference/fixed_evalue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fixed betting strategy for e-values — fixed_evalue","text":"E-value using fixed betting","code":""},{"path":"/reference/generate_all_intersections.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate all possible intersection hypotheses — generate_all_intersections","title":"Generate all possible intersection hypotheses — generate_all_intersections","text":"Generate possible intersection hypotheses","code":""},{"path":"/reference/generate_all_intersections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate all possible intersection hypotheses — generate_all_intersections","text":"","code":"generate_all_intersections(leaf_nodes, tracker)"},{"path":"/reference/generate_all_intersections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate all possible intersection hypotheses — generate_all_intersections","text":"leaf_nodes Vector leaf node IDs tracker Node tracker object","code":""},{"path":"/reference/generate_all_intersections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate all possible intersection hypotheses — generate_all_intersections","text":"List intersection hypotheses (sets nodes)","code":""},{"path":"/reference/generate_corrective_actions.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate corrective actions for consonance violations — generate_corrective_actions","title":"Generate corrective actions for consonance violations — generate_corrective_actions","text":"Generate corrective actions consonance violations","code":""},{"path":"/reference/generate_corrective_actions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate corrective actions for consonance violations — generate_corrective_actions","text":"","code":"generate_corrective_actions(violations, node_dat, hierarchy)"},{"path":"/reference/generate_corrective_actions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate corrective actions for consonance violations — generate_corrective_actions","text":"violations List violations node_dat Node data table hierarchy Hierarchy structure","code":""},{"path":"/reference/generate_corrective_actions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate corrective actions for consonance violations — generate_corrective_actions","text":"Recommended corrective actions","code":""},{"path":"/reference/generate_meinshausen_hierarchy.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Meinshausen hierarchical clustering for variables — generate_meinshausen_hierarchy","title":"Generate Meinshausen hierarchical clustering for variables — generate_meinshausen_hierarchy","text":"Creates hierarchical clustering structure suitable Meinshausen testing applied variable selection problems. typically used goal test groups correlated variables hierarchically.","code":""},{"path":"/reference/generate_meinshausen_hierarchy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Meinshausen hierarchical clustering for variables — generate_meinshausen_hierarchy","text":"","code":"generate_meinshausen_hierarchy(   correlation_matrix,   method = \"complete\",   min_cluster_size = 2 )"},{"path":"/reference/generate_meinshausen_hierarchy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Meinshausen hierarchical clustering for variables — generate_meinshausen_hierarchy","text":"correlation_matrix Correlation matrix variables method Clustering method (\"complete\", \"average\", \"single\") min_cluster_size Minimum size clusters tested","code":""},{"path":"/reference/generate_meinshausen_hierarchy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Meinshausen hierarchical clustering for variables — generate_meinshausen_hierarchy","text":"List clustering structure compatible node_tracker format","code":""},{"path":"/reference/get_children_nodes.html","id":null,"dir":"Reference","previous_headings":"","what":"Get children nodes from tracker — get_children_nodes","title":"Get children nodes from tracker — get_children_nodes","text":"Get children nodes tracker","code":""},{"path":"/reference/get_children_nodes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get children nodes from tracker — get_children_nodes","text":"","code":"get_children_nodes(parent_id, node_tracker)"},{"path":"/reference/get_children_nodes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get children nodes from tracker — get_children_nodes","text":"parent_id Parent node ID node_tracker Node tracking structure","code":""},{"path":"/reference/get_children_nodes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get children nodes from tracker — get_children_nodes","text":"Vector child node IDs","code":""},{"path":"/reference/get_descendant_nodes.html","id":null,"dir":"Reference","previous_headings":"","what":"Get all descendant nodes for a given node — get_descendant_nodes","title":"Get all descendant nodes for a given node — get_descendant_nodes","text":"Get descendant nodes given node","code":""},{"path":"/reference/get_descendant_nodes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get all descendant nodes for a given node — get_descendant_nodes","text":"","code":"get_descendant_nodes(node_id, node_tracker, node_dat)"},{"path":"/reference/get_descendant_nodes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get all descendant nodes for a given node — get_descendant_nodes","text":"node_id Node ID node_tracker Node tracking structure node_dat Node data","code":""},{"path":"/reference/get_descendant_nodes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get all descendant nodes for a given node — get_descendant_nodes","text":"Data table descendant nodes","code":""},{"path":"/reference/get_parent_from_tracker.html","id":null,"dir":"Reference","previous_headings":"","what":"Get parent ID from tracker — get_parent_from_tracker","title":"Get parent ID from tracker — get_parent_from_tracker","text":"Get parent ID tracker","code":""},{"path":"/reference/get_parent_from_tracker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get parent ID from tracker — get_parent_from_tracker","text":"","code":"get_parent_from_tracker(tracker, node_ids)"},{"path":"/reference/get_parent_from_tracker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get parent ID from tracker — get_parent_from_tracker","text":"tracker Node tracking object node_ids Vector node IDs find parents ","code":""},{"path":"/reference/get_parent_from_tracker.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get parent ID from tracker — get_parent_from_tracker","text":"Vector parent node IDs (0 root)","code":""},{"path":"/reference/hodges_lehmann_sensitivity_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Hodges-Lehmann estimator sensitivity bounds — hodges_lehmann_sensitivity_bounds","title":"Hodges-Lehmann estimator sensitivity bounds — hodges_lehmann_sensitivity_bounds","text":"Hodges-Lehmann estimator sensitivity bounds","code":""},{"path":"/reference/hodges_lehmann_sensitivity_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hodges-Lehmann estimator sensitivity bounds — hodges_lehmann_sensitivity_bounds","text":"","code":"hodges_lehmann_sensitivity_bounds(y1, y0, gamma)"},{"path":"/reference/hodges_lehmann_sensitivity_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hodges-Lehmann estimator sensitivity bounds — hodges_lehmann_sensitivity_bounds","text":"y1 Treatment group outcomes y0 Control group outcomes gamma Sensitivity parameter","code":""},{"path":"/reference/hodges_lehmann_sensitivity_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hodges-Lehmann estimator sensitivity bounds — hodges_lehmann_sensitivity_bounds","text":"Upper lower p-value bounds","code":""},{"path":"/reference/idt.html","id":null,"dir":"Reference","previous_headings":"","what":"Example tree-testing data — idt","title":"Example tree-testing data — idt","text":"Synthetic data used examples tests manytestsr.","code":""},{"path":"/reference/idt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example tree-testing data — idt","text":"","code":"idt  bdt1"},{"path":"/reference/idt.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example tree-testing data — idt","text":"idt Individual-level data.table outcomes assignments. bdt1 Block-level data.table containing summaries per block. object class data.table (inherits data.frame) 64 rows 9 columns.","code":""},{"path":"/reference/idt.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example tree-testing data — idt","text":"See data-raw/make_k_ary_tree_data.R; produced TreeTestSim (≥ X.Y.Z) fixed RNG seed 2025-11-08.","code":""},{"path":"/reference/idt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example tree-testing data — idt","text":"Generated stored package data avoid slow/regenerative simulation examples/tests. full recipe lives data-raw/make_k_ary_tree_data.R.","code":""},{"path":"/reference/idt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Example tree-testing data — idt","text":"Bowers et al., TreeTestSim, manytestsr.","code":""},{"path":"/reference/idt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example tree-testing data — idt","text":"","code":"str(idt) #> Classes ‘data.table’ and 'data.frame':\t3200 obs. of  13 variables: #>  $ bF          : Factor w/ 64 levels \"22\",\"23\",\"24\",..: 1 1 1 1 1 1 1 1 1 1 ... #>  $ b           : int  22 22 22 22 22 22 22 22 22 22 ... #>  $ id          : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ bary0       : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ y0          : num  -0.966 2.067 -2.617 -0.452 1.039 ... #>  $ nonnull     : logi  TRUE TRUE TRUE TRUE TRUE TRUE ... #>  $ lvls_fac    : Factor w/ 64 levels \"2.6.22\",\"2.6.23\",..: 1 1 1 1 1 1 1 1 1 1 ... #>  $ y1          : num  -0.966 2.067 -2.617 -0.452 1.039 ... #>  $ trt         : num  1 0 0 0 1 0 1 1 0 1 ... #>  $ Y           : num  -0.966 2.067 -2.617 -0.452 1.039 ... #>  $ trtF        : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 2 2 1 2 ... #>  $ y1_half_tau1: num  0.492 3.308 -1.329 0.46 1.963 ... #>  $ Y_half_tau1 : num  0.492 2.067 -2.617 -0.452 1.963 ... #>  - attr(*, \".internal.selfref\")=<externalptr>  #>  - attr(*, \"sorted\")= chr \"bF\" #>  - attr(*, \"index\")= int(0)  #>   ..- attr(*, \"__nonnull\")= int [1:3200] 201 202 203 204 205 206 207 208 209 210 ... str(bdt1) #> Classes ‘data.table’ and 'data.frame':\t64 obs. of  9 variables: #>  $ node    : int  22 23 24 25 26 27 28 29 30 31 ... #>  $ level   : int  3 3 3 3 3 3 3 3 3 3 ... #>  $ parent  : num  6 6 6 6 7 7 7 7 8 8 ... #>  $ nonnull : logi  TRUE TRUE TRUE TRUE FALSE FALSE ... #>  $ leaf    : num  1 1 1 1 1 1 1 1 1 1 ... #>  $ lvls_fac: Factor w/ 64 levels \"2.6.22\",\"2.6.23\",..: 1 2 3 4 5 6 7 8 9 10 ... #>  $ nb      : num  50 50 50 50 50 50 50 50 50 50 ... #>  $ bary0   : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ bF      : Factor w/ 64 levels \"22\",\"23\",\"24\",..: 1 2 3 4 5 6 7 8 9 10 ... #>  - attr(*, \".internal.selfref\")=<externalptr>"},{"path":"/reference/integrate_meinshausen_find_blocks.html","id":null,"dir":"Reference","previous_headings":"","what":"Integration function for find_blocks with Meinshausen testing — integrate_meinshausen_find_blocks","title":"Integration function for find_blocks with Meinshausen testing — integrate_meinshausen_find_blocks","text":"Integrates Meinshausen hierarchical testing find_blocks workflow","code":""},{"path":"/reference/integrate_meinshausen_find_blocks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Integration function for find_blocks with Meinshausen testing — integrate_meinshausen_find_blocks","text":"","code":"integrate_meinshausen_find_blocks(   node_dat,   node_tracker,   alpha = 0.05,   method = \"simes\",   use_sequential = TRUE )"},{"path":"/reference/integrate_meinshausen_find_blocks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Integration function for find_blocks with Meinshausen testing — integrate_meinshausen_find_blocks","text":"node_dat Node-level data find_blocks node_tracker Node tracking structure alpha Error rate method P-value combination method use_sequential Use sequential rejection principle","code":""},{"path":"/reference/integrate_meinshausen_find_blocks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Integration function for find_blocks with Meinshausen testing — integrate_meinshausen_find_blocks","text":"Enhanced node_dat Meinshausen results","code":""},{"path":"/reference/intersection_union_tests.html","id":null,"dir":"Reference","previous_headings":"","what":"Intersection-Union Tests for Hierarchical Hypotheses — intersection_union_tests","title":"Intersection-Union Tests for Hierarchical Hypotheses — intersection_union_tests","text":"Implementation proper intersection-union testing hierarchical structures. ensures composite null hypotheses tested appropriately hierarchical structure respected hypothesis testing. Particularly useful validating logical structure find_blocks() results.","code":""},{"path":"/reference/intersection_union_tests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Intersection-Union Tests for Hierarchical Hypotheses — intersection_union_tests","text":"","code":"intersection_union_tests(   node_dat,   tracker,   alpha = 0.05,   union_method = \"max\",   intersection_method = \"simes\" )"},{"path":"/reference/intersection_union_tests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Intersection-Union Tests for Hierarchical Hypotheses — intersection_union_tests","text":"node_dat Node data p-values hierarchy tracker Node tracker tree structure alpha Type error rate union_method Method testing union alternatives (\"max\", \"simes\", \"fisher\") intersection_method Method testing intersection nulls (\"min\", \"simes\", \"fisher\")","code":""},{"path":"/reference/intersection_union_tests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Intersection-Union Tests for Hierarchical Hypotheses — intersection_union_tests","text":"Updated node data intersection-union test results","code":""},{"path":"/reference/intersection_union_tests.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Intersection-Union Tests for Hierarchical Hypotheses — intersection_union_tests","text":"Berger, R. L. (1982). Multiparameter hypothesis testing acceptance sampling. Technometrics, 24(4), 295-300.","code":""},{"path":"/reference/intersection_union_tests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Intersection-Union Tests for Hierarchical Hypotheses — intersection_union_tests","text":"","code":"if (FALSE) { # \\dontrun{ # Apply intersection-union tests to find_blocks results # Requires dplyr package data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Prepare data idat <- as.data.table(example_dat) bdat <- idat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(idat)) * (pb * (1 - pb)),     .groups = \"drop\"   ) %>%   as.data.table()  # Run find_blocks to get hierarchical structure results <- find_blocks(   idat = idat,   bdat = bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pIndepDist,   fmla = Y1 ~ trtF | blockF,   splitby = \"hwt\",   parallel = \"no\",   maxtest = 15,   trace = FALSE )  # Apply intersection-union tests to validate hypothesis structure iu_results <- intersection_union_tests(   node_dat = results$node_dat,   tracker = results$node_tracker, # Assuming tracker is available   alpha = 0.05,   union_method = \"simes\",   intersection_method = \"simes\" )  # Examine intersection-union test results if (\"iu_p_intersection\" %in% names(iu_results$node_dat)) {   iu_summary <- iu_results$node_dat[, .(     nodenum,     original_p = p,     intersection_p = iu_p_intersection,     union_p = iu_p_union,     reject_intersection = iu_reject_intersection,     reject_union = iu_reject_union   )]    cat(\"Intersection-Union Test Results:\\n\")   print(head(iu_summary))    # Count rejections by method   cat(\"Intersection null rejections:\", sum(iu_summary$reject_intersection, na.rm = TRUE), \"\\n\")   cat(\"Union alternative rejections:\", sum(iu_summary$reject_union, na.rm = TRUE), \"\\n\") }  # Check consistency of intersection-union results consistency_check <- check_iu_consistency(iu_results) if (consistency_check$is_consistent) {   cat(\"Intersection-union tests are logically consistent.\\n\") } else {   cat(\"Found inconsistencies:\\n\")   print(consistency_check$inconsistencies) }  # Visualize intersection-union results if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   iu_plot <- plot_intersection_union_results(iu_results)   print(iu_plot) } } # }"},{"path":"/reference/kelly_evalue.html","id":null,"dir":"Reference","previous_headings":"","what":"Kelly optimal betting for e-values — kelly_evalue","title":"Kelly optimal betting for e-values — kelly_evalue","text":"Kelly optimal betting e-values","code":""},{"path":"/reference/kelly_evalue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kelly optimal betting for e-values — kelly_evalue","text":"","code":"kelly_evalue(t_stat, n1, n0)"},{"path":"/reference/kelly_evalue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kelly optimal betting for e-values — kelly_evalue","text":"t_stat T-statistic n1 Treatment group size n0 Control group size","code":""},{"path":"/reference/kelly_evalue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kelly optimal betting for e-values — kelly_evalue","text":"E-value using Kelly betting","code":""},{"path":"/reference/local_bh_all_ps.html","id":null,"dir":"Reference","previous_headings":"","what":"BH local adjustment — local_bh_all_ps","title":"BH local adjustment — local_bh_all_ps","text":"Performs Benjamini-Hochberg adjustment vector p-values.","code":""},{"path":"/reference/local_bh_all_ps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BH local adjustment — local_bh_all_ps","text":"","code":"local_bh_all_ps(pvals_children, alpha = 0.05)"},{"path":"/reference/local_bh_all_ps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BH local adjustment — local_bh_all_ps","text":"pvals_children Numeric vector child p-values. alpha Numeric scalar alpha (used)","code":""},{"path":"/reference/local_bh_all_ps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BH local adjustment — local_bh_all_ps","text":"numeric vector BH-adjusted p-values.","code":""},{"path":"/reference/local_bh_all_ps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BH local adjustment — local_bh_all_ps","text":"","code":"local_bh_all_ps(c(0.01, 0.04, 0.10, 0.20)) #> [1] 0.0400000 0.0800000 0.1333333 0.2000000"},{"path":"/reference/local_hommel_all_ps.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute local Hommel p-value for a Vector of Child p-values — local_hommel_all_ps","title":"Compute local Hommel p-value for a Vector of Child p-values — local_hommel_all_ps","text":"Given \\(k\\) child p-values, computes Hommel-adjusted p-values","code":""},{"path":"/reference/local_hommel_all_ps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute local Hommel p-value for a Vector of Child p-values — local_hommel_all_ps","text":"","code":"local_hommel_all_ps(pvals_children, alpha = 0.05)"},{"path":"/reference/local_hommel_all_ps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute local Hommel p-value for a Vector of Child p-values — local_hommel_all_ps","text":"pvals_children Numeric vector child p-values. alpha Numeric scalar alpha (used function)","code":""},{"path":"/reference/local_hommel_all_ps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute local Hommel p-value for a Vector of Child p-values — local_hommel_all_ps","text":"vector adjusted p-values","code":""},{"path":"/reference/local_hommel_all_ps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute local Hommel p-value for a Vector of Child p-values — local_hommel_all_ps","text":"","code":"local_hommel_all_ps(c(0.01, 0.04, 0.10, 0.20)) #> [1] 0.04 0.12 0.20 0.20"},{"path":"/reference/local_min_p.html","id":null,"dir":"Reference","previous_headings":"","what":"Unadjusted local minimal p-value — local_min_p","title":"Unadjusted local minimal p-value — local_min_p","text":"Given \\(k\\) child p-values, returns highest p-value alpha; none alpha returns smallest p-value.","code":""},{"path":"/reference/local_min_p.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unadjusted local minimal p-value — local_min_p","text":"","code":"local_min_p(pvals_children, alpha = 0.05)"},{"path":"/reference/local_min_p.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unadjusted local minimal p-value — local_min_p","text":"pvals_children Numeric vector child p-values. alpha Numeric scalar alpha","code":""},{"path":"/reference/local_min_p.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unadjusted local minimal p-value — local_min_p","text":"single numeric value.","code":""},{"path":"/reference/local_min_p.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unadjusted local minimal p-value — local_min_p","text":"","code":"local_min_p(c(0.01, 0.04, 0.10, 0.20)) #> [1] 0.04 local_min_p(c(0.10, 0.20)) #> [1] 0.1"},{"path":"/reference/local_simes.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute local Simes p-value for a Vector of Child p-values — local_simes","title":"Compute local Simes p-value for a Vector of Child p-values — local_simes","text":"Given \\(k\\) child p-values, computes Simes p-value \\(\\min_{=1\\ldots k} \\{ (k/) * p_{()} \\}\\), \\(p_{(1)} \\le \\ldots \\le p_{(k)}\\).","code":""},{"path":"/reference/local_simes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute local Simes p-value for a Vector of Child p-values — local_simes","text":"","code":"local_simes(pvals_children, alpha = 0.05)"},{"path":"/reference/local_simes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute local Simes p-value for a Vector of Child p-values — local_simes","text":"pvals_children Numeric vector child p-values. alpha Numeric scalar alpha (used function)","code":""},{"path":"/reference/local_simes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute local Simes p-value for a Vector of Child p-values — local_simes","text":"single numeric value: Simes combination p-value.","code":""},{"path":"/reference/local_simes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute local Simes p-value for a Vector of Child p-values — local_simes","text":"","code":"local_simes(c(0.01, 0.04, 0.10, 0.20)) #> [1] 0.04"},{"path":"/reference/local_unadj_all_ps.html","id":null,"dir":"Reference","previous_headings":"","what":"Unadjusted local step (pass-through) — local_unadj_all_ps","title":"Unadjusted local step (pass-through) — local_unadj_all_ps","text":"Returns input p-values unmodified.","code":""},{"path":"/reference/local_unadj_all_ps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unadjusted local step (pass-through) — local_unadj_all_ps","text":"","code":"local_unadj_all_ps(pvals_children, alpha = 0.05)"},{"path":"/reference/local_unadj_all_ps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unadjusted local step (pass-through) — local_unadj_all_ps","text":"pvals_children Numeric vector child p-values. alpha Numeric scalar alpha (used)","code":""},{"path":"/reference/local_unadj_all_ps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unadjusted local step (pass-through) — local_unadj_all_ps","text":"numeric vector: input.","code":""},{"path":"/reference/local_unadj_all_ps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unadjusted local step (pass-through) — local_unadj_all_ps","text":"","code":"local_unadj_all_ps(c(0.01, 0.04)) #> [1] 0.01 0.04"},{"path":"/reference/make_results_ggraph.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a plot of the nodes — make_results_ggraph","title":"Make a plot of the nodes — make_results_ggraph","text":"Given results splitting testing algorithm form graph make_results_tree, make node level data set use reporting results terms binary tree graph. print plot graph. need resulting object.","code":""},{"path":"/reference/make_results_ggraph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a plot of the nodes — make_results_ggraph","text":"","code":"make_results_ggraph(res_graph, remove_na_p = TRUE)"},{"path":"/reference/make_results_ggraph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a plot of the nodes — make_results_ggraph","text":"res_graph tidygraph object produced make_results_tree remove_na_p logical indicating whether graph include nodes/leaves tested. Default (TRUE) remove . remove_na_p FALSE, graph might look strange since blocks known position graph  (graph fixed, specified find_blocks function node block visited testing.)","code":""},{"path":"/reference/make_results_ggraph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a plot of the nodes — make_results_ggraph","text":"ggraph object","code":""},{"path":"/reference/make_results_ggraph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make a plot of the nodes — make_results_ggraph","text":"","code":"if (FALSE) { # \\dontrun{ # Complete workflow example data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Create block-level dataset example_bdat <- example_dat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(example_dat)) * (pb * (1 - pb)),     .groups = \"drop\"   ) %>%   as.data.table()  # Run find_blocks results <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   parallel = \"no\" )  # Create tree structure tree_results <- make_results_tree(results, block_id = \"blockF\")  # Create ggraph visualization library(ggraph) library(ggplot2) graph_plot <- make_results_ggraph(tree_results$graph)  # Display the plot print(graph_plot)  # Customize the visualization graph_plot +   labs(title = \"Hierarchical Testing Results Tree\") +   theme_void() } # }"},{"path":"/reference/make_results_tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a node-level dataset from a block-level dataset — make_results_tree","title":"Make a node-level dataset from a block-level dataset — make_results_tree","text":"Given results splitting testing algorithm, make node level data set use reporting results input ggraph visualization terms tree graph.","code":""},{"path":"/reference/make_results_tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a node-level dataset from a block-level dataset — make_results_tree","text":"","code":"make_results_tree(   orig_res,   block_id,   node_label = NULL,   return_what = \"all\",   truevar_name = NULL,   node_dat = NULL,   node_tracker = NULL )"},{"path":"/reference/make_results_tree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a node-level dataset from a block-level dataset — make_results_tree","text":"orig_res data.table find_blocks(); must include elements node_id    (node identifier, may factorized), p1,p2,… pfinal*, alpha1, alpha2, … block_id name block ID column (e.g. \"bF\") node_label optional name descriptive label column return_what character vector containing \"\", \"graph\" (tbl_graph object nodes edges), \"nodes\" (data.table node level information), \"test_summary\" (data.table object one row indicating false true discoveries, etc.) truevar_name optional name column recording true treatment effect (used find blocks true effect 0 ). simulations column called nonnull TRUE block node non-zero effect FALSE block node truly zero effect. , truevar_name can \"nonnull\" node_dat optional node-level output find_blocks(). full list returned find_blocks() supplied orig_res argument populated automatically. providing block-level data.table must also pass node_dat node identifiers can matched without relying p-values. node_tracker optional tracker returned find_blocks(). required constructing node table accepted compatibility.","code":""},{"path":"/reference/make_results_tree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a node-level dataset from a block-level dataset — make_results_tree","text":"list can contain nodes, tbl_graph object, /test_summary","code":""},{"path":"/reference/make_results_tree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make a node-level dataset from a block-level dataset — make_results_tree","text":"","code":"if (FALSE) { # \\dontrun{ # Complete workflow example data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Create block-level dataset example_bdat <- example_dat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(example_dat)) * (pb * (1 - pb)),     .groups = \"drop\"   ) %>%   as.data.table()  # Run find_blocks results <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   parallel = \"no\" )  # Create tree structure (default returns all components) tree_results <- make_results_tree(results, block_id = \"blockF\")  # Examine the components str(tree_results)  # Look at node-level information head(tree_results$nodes)  # Look at test summary print(tree_results$test_summary)  # Get only the graph component tree_graph <- make_results_tree(results,   block_id = \"blockF\",   return_what = \"graph\" ) print(tree_graph)  # Get only node information tree_nodes <- make_results_tree(results,   block_id = \"blockF\",   return_what = \"nodes\" ) head(tree_nodes) } # }"},{"path":"/reference/meinshausen_hierarchical_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Meinshausen's Hierarchical Testing with Sequential Rejection Principle — meinshausen_hierarchical_test","title":"Meinshausen's Hierarchical Testing with Sequential Rejection Principle — meinshausen_hierarchical_test","text":"Implementation Meinshausen's (2008) hierarchical testing variable importance enhanced sequential rejection principle Goeman Solari (2010). approach provides unified framework hierarchical testing maintains FWER control leveraging hierarchical structure increased power.","code":""},{"path":"/reference/meinshausen_hierarchical_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Meinshausen's Hierarchical Testing with Sequential Rejection Principle — meinshausen_hierarchical_test","text":"","code":"meinshausen_hierarchical_test(   node_dat,   node_tracker,   alpha = 0.05,   method = \"simes\",   use_sequential = TRUE )"},{"path":"/reference/meinshausen_hierarchical_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Meinshausen's Hierarchical Testing with Sequential Rejection Principle — meinshausen_hierarchical_test","text":"node_dat Data.table containing node-level test results columns: nodenum, p, depth, nodesize, optionally parent, testable node_tracker Node tracking object containing hierarchical structure alpha Global Type error rate (default: 0.05) method Method combining p-values (\"simes\", \"fisher\", \"bonferroni\") use_sequential Logical, whether use sequential rejection principle","code":""},{"path":"/reference/meinshausen_hierarchical_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Meinshausen's Hierarchical Testing with Sequential Rejection Principle — meinshausen_hierarchical_test","text":"Updated node_dat Meinshausen hierarchical testing results","code":""},{"path":"/reference/meinshausen_hierarchical_test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Meinshausen's Hierarchical Testing with Sequential Rejection Principle — meinshausen_hierarchical_test","text":"Meinshausen, N. (2008). Hierarchical testing variable importance. Biometrika 95, 265-278. Goeman, J. J., & Solari, . (2010). sequential rejection principle familywise error control. Annals Statistics 38, 3782-3810.","code":""},{"path":"/reference/nodeidfn.html","id":null,"dir":"Reference","previous_headings":"","what":"Use hashing to make a node id (DEPRECATED) — nodeidfn","title":"Use hashing to make a node id (DEPRECATED) — nodeidfn","text":"function deprecated favor numeric node tracking. Kept backward compatibility .","code":""},{"path":"/reference/nodeidfn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use hashing to make a node id (DEPRECATED) — nodeidfn","text":"","code":"nodeidfn(d)"},{"path":"/reference/nodeidfn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use hashing to make a node id (DEPRECATED) — nodeidfn","text":"d vector","code":""},{"path":"/reference/nodeidfn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use hashing to make a node id (DEPRECATED) — nodeidfn","text":"vector hashes","code":""},{"path":"/reference/pCombCauchyDist.html","id":null,"dir":"Reference","previous_headings":"","what":"P-value function: Cauchy Combined Indepence Test — pCombCauchyDist","title":"P-value function: Cauchy Combined Indepence Test — pCombCauchyDist","text":"P-value function: Cauchy Combined Indepence Test","code":""},{"path":"/reference/pCombCauchyDist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"P-value function: Cauchy Combined Indepence Test — pCombCauchyDist","text":"","code":"pCombCauchyDist(   dat,   fmla = YcontNorm ~ trtF | blockF,   simthresh = 20,   sims = 1000,   parallel = \"no\",   ncpu = NULL,   distfn = fast_dists_and_trans_nomax_hybrid )"},{"path":"/reference/pCombCauchyDist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"P-value function: Cauchy Combined Indepence Test — pCombCauchyDist","text":"dat object inheriting class data.frame fmla formula  appropriate function.  something like outcome~treatment|block simthresh size data use direct permutations p-values sims Either NULL (meaning use asymptotic reference dist) number (meaning sampling randomization distribution implied formula) parallel \"\" parallelization required, otherwise \"multicore\" \"snow\" call coin::independence_test() (see help coin::approximate()). Also, parallel \"\" adaptive_dist_function TRUE, openmp version distance creation function called using ncpu threads (parallel::detectCores(logical=FALSE) cores). ncpu number cpus  used parallel operation. distfn  function produces one vectors (data frame matrix)  number  rows dat.  notice users leave mean_dist_raw_rank_tanh since function purpose built function.","code":""},{"path":"/reference/pCombCauchyDist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"P-value function: Cauchy Combined Indepence Test — pCombCauchyDist","text":"p-value","code":""},{"path":"/reference/pCombCauchyDist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"P-value function: Cauchy Combined Indepence Test — pCombCauchyDist","text":" function combines p-values univariate tests using using (1)  raw outcome, (2) rank-transformed outcome, (3) tanh transformed raw outcome (another test statistic sensitive skew), (4) mean difference  raw outcome Euclidean distances bwtween treated control observations within block;  (5) mean difference ranked outcome Euclidean distances bwtween treated control observations; (6) Hotelling-T style quadratic combination preceding test statistics. Inspired Rizzo Székely's work Euclidean distance based testing Liu Xie (2020) Cauchy Combination Test Hansen Bowers (2008) omnibus tests. Distance ranks transformations calculated block block supplied formula.","code":""},{"path":"/reference/pIndepDist.html","id":null,"dir":"Reference","previous_headings":"","what":"P-value function: Independence Treatment Distance Test — pIndepDist","title":"P-value function: Independence Treatment Distance Test — pIndepDist","text":"functions accept data frame perhaps test specific arguments (like whether test asymptotic simulation based). produces p-value.","code":""},{"path":"/reference/pIndepDist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"P-value function: Independence Treatment Distance Test — pIndepDist","text":"","code":"pIndepDist(   dat,   fmla = YcontNorm ~ trtF | blockF,   simthresh = 20,   sims = 1000,   parallel = \"yes\",   ncpu = NULL,   distfn = fast_dists_and_trans_hybrid )"},{"path":"/reference/pIndepDist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"P-value function: Independence Treatment Distance Test — pIndepDist","text":"dat object inheriting class data.frame fmla formula  appropriate function.  something like outcome~treatment|block simthresh size data use direct permutations p-values sims Either NULL (meaning use asymptotic reference dist) number (meaning sampling randomization distribution implied formula) parallel \"\" parallelization required, otherwise \"multicore\" \"snow\" call coin::independence_test() (see help coin::approximate()). Also, parallel \"\" adaptive_dist_function TRUE, openmp version distance creation function called using ncpu threads (parallel::detectCores(logical=FALSE) cores). ncpu number cpus  used parallel operation. distfn  function produces one vectors (data frame matrix)  number  rows dat","code":""},{"path":"/reference/pIndepDist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"P-value function: Independence Treatment Distance Test — pIndepDist","text":"p-value","code":""},{"path":"/reference/pIndepDist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"P-value function: Independence Treatment Distance Test — pIndepDist","text":"now,  function  omnibus-style chi-square  test using (1) ratio   distances controls distances treated observations within block;  (2)   rank  distances  controls unit; (3) raw outcome. Although distances calculated block, profiling suggests better parallelize distance creation distfn (done C++ fastfns.cpp file) rather use data.table approach setDTthreads(). , assume threads data.table 1.","code":""},{"path":"/reference/pIndepDist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"P-value function: Independence Treatment Distance Test — pIndepDist","text":"","code":"# \\donttest{ # Example using distance-based independence test data(example_dat, package = \"manytestsr\") library(data.table)  # Test for treatment effect using distance-based approach single_block <- as.data.table(subset(example_dat, blockF == \"B080\")) p_val <- pIndepDist(single_block, Y1 ~ trtF | blockF, parallel = \"no\") print(p_val) #> [1] 0.5394499  # Test with different outcome variable p_val2 <- pIndepDist(single_block, Y2 ~ trtF | blockF, parallel = \"no\") print(p_val2) #> [1] 0.7764711 # }"},{"path":"/reference/pOneway.html","id":null,"dir":"Reference","previous_headings":"","what":"P-value function: T-test — pOneway","title":"P-value function: T-test — pOneway","text":"functions accept data frame perhaps test specific arguments (like whether test asymptotic simulation based). produces p-value.","code":""},{"path":"/reference/pOneway.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"P-value function: T-test — pOneway","text":"","code":"pOneway(   dat,   fmla = YContNorm ~ trtF | blockF,   simthresh = 20,   sims = 1000,   parallel = \"no\",   ncpu = NULL )"},{"path":"/reference/pOneway.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"P-value function: T-test — pOneway","text":"dat object inheriting class data.frame fmla outcome~treatment factor | block factor (following coin API). simthresh number total observations p-value functions use permutations rather asymptotic approximations sims Either NULL (meaning use asymptotic reference dist) number (meaning sampling randomization distribution implied formula) parallel function use multicore processing permutation based testing. Default . \"snow\" \"multicore\" following approximate coin package. ncpu number workers (\"snow\") cores (\"multicore\").","code":""},{"path":"/reference/pOneway.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"P-value function: T-test — pOneway","text":"p-value","code":""},{"path":"/reference/pOneway.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"P-value function: T-test — pOneway","text":"","code":"# Example using built-in data data(example_dat, package = \"manytestsr\")  # Test for treatment effect on Y1 within a single block single_block <- subset(example_dat, blockF == \"B080\") p_val <- pOneway(single_block, Y1 ~ trtF | blockF, parallel = \"no\") print(p_val) #> [1] 0.2576562  # Test with permutation-based inference for small samples p_val_perm <- pOneway(single_block, Y1 ~ trtF | blockF,   simthresh = 100, sims = 500, parallel = \"no\" ) print(p_val_perm) #> [1] 0.2576562"},{"path":"/reference/pTestTwice.html","id":null,"dir":"Reference","previous_headings":"","what":"P-value function: Testing twice — pTestTwice","title":"P-value function: Testing twice — pTestTwice","text":"functions accept data frame perhaps test specific arguments (like whether test asymptotic simulation based). produces p-value.","code":""},{"path":"/reference/pTestTwice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"P-value function: Testing twice — pTestTwice","text":"","code":"pTestTwice(   dat,   fmla = YcontNorm ~ trtF | blockF,   simthresh = 20,   sims = 1000,   parallel = \"yes\",   ncpu = NULL,   groups = NULL )"},{"path":"/reference/pTestTwice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"P-value function: Testing twice — pTestTwice","text":"dat object inheriting class data.frame fmla formula  appropriate function.  something like outcome~treatment|block simthresh size data use direct permutations p-values sims Either NULL (meaning use asymptotic reference dist) number (meaning sampling randomization distribution implied formula) parallel \"\" parallelization required, otherwise \"multicore\" \"snow\" call coin::independence_test() (see help coin::approximate()). Also, parallel \"\" adaptive_dist_function TRUE, openmp version distance creation function called using ncpu threads (parallel::detectCores(logical=FALSE) cores). ncpu number cpus  used parallel operation. groups Currently unused parameter, reserved future functionality","code":""},{"path":"/reference/pTestTwice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"P-value function: Testing twice — pTestTwice","text":"p-value","code":""},{"path":"/reference/pTestTwice.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"P-value function: Testing twice — pTestTwice","text":"now,  function  omnibus-style max-T test using (1) raw outcome (2) rank transformed raw outcome. Inspired Rosenbaum (2008) Testing Twicee","code":""},{"path":"/reference/pWilcox.html","id":null,"dir":"Reference","previous_headings":"","what":"P-value function: Wilcox Test — pWilcox","title":"P-value function: Wilcox Test — pWilcox","text":"functions accept data frame perhaps test specific arguments (like whether test asympotic simulation based). produces p-value.","code":""},{"path":"/reference/pWilcox.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"P-value function: Wilcox Test — pWilcox","text":"","code":"pWilcox(   dat,   fmla = YContNorm ~ trtF | blockF,   simthresh = 20,   sims = 1000,   parallel = \"no\",   ncpu = NULL )"},{"path":"/reference/pWilcox.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"P-value function: Wilcox Test — pWilcox","text":"dat object inheriting class data.frame fmla outcome~treatment factor | block factor (following coin API). simthresh number total observations p-value functions use permutations rather asymptotic approximations sims Either NULL (meaning use asymptotic reference dist) number (meaning sampling randomization distribution implied formula) parallel function use multicore processing permutation based testing. Default . \"snow\" \"multicore\" following approximate coin package. ncpu number workers (\"snow\") cores (\"multicore\").","code":""},{"path":"/reference/pWilcox.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"P-value function: Wilcox Test — pWilcox","text":"p-value","code":""},{"path":"/reference/pWilcox.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"P-value function: Wilcox Test — pWilcox","text":"","code":"# Example using Wilcoxon rank-sum test data(example_dat, package = \"manytestsr\")  # Test for treatment effect on Y1 within a single block single_block <- subset(example_dat, blockF == \"B080\") p_val <- pWilcox(single_block, Y1 ~ trtF | blockF, parallel = \"no\") print(p_val) #> [1] 0.2547046  # Compare with permutation-based version p_val_perm <- pWilcox(single_block, Y1 ~ trtF | blockF,   simthresh = 100, sims = 500, parallel = \"no\" ) print(p_val_perm) #> [1] 0.2547046"},{"path":"/reference/parse_formula.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse formula to extract components — parse_formula","title":"Parse formula to extract components — parse_formula","text":"Parse formula extract components","code":""},{"path":"/reference/parse_formula.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse formula to extract components — parse_formula","text":"","code":"parse_formula(formula)"},{"path":"/reference/parse_formula.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse formula to extract components — parse_formula","text":"formula Test formula","code":""},{"path":"/reference/parse_formula.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse formula to extract components — parse_formula","text":"List outcome treatment components","code":""},{"path":"/reference/parse_formula_components.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse formula components — parse_formula_components","title":"Parse formula components — parse_formula_components","text":"Parse formula components","code":""},{"path":"/reference/parse_formula_components.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse formula components — parse_formula_components","text":"","code":"parse_formula_components(formula)"},{"path":"/reference/parse_formula_components.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse formula components — parse_formula_components","text":"formula Formula object","code":""},{"path":"/reference/parse_formula_components.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse formula components — parse_formula_components","text":"List parsed components","code":""},{"path":"/reference/plot_design_sensitivity.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot sensitivity analysis results — plot_design_sensitivity","title":"Plot sensitivity analysis results — plot_design_sensitivity","text":"Plot sensitivity analysis results","code":""},{"path":"/reference/plot_design_sensitivity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot sensitivity analysis results — plot_design_sensitivity","text":"","code":"plot_design_sensitivity(sensitivity_analysis, blocks = NULL)"},{"path":"/reference/plot_design_sensitivity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot sensitivity analysis results — plot_design_sensitivity","text":"sensitivity_analysis Results design_sensitivity_analysis blocks Vector block IDs plot (NULL )","code":""},{"path":"/reference/plot_design_sensitivity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot sensitivity analysis results — plot_design_sensitivity","text":"ggplot object","code":""},{"path":"/reference/plot_intersection_union_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize intersection-union test results — plot_intersection_union_results","title":"Visualize intersection-union test results — plot_intersection_union_results","text":"Visualize intersection-union test results","code":""},{"path":"/reference/plot_intersection_union_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize intersection-union test results — plot_intersection_union_results","text":"","code":"plot_intersection_union_results(iu_results, highlight_inconsistencies = TRUE)"},{"path":"/reference/plot_intersection_union_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize intersection-union test results — plot_intersection_union_results","text":"iu_results Results intersection_union_tests highlight_inconsistencies Logical, whether highlight inconsistencies","code":""},{"path":"/reference/plot_intersection_union_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize intersection-union test results — plot_intersection_union_results","text":"ggplot object","code":""},{"path":"/reference/report_detections.html","id":null,"dir":"Reference","previous_headings":"","what":"Return detected blocks plus info — report_detections","title":"Return detected blocks plus info — report_detections","text":"Given results splitting testing algorithm, report blocks null effects rejected level alpha. Currently calculates rejections using FWER style criteria (p node = max previous nodes) final alphas scalar alpha fwer=TRUE.","code":""},{"path":"/reference/report_detections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return detected blocks plus info — report_detections","text":"","code":"report_detections(   orig_res,   fwer = TRUE,   alpha = 0.05,   only_hits = FALSE,   blockid = \"blockF\" )"},{"path":"/reference/report_detections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return detected blocks plus info — report_detections","text":"orig_res results data.table output find_blocks function. fwer (default TRUE) means block detected () using maximum p-value associated block (groups containing block). fwer=FALSE detect blocks (groups blocks) using FDR control. alpha false positive rate used detecting effect constant (.e. FDR-style approach). only_hits (default FALSE) returns detected blocks instead blockid Name block variable (blocking variable factor)","code":""},{"path":"/reference/report_detections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return detected blocks plus info — report_detections","text":"data.table adding column hit res data.table indicating \"hit\" detection block (group blocks)","code":""},{"path":"/reference/report_detections.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return detected blocks plus info — report_detections","text":"","code":"if (FALSE) { # \\dontrun{ # Use example data and run find_blocks data(example_dat, package = \"manytestsr\") library(data.table) library(dplyr)  # Create block-level dataset example_bdat <- example_dat %>%   group_by(blockF) %>%   summarize(     nb = n(),     pb = mean(trt),     hwt = (nb / nrow(example_dat)) * (pb * (1 - pb)),     .groups = \"drop\"   ) %>%   as.data.table()  # Run find_blocks results <- find_blocks(   idat = example_dat,   bdat = example_bdat,   blockid = \"blockF\",   splitfn = splitCluster,   pfn = pOneway,   fmla = Y1 ~ trtF | blockF,   parallel = \"no\" )  # Report detections using FWER control detections_fwer <- report_detections(results$bdat, fwer = TRUE, alpha = 0.05) head(detections_fwer[, .(blockF, hit, pfinalb)])  # Report only significant blocks hits_only <- report_detections(results$bdat, fwer = TRUE, only_hits = TRUE) print(hits_only) } # }"},{"path":"/reference/sign_test_sensitivity_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Sign test sensitivity bounds — sign_test_sensitivity_bounds","title":"Sign test sensitivity bounds — sign_test_sensitivity_bounds","text":"Sign test sensitivity bounds","code":""},{"path":"/reference/sign_test_sensitivity_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sign test sensitivity bounds — sign_test_sensitivity_bounds","text":"","code":"sign_test_sensitivity_bounds(y1, y0, gamma)"},{"path":"/reference/sign_test_sensitivity_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sign test sensitivity bounds — sign_test_sensitivity_bounds","text":"y1 Treatment group outcomes y0 Control group outcomes gamma Sensitivity parameter","code":""},{"path":"/reference/sign_test_sensitivity_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sign test sensitivity bounds — sign_test_sensitivity_bounds","text":"Upper lower p-value bounds","code":""},{"path":"/reference/splitCluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Splitting function: K-Means Clustering — splitCluster","title":"Splitting function: K-Means Clustering — splitCluster","text":"splitting function takes block ids block ordering vector (vectors) produces factor assigns block ids one group another group.","code":""},{"path":"/reference/splitCluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Splitting function: K-Means Clustering — splitCluster","text":"","code":"splitCluster(bid, x)"},{"path":"/reference/splitCluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Splitting function: K-Means Clustering — splitCluster","text":"bid Block id x vector can use order blocks","code":""},{"path":"/reference/splitCluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Splitting function: K-Means Clustering — splitCluster","text":"factor categorizing blocks groups.","code":""},{"path":"/reference/splitCluster.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Splitting function: K-Means Clustering — splitCluster","text":"","code":"# Simple example with block weights block_ids <- c(\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\") block_weights <- c(0.1, 0.8, 0.2, 0.9, 0.3, 0.7)  # Split blocks into clusters based on weights groups <- splitCluster(block_ids, block_weights) print(groups) #> [1] 1 0 1 0 1 0 #> Levels: 0 1  # View which blocks are in each group data.frame(block_id = block_ids, weight = block_weights, group = groups) #>   block_id weight group #> 1       B1    0.1     1 #> 2       B2    0.8     0 #> 3       B3    0.2     1 #> 4       B4    0.9     0 #> 5       B5    0.3     1 #> 6       B6    0.7     0"},{"path":"/reference/splitEqualApprox.html","id":null,"dir":"Reference","previous_headings":"","what":"Splitting function: Approx Equal Splits — splitEqualApprox","title":"Splitting function: Approx Equal Splits — splitEqualApprox","text":"splitting function takes block ids block ordering vector (vectors) produces factor assigns block ids one group another group sum x within one group approximately equal sum x group. x something like block-size equalized total observations split. x covariate, splitting approach may make less conceptual sense — x <- c(1,2,3,4) put 1 3 one group 2 4 another group.","code":""},{"path":"/reference/splitEqualApprox.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Splitting function: Approx Equal Splits — splitEqualApprox","text":"","code":"splitEqualApprox(bid, x)"},{"path":"/reference/splitEqualApprox.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Splitting function: Approx Equal Splits — splitEqualApprox","text":"bid Block id x vector can use order blocks","code":""},{"path":"/reference/splitEqualApprox.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Splitting function: Approx Equal Splits — splitEqualApprox","text":"factor categorizing blocks groups.","code":""},{"path":"/reference/splitEqualApprox.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Splitting function: Approx Equal Splits — splitEqualApprox","text":"","code":"# Example with block sizes - equalizes total size across groups block_ids <- c(\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\") block_sizes <- c(10, 30, 15, 35, 20, 25) # Total = 135  groups <- splitEqualApprox(block_ids, block_sizes)  # Check the split - should have approximately equal sums tapply(block_sizes, groups, sum) #>  0  1  #> 60 75"},{"path":"/reference/splitLOO.html","id":null,"dir":"Reference","previous_headings":"","what":"Splitting function: Leave One Out — splitLOO","title":"Splitting function: Leave One Out — splitLOO","text":"splitting function takes block ids block ordering vector (vectors) produces factor assigns block ids one group another group.","code":""},{"path":"/reference/splitLOO.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Splitting function: Leave One Out — splitLOO","text":"","code":"splitLOO(bid, x)"},{"path":"/reference/splitLOO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Splitting function: Leave One Out — splitLOO","text":"bid Block id x vector can use order blocks. block-level value. Like N harmonic mean weight block.","code":""},{"path":"/reference/splitLOO.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Splitting function: Leave One Out — splitLOO","text":"factor categorizing blocks groups.","code":""},{"path":"/reference/splitLOO.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Splitting function: Leave One Out — splitLOO","text":"","code":"# Leave-one-out splitting - focuses on largest block vs rest block_ids <- c(\"B1\", \"B2\", \"B3\", \"B4\", \"B5\") block_weights <- c(0.1, 0.3, 0.8, 0.2, 0.4) # B3 is largest  groups <- splitLOO(block_ids, block_weights)  # Show which block is isolated (should be B3 with weight 0.8) data.frame(block_id = block_ids, weight = block_weights, group = groups) #>   block_id weight group #> 1       B1    0.1     1 #> 2       B2    0.3     1 #> 3       B3    0.8     0 #> 4       B4    0.2     1 #> 5       B5    0.4     1"},{"path":"/reference/splitSpecified.html","id":null,"dir":"Reference","previous_headings":"","what":"A set of pre-specified splits using a data.table object (Deprecate) — splitSpecified","title":"A set of pre-specified splits using a data.table object (Deprecate) — splitSpecified","text":"set pre-specified splits using data.table object (Deprecate)","code":""},{"path":"/reference/splitSpecified.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A set of pre-specified splits using a data.table object (Deprecate) — splitSpecified","text":"","code":"splitSpecified(bid, x)"},{"path":"/reference/splitSpecified.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A set of pre-specified splits using a data.table object (Deprecate) — splitSpecified","text":"bid used x data.table object column 1 k divides blocks. Column one highest level column nested","code":""},{"path":"/reference/splitSpecifiedFactor.html","id":null,"dir":"Reference","previous_headings":"","what":"A set of pre-specified splits — splitSpecifiedFactor","title":"A set of pre-specified splits — splitSpecifiedFactor","text":"function binary splits using factor variable dots separating names subgroups. two subgroups level, makes one group largest subgroup another group rest. multiple subgroups size, chooses first subgroup order levels factor.","code":""},{"path":"/reference/splitSpecifiedFactor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A set of pre-specified splits — splitSpecifiedFactor","text":"","code":"splitSpecifiedFactor(bid, x)"},{"path":"/reference/splitSpecifiedFactor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A set of pre-specified splits — splitSpecifiedFactor","text":"bid Block id x factor levels like \"state.district.school\". splits occur left right depending whether existing variation level","code":""},{"path":"/reference/splitSpecifiedFactor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A set of pre-specified splits — splitSpecifiedFactor","text":"","code":"# Create hierarchical factor for pre-specified splitting block_ids <- c(\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\") hierarchical_factor <- factor(c(   \"StateA.District1.School1\",   \"StateA.District1.School2\",   \"StateA.District2.School1\",   \"StateB.District1.School1\",   \"StateB.District1.School2\",   \"StateB.District2.School1\" ))  # First split will separate by state (StateA vs StateB) groups <- splitSpecifiedFactor(block_ids, hierarchical_factor)  # Show the grouping data.frame(block = block_ids, hierarchy = hierarchical_factor, group = groups) #>   block                hierarchy group #> 1    B1 StateA.District1.School1     1 #> 2    B2 StateA.District1.School2     1 #> 3    B3 StateA.District2.School1     1 #> 4    B4 StateB.District1.School1     0 #> 5    B5 StateB.District1.School2     0 #> 6    B6 StateB.District2.School1     0"},{"path":"/reference/splitSpecifiedFactorMulti.html","id":null,"dir":"Reference","previous_headings":"","what":"A set of pre-specified splits — splitSpecifiedFactorMulti","title":"A set of pre-specified splits — splitSpecifiedFactorMulti","text":"function allows two splits level","code":""},{"path":"/reference/splitSpecifiedFactorMulti.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A set of pre-specified splits — splitSpecifiedFactorMulti","text":"","code":"splitSpecifiedFactorMulti(bid, x)"},{"path":"/reference/splitSpecifiedFactorMulti.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A set of pre-specified splits — splitSpecifiedFactorMulti","text":"bid Block id x factor levels like \"state.district.school\". splits occur left right depending whether existing variation level","code":""},{"path":"/reference/test_children_recursive_meinshausen.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursive testing of children in Meinshausen procedure — test_children_recursive_meinshausen","title":"Recursive testing of children in Meinshausen procedure — test_children_recursive_meinshausen","text":"Recursive testing children Meinshausen procedure","code":""},{"path":"/reference/test_children_recursive_meinshausen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursive testing of children in Meinshausen procedure — test_children_recursive_meinshausen","text":"","code":"test_children_recursive_meinshausen(   parent_id,   node_dat,   node_tracker,   alpha,   method )"},{"path":"/reference/test_children_recursive_meinshausen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursive testing of children in Meinshausen procedure — test_children_recursive_meinshausen","text":"parent_id Parent node ID node_dat Node data node_tracker Node tracking structure alpha Error rate method P-value combination method","code":""},{"path":"/reference/test_children_recursive_meinshausen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recursive testing of children in Meinshausen procedure — test_children_recursive_meinshausen","text":"Updated node data","code":""},{"path":"/reference/test_intersection_null.html","id":null,"dir":"Reference","previous_headings":"","what":"Test intersection null hypothesis — test_intersection_null","title":"Test intersection null hypothesis — test_intersection_null","text":"Test intersection null hypothesis","code":""},{"path":"/reference/test_intersection_null.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test intersection null hypothesis — test_intersection_null","text":"","code":"test_intersection_null(p_values, method)"},{"path":"/reference/test_intersection_null.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test intersection null hypothesis — test_intersection_null","text":"p_values Vector child p-values method Method combination","code":""},{"path":"/reference/test_intersection_null.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test intersection null hypothesis — test_intersection_null","text":"Combined p-value intersection null","code":""},{"path":"/reference/test_intersections.html","id":null,"dir":"Reference","previous_headings":"","what":"Test intersection hypotheses — test_intersections","title":"Test intersection hypotheses — test_intersections","text":"Test intersection hypotheses","code":""},{"path":"/reference/test_intersections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test intersection hypotheses — test_intersections","text":"","code":"test_intersections(intersections, node_dat, method, alpha)"},{"path":"/reference/test_intersections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test intersection hypotheses — test_intersections","text":"intersections List intersection hypotheses node_dat Node data table p-values method Method combining p-values alpha Type error rate","code":""},{"path":"/reference/test_intersections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test intersection hypotheses — test_intersections","text":"List intersection test results","code":""},{"path":"/reference/test_union_alternative.html","id":null,"dir":"Reference","previous_headings":"","what":"Test union alternative hypothesis — test_union_alternative","title":"Test union alternative hypothesis — test_union_alternative","text":"Test union alternative hypothesis","code":""},{"path":"/reference/test_union_alternative.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test union alternative hypothesis — test_union_alternative","text":"","code":"test_union_alternative(p_values, method)"},{"path":"/reference/test_union_alternative.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test union alternative hypothesis — test_union_alternative","text":"p_values Vector child p-values method Method combination","code":""},{"path":"/reference/test_union_alternative.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test union alternative hypothesis — test_union_alternative","text":"Combined p-value union alternative","code":""},{"path":"/reference/update_wealth.html","id":null,"dir":"Reference","previous_headings":"","what":"Update wealth using e-value — update_wealth","title":"Update wealth using e-value — update_wealth","text":"Update wealth using e-value","code":""},{"path":"/reference/update_wealth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update wealth using e-value — update_wealth","text":"","code":"update_wealth(current_wealth, e_value, alpha, wealth_rule)"},{"path":"/reference/update_wealth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update wealth using e-value — update_wealth","text":"current_wealth Current wealth level e_value New e-value alpha Type error rate wealth_rule Wealth update rule","code":""},{"path":"/reference/update_wealth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update wealth using e-value — update_wealth","text":"Updated wealth","code":""},{"path":"/reference/validate_fwer_control.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate closed testing procedure maintains FWER control — validate_fwer_control","title":"Validate closed testing procedure maintains FWER control — validate_fwer_control","text":"Validate closed testing procedure maintains FWER control","code":""},{"path":"/reference/validate_fwer_control.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate closed testing procedure maintains FWER control — validate_fwer_control","text":"","code":"validate_fwer_control(node_dat, alpha)"},{"path":"/reference/validate_fwer_control.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate closed testing procedure maintains FWER control — validate_fwer_control","text":"node_dat Results closed testing alpha Nominal Type error rate","code":""},{"path":"/reference/validate_fwer_control.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate closed testing procedure maintains FWER control — validate_fwer_control","text":"Logical indicating FWER properly controlled","code":""},{"path":"/reference/validate_logical_hierarchy.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate logical hierarchy structure — validate_logical_hierarchy","title":"Validate logical hierarchy structure — validate_logical_hierarchy","text":"Validate logical hierarchy structure","code":""},{"path":"/reference/validate_logical_hierarchy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate logical hierarchy structure — validate_logical_hierarchy","text":"","code":"validate_logical_hierarchy(hierarchy, node_dat)"},{"path":"/reference/validate_logical_hierarchy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate logical hierarchy structure — validate_logical_hierarchy","text":"hierarchy Hierarchy object node_dat Node data table","code":""},{"path":"/reference/validate_logical_hierarchy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate logical hierarchy structure — validate_logical_hierarchy","text":"Logical consistency check results","code":""},{"path":"/reference/wilcoxon_sensitivity_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Wilcoxon test sensitivity bounds — wilcoxon_sensitivity_bounds","title":"Wilcoxon test sensitivity bounds — wilcoxon_sensitivity_bounds","text":"Wilcoxon test sensitivity bounds","code":""},{"path":"/reference/wilcoxon_sensitivity_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wilcoxon test sensitivity bounds — wilcoxon_sensitivity_bounds","text":"","code":"wilcoxon_sensitivity_bounds(y1, y0, gamma)"},{"path":"/reference/wilcoxon_sensitivity_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wilcoxon test sensitivity bounds — wilcoxon_sensitivity_bounds","text":"y1 Treatment group outcomes y0 Control group outcomes gamma Sensitivity parameter","code":""},{"path":"/reference/wilcoxon_sensitivity_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wilcoxon test sensitivity bounds — wilcoxon_sensitivity_bounds","text":"Upper lower p-value bounds","code":""},{"path":[]},{"path":"/news/index.html","id":"new-features-0-0-4-1001","dir":"Changelog","previous_headings":"","what":"New features","title":"manytestsr 0.0.4.1001","text":"New exported function compute_adaptive_alphas_tree() computes per-depth adjusted significance levels actual (possibly irregular) tree structure. Takes node_dat per-node sample sizes (returned find_blocks()) instead assuming regular k-ary tree. algorithm divides alpha depth sum path powers — expected number tests conducted depth. regular k-ary trees, produces identical results parametric compute_adaptive_alphas(). New exported factory function alpha_adaptive_tree() creates closure use find_blocks(alphafn = ...), using tree-based alpha schedule compute_adaptive_alphas_tree(). Drop-replacement alpha_adaptive() tree irregular branching unequal sample sizes across nodes.","code":""},{"path":"/news/index.html","id":"bug-fixes-0-0-4-1001","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"manytestsr 0.0.4.1001","text":"Fixed 11 test failures test_alpha_adaptive.R referenced removed tau parameter. Two tests (“tau = 0” “tau = 1”) rewritten error-load equivalents; rest tau arguments removed.","code":""},{"path":[]},{"path":"/news/index.html","id":"new-features-0-0-4-1000","dir":"Changelog","previous_headings":"","what":"New features","title":"manytestsr 0.0.4.1000","text":"New exported function compute_error_load() computes error load tree level — expected number -null sibling groups procedure tests. total error load 1, unadjusted procedure controls FWER via natural gating; exceeds 1, adaptive alpha adjustment required. Supports parametric interface (regular k-ary trees equal splits) tree interface (irregular trees per-node sample sizes find_blocks()).","code":""},{"path":"/news/index.html","id":"changes-0-0-4-1000","dir":"Changelog","previous_headings":"","what":"Changes","title":"manytestsr 0.0.4.1000","text":"compute_adaptive_alphas() now checks error load computing adjusted alphas. total error load 1 (natural gating suffices), nominal alpha returned every level without adjustment. tau parameter removed; error load check replaces . compute_adaptive_alphas() gains \"error_load\" attribute return value, callers can inspect error load diagnostics without separate call compute_error_load().","code":""},{"path":[]},{"path":"/news/index.html","id":"new-features-0-0-4-0000","dir":"Changelog","previous_headings":"","what":"New features","title":"manytestsr 0.0.4.0000","text":"Added adaptive alpha adjustment tree-structured hypothesis testing (compute_adaptive_alphas() alpha_adaptive()). implements Algorithm 1 paper’s Appendix D, adjusts significance levels tree depth based estimated power decay. cumulative power high, alpha tightened account multiplicity tests power enables. cumulative power drops threshold, natural gating suffices nominal alpha used. procedure supports constant variable branching factors.","code":""},{"path":"/news/index.html","id":"interface-changes-0-0-4-0000","dir":"Changelog","previous_headings":"","what":"Interface changes","title":"manytestsr 0.0.4.0000","text":"alphafn interface used find_blocks() now passes depth parameter (integer vector tree depths, 1 = root) alpha adjustment functions. enables alpha strategies depend tree structure rather treating p-values flat stream. alpha_investing(), alpha_saffron(), alpha_addis() now accept depth argument interface compatibility. use ; behavior unchanged.","code":""},{"path":"/news/index.html","id":"manytestsr-0030000","dir":"Changelog","previous_headings":"","what":"manytestsr 0.0.3.0000","title":"manytestsr 0.0.3.0000","text":"Initial tracked version find_blocks(), splitting functions (splitCluster, splitEqualApprox, splitLOO, splitSpecifiedFactor, splitSpecifiedFactorMulti), p-value functions (pOneway, pWilcox, pIndepDist, pCombCauchyDist, pTestTwice), online FDR alpha adjustment (alpha_investing, alpha_saffron, alpha_addis), local p-value adjustment (local_hommel_all_ps, local_simes, local_bh_all_ps), reporting/visualization functions.","code":""}]
